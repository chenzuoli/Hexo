<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Hexo添加阅读排行功能]]></title>
    <url>%2F2019%2F12%2F08%2FHexo%E6%B7%BB%E5%8A%A0%E9%98%85%E8%AF%BB%E6%8E%92%E8%A1%8C%E5%8A%9F%E8%83%BD%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;相比于国内和国外的开源技术和产品，我更青睐国外的，更何况是google的产品呢？&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;目前Hexo的阅读排行功能，用的最多的是Google的Firestore和国内的Leancloud。由于尝试了firestore2天，一些js和中文乱码的问题困扰着做后端的我，决定放弃firestore，使用Leancloud了，不过确实比firestore配置简单。下面来看看吧。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;主要的实现原理就是将每篇文章的访问url和次数存储到数据库中，每篇文章初始化的次数为0，每访问一次+1，取出列表时，对访问次数降序排序展示到页面上，就这么简单，主要使用js实现。 1.新建页面&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;hexo n page top新建页面，会生成 top 目录，编辑其中自动生成的 index.md 文件，将其中的代码替换一下，这个代码主要从leancloud云数据库中取出文章url和对应的阅读数量count进行排序展示，具体如下： 123456789101112131415161718192021222324252627282930---title: topdate: 2019-11-24 14:34:05---&lt;div id=&quot;top&quot;&gt;&lt;/div&gt;&lt;script src=&quot;https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js&quot;&gt;&lt;/script&gt;&lt;script&gt;AV.initialize(&quot;leancloud-appid&quot;, &quot;leancloud-appkey&quot;);&lt;/script&gt;&lt;script type=&quot;text/javascript&quot;&gt; var time=0 var title=&quot;&quot; var url=&quot;&quot; var query = new AV.Query(&apos;Counter&apos;); query.notEqualTo(&apos;id&apos;,0); query.descending(&apos;time&apos;); query.limit(1000); query.find().then(function (todo) &#123; for (var i=0;i&lt;1000;i++)&#123; var result=todo[i].attributes; time=result.time; title=result.title; url=result.url; // var content=&quot;&lt;a href=&apos;&quot;+&quot;http://wetech.top&quot;+url+&quot;&apos;&gt;&quot;+title+&quot;&lt;/a&gt;&quot;+&quot;&lt;br&gt;&quot;+&quot;&lt;font color=&apos;#fff&apos;&gt;&quot;+&quot;阅读次数：&quot;+time+&quot;&lt;/font&gt;&quot;+&quot;&lt;br&gt;&lt;br&gt;&quot;; var content=&quot;&lt;p&gt;&quot;+&quot;&lt;font color=&apos;#1C1C1C&apos;&gt;&quot;+&quot;【文章热度:&quot;+time+&quot;℃】&quot;+&quot;&lt;/font&gt;&quot;+&quot;&lt;a href=&apos;&quot;+&quot;http://wetech.top&quot;+url+&quot;&apos;&gt;&quot;+title+&quot;&lt;/a&gt;&quot;+&quot;&lt;/p&gt;&quot;; document.getElementById(&quot;top&quot;).innerHTML+=content &#125; &#125;, function (error) &#123; console.log(&quot;error&quot;); &#125;);&lt;/script&gt; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;并将其中的 leancloud_appid、leancloud_appkey 和页面链接替换为你的。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这2个参数可以从leancloud申请：https://console.leancloud.app/login.html&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;登录leancloud，进入控制台，操作步骤如下： 2.配置菜单显示&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;编辑主题配置文件 themes\next_config.yml，添加 top： 123menu: home: / || home top: /top/ || signal &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;新增菜单栏的显示名称 hexo/theme/next/languages/zh-Hans.yml，同样新增 top 对应的中文： 1234567891011menu: home: 首页 archives: 归档 categories: 分类 tags: 标签 about: 关于 search: 搜索 schedule: 日程表 sitemap: 站点地图 commonweal: 公益404 top: 阅读排行 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;开启leancloud-analysis统计功能，主要就是将url和访问量存储到云数据库中，参考文件theme/next/layout/_script/lean-analytics.swig文件&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;修改文件：theme/next/_config.yml 123456# Show number of visitors to each article.# You can visit https://leancloud.cn get AppID and AppKey.leancloud_visitors: enable: true app_id: YJgpFR6aBjuB1wPKjFFRp443-MdYXbMMI app_key: r8fnn6zRKLkDw1iaEe8g3qwb &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;最后，hexo d -g 部署后可以显示。 Keep learning.]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>阅读排行</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo添加置顶功能]]></title>
    <url>%2F2019%2F12%2F08%2FHexo%E6%B7%BB%E5%8A%A0%E7%BD%AE%E9%A1%B6%E5%8A%9F%E8%83%BD%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;目前，Hexo项目默认按照创建日期进行的排序，置顶功能如何实现呢，下面来看看吧。 修改node_modules/hexo-generator-index/lib/generator.js&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;原理就是，先按照设定的top值进行排序，再按照日期排序： 12345678910111213141516171819202122232425262728&apos;use strict&apos;;var pagination = require(&apos;hexo-pagination&apos;);module.exports = function(locals)&#123; var config = this.config; var posts = locals.posts; posts.data = posts.data.sort(function(a, b) &#123; if(a.top &amp;&amp; b.top) &#123; // 两篇文章top都有定义 if(a.top == b.top) return b.date - a.date; // 若top值一样则按照文章日期降序排 else return b.top - a.top; // 否则按照top值降序排 &#125; else if(a.top &amp;&amp; !b.top) &#123; // 以下是只有一篇文章top有定义，那么将有top的排在前面（这里用异或操作居然不行233） return -1; &#125; else if(!a.top &amp;&amp; b.top) &#123; return 1; &#125; else return b.date - a.date; // 都没定义按照文章日期降序排 &#125;); var paginationDir = config.pagination_dir || &apos;page&apos;; return pagination(&apos;&apos;, posts, &#123; perPage: config.index_generator.per_page, layout: [&apos;index&apos;, &apos;archive&apos;], format: paginationDir + &apos;/%d/&apos;, data: &#123; __index: true &#125; &#125;);&#125;; 设置文章置顶&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在文章Front-matter中添加top值，数值越大文章越靠前，如： 1234567---title: Hexo+nexT主题配置备忘date: 2019-12-08 11:49:33tags: [Hexo,next-theme,Seo]categories: Hexotop: 10--- When you are talking about something in front of so many people, don’t forget to smile.]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>置顶</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[英文缩写释义]]></title>
    <url>%2F2019%2F12%2F08%2F%E8%8B%B1%E6%96%87%E7%BC%A9%E5%86%99%E9%87%8A%E4%B9%89%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;In English language world, local people love using addr word to replace the phrase or sentence to express emotions and response, so if you want to know what said, remember the addr blow first. Let’s check is out. FYI - For your information TGIF - Thank god is Friday ASAP - As soon as possible AKA - Also known as PRC - People’s Republic of China DPRK - Democratic People’s Republic of Korea 朝鲜（North Korea） &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;I will update it irregularly. If I was wrong, please contact and correct me, really appreciate it. If you are afraid of sharing IT skills, what else you can do successfully.]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>English</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zip与unzip]]></title>
    <url>%2F2019%2F12%2F06%2Fzip%E4%B8%8Eunzip%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Linux或者mac命令zip和unzip使用频率还是蛮高的，下面介绍下如何打zip包与解压zip包。 1.打包12345zip -q -r -m -o a.zip a/ -q 不显示压缩进度 -r 子目录子文件也包含到zip中（很重要） -e 加密 -m 压缩完，删除源文件夹 2.解压12unzip a.zip 即可解压unzip &quot;*.zip”：解压目录下的多个zip文件 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果解压目录下所有的zip包，使用下面命令： 1unzip *.zip &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;会报错，因为它会去第一个解压后的目录中查找第二个zip，第二个解压的zip目录中查找第三个解压的zip，所以会出现filename not matched问题。 1caution: filename not matched: a.zip Where there is an enemy, there is a friend.]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>zip</tag>
        <tag>unzip</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive元数据表介绍]]></title>
    <url>%2F2019%2F12%2F01%2FHive%E5%85%83%E6%95%B0%E6%8D%AE%E8%A1%A8%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Hive的元数据信息通常存储在关系型数据库中，常用MySQL数据库作为元数据库管理。上一篇hive的安装也是将元数据信息存放在MySQL数据库中。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Hive的元数据信息在MySQL数据中有57张表 一、存储Hive版本的元数据表（VERSION）1VERSION -- 查询版本信息 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;该表比较简单，但很重要。 VER_ID SCHEMA_VERSION VERSION_COMMENT ID主键 Hive版本 版本说明 1 2.3.0 Set by MetaStore &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果该表出现问题，根本进入不了Hive-Cli。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;比如该表不存在，当启动Hive-Cli时候，就会报错”Table ‘hive.version’ doesn’t exist”。 二、Hive数据库相关的元数据表（DBS、DATABASE_PARAMS）1、DBS1DBS -- 存储Hive中所有数据库的基本信息 元数据表字段 说明 示例数据 DB_ID 数据库ID 1 DESC 数据库描述 测试库 DB_LOCATION_URI 数据库HDFS路径 hdfs://mycluster/user/hive/warehouse NAME 数据库名 OWNER_NAME 数据库所有者用户名 OWNER_TYPE 所有者角色 USER 2、DATABASE_PARAMS1DATABASE_PARAMS --该表存储数据库的相关参数，在CREATE DATABASE时候用 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;WITH DBPROPERTIES (property_name=property_value, …)指定的参数。 元数据表字段 说明 示例数据 DB_ID 数据库ID 2 PARAM_KEY 参数名 createdby PARAM_VALUE 参数值 lxw1234 - 注意： - - DBS和DATABASE_PARAMS这两张表通过DB_ID字段关联。 三、Hive表和视图相关的元数据表&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;主要有TBLS、TABLE_PARAMS、TBL_PRIVS，这三张表通过TBL_ID关联。 1、TBLS&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;该表中存储Hive表、视图、索引表的基本信息。 元数据表字段 说明 示例数据 TBL_ID 表ID 1 CREATE_TIME 创建时间 1436317071 DB_ID 数据库ID 2，对应DBS中的DB_ID LAST_ACCESS_TIME 上次访问时间 1436317071 OWNER 所有者 admin RETENTION 保留字段 0 SD_ID 序列化配置信息 86，对应SDS表中的SD_ID TBL_NAME 表名 student TBL_TYPE 表类型 MANAGED_TABLE、EXTERNAL_TABLE、INDEX_TABLE、VIRTUAL_VIEW VIEW_EXPANDED_TEXT 视图的详细HQL语句 select lxw1234.pt, lxw1234.pcid from liuxiaowen.lxw1234 VIEW_ORIGINAL_TEXT 视图的原始HQL语句 select * from lxw1234 2、TABLE_PARAMS&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;该表存储表/视图的属性信息。 元数据表字段 说明 示例数据 TBL_ID 表ID 1 PARAM_KEY 属性名 totalSize、numRows、EXTERNAL PARAM_VALUE 属性值 970107336、21231028、TRUE 3、TBL_PRIVS&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;该表存储表/视图的授权信息 元数据表字段 说明 示例数据 TBL_GRANT_ID 授权ID 1 CREATE_TIME 授权时间 1436320455 GRANT_OPTION 0 GRANTOR 授权执行用户 liuxiaowen GRANTOR_TYPE 授权者类型 USER PRINCIPAL_NAME 被授权用户 username PRINCIPAL_TYPE 被授权用户类型 USER TBL_PRIV 权限 Select、Alter TBL_ID 表ID 22，对应TBLS表中的TBL_ID 四、Hive文件存储信息相关的元数据表&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;主要涉及SDS、SD_PARAMS、SERDES、SERDE_PARAMS&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由于HDFS支持的文件格式很多，而建Hive表时候也可以指定各种文件格式，Hive在将HQL解析成MapReduce时候，需要知道去哪里，使用哪种格式去读写HDFS文件，而这些信息就保存在这几张表中。 1、SDS&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;该表保存文件存储的基本信息，如INPUT_FORMAT、OUTPUT_FORMAT、是否压缩等。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TBLS表中的SD_ID与该表关联，可以获取Hive表的存储信息。 元数据表字段 说明 示例数据 SD_ID 存储信息ID 1 CD_ID 字段信息ID 21，对应CDS表 INPUT_FORMAT 文件输入格式 org.apache.hadoop.mapred.TextInputFormat IS_COMPRESSED 是否压缩 0 IS_STOREDASSUBDIRECTORIES 是否以子目录存储 0 LOCATION HDFS路径 hdfs://namenode/hivedata/warehouse/ut.db/t_lxw NUM_BUCKETS 分桶数量 5 OUTPUT_FORMAT 文件输出格式 org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat SERDE_ID 序列化类ID 3，对应SERDES表 2、SD_PARAMS&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;该表存储Hive存储的属性信息，在创建表时候使用&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;STORED BY ‘storage.handler.class.name’ [WITH SERDEPROPERTIES (…)指定。 元数据表字段 说明 示例数据 SD_ID 存储配置ID 1 PARAM_KEY 存储属性名 PARAM_VALUE 存储属性值 3、SERDES&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;该表存储序列化使用的类信息 元数据表字段 说明 示例数据 SERDE_ID 序列化类配置ID 1 NAME 序列化类别名 SLIB 序列化类 org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe 4、SERDE_PARAMS&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;该表存储序列化的一些属性、格式信息,比如：行、列分隔符 元数据表字段 说明 示例数据 SERDE_ID 序列化类配置ID 1 PARAM_KEY 属性名 field.delim PARAM_VALUE 属性值 , 五、Hive表字段相关的元数据表&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;主要涉及COLUMNS_V2 1、COLUMNS_V2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;该表存储表对应的字段信息。 元数据表字段 说明 示例数据 CD_ID 字段信息ID 1 COMMENT 字段注释 COLUMN_NAME 字段名 pt TYPE_NAME 字段类型 string INTEGER_IDX 字段顺序 2 六、Hive表分区相关的元数据表&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;主要涉及PARTITIONS、PARTITION_KEYS、PARTITION_KEY_VALS、PARTITION_PARAMS 1、PARTITIONS&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;该表存储表分区的基本信息。 元数据表字段 说明 示例数据 PART_ID 分区ID 1 CREATE_TIME 分区创建时间 LAST_ACCESS_TIME 最后一次访问时间 PART_NAME 分区名 pt=2015-06-12 SD_ID 分区存储ID 21 TBL_ID 表ID 2 2、PARTITION_KEYS&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;该表存储分区的字段信息。 元数据表字段 说明 示例数据 TBL_ID 表ID 2 PKEY_COMMENT 分区字段说明 PKEY_NAME 分区字段名 pt PKEY_TYPE 分区字段类型 string INTEGER_IDX 分区字段顺序 1 3、PARTITION_KEY_VALS&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;该表存储分区字段值。 元数据表字段 说明 示例数据 PART_ID 分区ID 2 PART_KEY_VAL 分区字段值 2015-06-12 INTEGER_IDX 分区字段值顺序 0 4、PARTITION_PARAMS&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;该表存储分区的属性信息。 元数据表字段 说明 示例数据 PART_ID 分区ID 2 PARAM_KEY 分区属性名 numFiles、numRows PARAM_VALUE 分区属性值 15、502195 七、其他不常用的元数据表DB_PRIVS&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据库权限信息表。通过GRANT语句对数据库授权后，将会在这里存储。 IDXS&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;索引表，存储Hive索引相关的元数据 INDEX_PARAMS&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;索引相关的属性信息。 TAB_COL_STATS&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;表字段的统计信息。使用ANALYZE语句对表字段分析后记录在这里。 TBL_COL_PRIVS&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;表字段的授权信息 PART_PRIVS&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;分区的授权信息 PART_COL_STATS&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;分区字段的统计信息。 PART_COL_PRIVS&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;分区字段的权限信息。 FUNCS&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;用户注册的函数信息 FUNC_RU&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;用户注册函数的资源信息 八、从元数据表中获取表结构&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;来看看主要信息概览： 12345678910111213141516171819dbs db_id nametbls tbl_id db_id sd_id tbl_namesds sd_id cd_id locationcolumns_v2 cd_id column_namepartitions part_name sd_id tbl_id &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;获取表字段： 12345678select COLUMNS_V2.column_namefrom dbs, tbls, sds, columns_v2where dbs.name = ? and tbls.tbl_name = ? and dbs.db_id = tbls.db_id and tbls.sd_id = sds.sd_id and sds.cd_id = columns_v2.cd_id &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;获取表location： 123456789select dbs.name, tbls.tbl_name, sds.locationfrom sds, tbls, dbswhere dbs.name = ? and tbls.tbl_name = ? and dbs.db_id = tbls.db_id and tbls.sd_id = sds.sd_id &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;获取表分区：select dbs.name, tbls.tbl_name, part.part_namefrom dbs, tbls, partitions as partwhere dbs.name = ? and tbls.tbl_name = ? and dbs.db_id = tbls.db_id and tbls.tbl_id = part.tbl_id Where there is a will, there is a way.]]></content>
      <categories>
        <category>数据仓库</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>元数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mac下Idea快捷键大全]]></title>
    <url>%2F2019%2F12%2F01%2FMac%E4%B8%8BIdea%E5%BF%AB%E6%8D%B7%E9%94%AE%E5%A4%A7%E5%85%A8%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这篇文章中大部分是IDEA统一的快捷键，先来看看Mac的一些快捷键及图表解释吧。 1、Mac键盘符号和修饰键说明 ⌘ Command ⇧ Shift ⌥ Option ⌃ Control ↩︎ Return/Enter ⌫ Delete ⌦ 向前删除键（Fn+Delete） ↑ 上箭头 ↓ 下箭头 ← 左箭头 → 右箭头 ⇞ Page Up（Fn+↑） ⇟ Page Down（Fn+↓） Home Fn + ← End Fn + → ⇥ 右制表符（Tab键） ⇤ 左制表符（Shift+Tab） ⎋ Escape (Esc) 2、编辑 操作 快捷键 移动光标至行首 CTRL+A 第一次按时，移动光标至行首；再次按时，回到原有位置 CTRL+X 移动光标至行尾 CTRL+E 光标向左移动一个字符 CTRL+B 光标向左移动一个单词 ESC+B 光标向右移动一个字符 CTRL+F 光标向右移动一个单词 ESC+F 删除光标前一个字符，即退格（Backspace） CTRL+H 删除光标后一个字符，（相当于Delete）无任何字符时相当于exit CTRL+D 删除光标前所有字符 CTRL+U 删除光标后所有字符；纵向制表符，在脚本中下移一行，用/x0b表示 CTRL+K 删除光标前一个单词（根据空格识别单词分隔） CTRL+W 粘贴之前（CTRL+U/K/W）删除的内容 CTRL+Y 清屏，相当于指令“clear” CTRL+L 查找并自动匹配之前使用过的指令 CTRL+R 回车，相当于Enter CTRL+M 跳到新行，等同于回车 CTRL+O 新起一行，命令行下等同于回车 CTRL+J 横行制表符，在命令行中补齐指令，效果和Tab键相同 CTRL+I 补齐指令 TAB 上一条指令，等同于向上箭 CTRL+P 下一条指令，等同于向下箭 CTRL+N 使下一个特殊字符可以插入在当前位置,如CTRL-V 可以在当前位置插入一个字符,其ASCII是9, 否则一般情况下按结果是命令补齐 CTRL+V 中断操作 CTRL+C 冻结终端操作（暂停脚本） CTRL+S 恢复冻结（继续执行脚本） CTRL+Q 使下一个单词首字母大写, 同时光标前进一个单词,如光标停留在单词的某个字母上,如word中的o字母上, 则o字母变大写. 而不是w ESC+C 使下一个单词所有字母变大写, 同时光标前进一个单词；如光标在o字母上, 则ord变大写, w不变. ESC+U 使下一个单词所有字母变小写, 同时光标前进一个单词；如光标在o字母上, 则ord变小写, w不变. ESC+I 将光标处的字符和光标前一个字符替换位置 CTRL+T 重复运行最近一条以“word”开头的指令，如!ls 或 !l !word 调用上一条指令的最后一个参数作为当前指令对象,如，假设上一条指令为： ls abc.txt bbc.txt 那么， vi !$ 相当于： vi bbc.txt !$ 调用执行指定编号的历史记录指令,如!2, !11 !number I am still finding the way to home.]]></content>
      <categories>
        <category>电脑</category>
      </categories>
      <tags>
        <tag>mac</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记录一次hive堆内存溢出问题]]></title>
    <url>%2F2019%2F11%2F24%2F%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1hive%E5%A0%86%E5%86%85%E5%AD%98%E6%BA%A2%E5%87%BA%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;目前运行环境是这样的： 123hive version: 3.1.0centos version: 7.4hadoop version: 3.1.4 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;默认tez引擎 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;此表数据量： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;原来设置的参数如下： 1234567set hive.merge.mapfiles=true;set hive.merge.mapredfiles=true;set hive.merge.smallfiles.avgsize=60000000;set hive.auto.convert.join.noconditionaltask.size=10000000;set hive.exec.compress.intermediate=true;set hive.intermediate.compression.codec=org.apache.hadoop.io.compress.snappycodec;set hive.intermediate.compression.type=block; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;运行时 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;运行结果出现如下错误： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;分析： 此表数据量很大； 运行时500个reducer中每个reducer所承载的数据量很大； &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;昨天此任务运行成功，去hdfs看下结果数据文件情况：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;发现结果文件只有5个，而且每个文件34M，可想而知1个reducer当时拉取了大量的数据做聚合，这是导致reducer内存溢出的原因，于是设置reducer个数： 1set mapred.reduce.tasks=1000; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;添加参数后运行时： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;添加参数后运行结果文件如下： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;解决。 A man is not old until his regrets take place of his dreams.]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
        <tag>内存溢出</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A dog's Journal]]></title>
    <url>%2F2019%2F11%2F23%2FA-Dog-s-Journal%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The story of dog Hatchi is really tearjerking.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This movie realised at 2017, and critics in douban is good.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;But today, I will recommend you another movie. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;There is two version of this movie tells us one touching,funny,moving story, it’s about the leading character’s whole life and several dog’s life.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;When Eson was a child, he saved a puppy named Belly, and then brought it to home, played with him. Eson really like the dog, train the dog a stunt action: when eson bend over, and throw the boll to the air, the dog can catch the boll from the back of eson.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;And the dog help eson to find his girl friend.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;And then the dog is dead, they are very sad.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;After the journal, the dog become another police dog, he was shotted by the crime and died.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;And then he is become a college’s student’s dog, a young couple’s dog, it’s a boring life, they drop him finally, and then he find eson from the smell of him. the dog find the boll from the house, give it to eson, do the stunt action, eson realized that the dog is Belly. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The next version is about Eson’s grandaughter CJ, When the dog is dead he told the dog to protect his grandaughter if he has another life.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CJ has a bad situation, she hate her mother because of drinking a lot of wine and came home lately, so when she grew up, finally go away from her mother, do her musical dream.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CJ has a dog too, the dog help her to forgive her mother, and help her to realze the musical dream, and when CJ bring her dog to her grandfather’s house with her boyfriend.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The dog find the boll in the old house, and give it to Eson, make hime do the stunt action, finally, Eson realize that the dog is the Belly, he is come back. Where there is a will, there is a way.]]></content>
      <categories>
        <category>电影</category>
      </categories>
      <tags>
        <tag>Movie</tag>
        <tag>Dog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[强大的调度系统是怎样炼成的？]]></title>
    <url>%2F2019%2F11%2F23%2F%E5%BC%BA%E5%A4%A7%E7%9A%84%E8%B0%83%E5%BA%A6%E7%B3%BB%E7%BB%9F%E6%98%AF%E6%80%8E%E6%A0%B7%E7%82%BC%E6%88%90%E7%9A%84%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;随着业务发展，我们的任务会越来越多，任务之间依赖关系会越来越复杂，任务之间需要跨周期依赖，不同部门、不同用户之间任务需要做权限控制，任务失败、超时、开始结束之间不符合自定义的报警需要完善，重要任务的优先执行等等，需要一个强大的调度系统支持，才能保证数据部门的正常运转，下面来看看一个强大的调度系统需要支持的功能。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;调度系统功能： 定时任务、单个任务、依赖任务、跨周期依赖任务； 任务可以查看依赖关系（上下游），重跑任务可选择是否触发所有下游任务； 单个任务报警机制：任务失败、开始时间延迟报警、结束时间延迟报警、执行时间超时（今日任务执行时长对比该任务的历史执行时长方差）报警 SLA报警机制：根据仓库层次，或者数据的特点，设置报警任务组，组内任务未完成，及时报警； 报警支持方式选择：电话、短信、企业微信/钉钉、邮件； 报警联系人：负责人、报警组（一批人）； 对于不同部门同事之间的任务，他们应没有权限修改对应任务的，同一部门中仓库不同层次的任务应需申请才能使用； 脚本管理：如果使用git或者svn，那么线上那么多任务的脚本，想要获取某个历史脚本，难道需要回滚所有的吗？不，使用hbase管理，自带版本管理，不是全局的； &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;问题： 如果调度系统崩溃，恢复了5小时，这5小时内的小时任务如何自动恢复？ 如果小时级的下游任务有天级任务，如何重刷自动重刷历史数据？ Nobody will care about you, that’s the thing you should know.]]></content>
      <categories>
        <category>调度系统</category>
      </categories>
      <tags>
        <tag>数据仓库</tag>
        <tag>调度系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[全球统计：人一生最后悔的五件事]]></title>
    <url>%2F2019%2F11%2F17%2F%E5%85%A8%E7%90%83%E7%BB%9F%E8%AE%A1%EF%BC%9A%E4%BA%BA%E4%B8%80%E7%94%9F%E6%9C%80%E5%90%8E%E6%82%94%E7%9A%84%E4%BA%94%E4%BB%B6%E4%BA%8B%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第五名：45%的人后悔没有善待自己的身体; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第四名：57%的人后悔没有好好珍惜自己的伴侣;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第三名：62%的人后悔对子女教育不当;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第二名：73%的人后悔在年轻的时候选错了职业;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第一名：92%的人后悔年轻时努力不够导致一事无成; What you need to do is persistence.]]></content>
      <categories>
        <category>世界排名</category>
      </categories>
      <tags>
        <tag>人生</tag>
        <tag>世界排名</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[移动互联网]]></title>
    <url>%2F2019%2F11%2F17%2F%E7%A7%BB%E5%8A%A8%E4%BA%92%E8%81%94%E7%BD%91%2F</url>
    <content type="text"><![CDATA[Make the World a Better Place. 无中间商，有了中间商，价格会提高10% - 50%： 二手车：瓜子，人人车，优信等； 农产品：淘宝，抖音等； 解决实际问题： 移动支付：解决现金支付麻烦的问题； 共享单车：解决最后1公里的问题； 网约车：解决打车难的问题； 外卖：解决就餐麻烦的问题； 降低用户成本： 做生意：淘宝，微信，拼多多，抖音； 短距离出行：共享单车； B2B： 快鱼：线上线下店铺与制衣厂商之间的桥梁； 药帮忙：线下线上药店与制药厂商之间的桥梁； 美菜网：餐饮供应链服务商，为农场主提供农产品销售渠道，为餐饮店提供食材； 使行业变得透明 商品价格：汽车之家、易车、懂车帝； 制作流程：美食、手工品； 运作流程：养殖、种植、企业； 使用流程：维修、保养； &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;你我皆在这个飞速发展的世界，感受到移动互联网给我们的生活带来的改变，有什么想法呢，可以联系我。 上天眷顾自信男孩儿]]></content>
      <categories>
        <category>移动互联网</category>
      </categories>
      <tags>
        <tag>移动互联网</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据分析常用指标和术语]]></title>
    <url>%2F2019%2F11%2F16%2F%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B8%B8%E7%94%A8%E6%8C%87%E6%A0%87%E5%92%8C%E6%9C%AF%E8%AF%AD%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;有个朋友是金融行业产品经理，最近在对已有的站内用户做分层与标签分类，需要对用户进行聚类分析。一般从事数据分析行业的朋友对这类词并不陌生，但是像市场运营人员就会把这类些名词概念搞混，导致结果不准确。数据分析相关概念多且杂，容易搞混。为了便于大家区分，今天小编就来盘点一下数据分析常用的术语解释。建议大家收藏起来方便查看。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;按照以下三类进行汇总: 1、互联网常用名词解释; 2、统计学名词解释; 3、数据分析名词解释; 一、互联网常用名词解释1、PV（Page View）页面浏览量&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;指某段时间内访问网站或某一页面的用户的总数量，通常用来衡量一篇文章或一次活动带来的流量效果，也是评价网站日常流量数据的重要指标。PV可重复累计，以用户访问网站作为统计依据，用户每刷新一次即重新计算一次。 2、UV（Unique Visitor）独立访客&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;指来到网站或页面的用户总数，这个用户是独立的，同一用户不同时段访问网站只算作一个独立访客，不会重复累计，通常以PC端的Cookie数量作为统计依据。 3、Visit 访问&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;指用户通过外部链接来到网站，从用户来到网站到用户在浏览器中关闭页面，这一过程算作一次访问。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Visit可重复累计，比如我打开一个网站又关闭，再重新打开，这就算作两次访问。 4、Home Page 主页&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;指一个网站起主目录功能的页面，也是网站起点。通常是网站首页。 5、Landing Page 着陆页&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;指用户从外部链接来到网站，直接跳转到的第一个页面。比如朋友给我发了一个介绍爆款T恤的淘宝链接，我点开会直接跳转到介绍T恤的那个页面，而不是淘宝网众多其他页面之一，这个介绍T恤的页面可以算作是着陆页。 6、Bounce Rate 跳出率&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;指用户通过链接来到网站，在当前页面没有任何交互就离开网站的行为，这就算作此页面增加了一个“跳出”，跳出率一般针对网站的某个页面而言。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;跳出率=在这个页面跳出的用户数/PV 7、退出率&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;一般针对某个页面而言。指用户访问某网站的某个页面之后，从浏览器中将与此网站相关的所有页面全部关闭，就算此页面增加了一个“退出“。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;退出率=在这个页面退出的用户数/PV 8、Click 点击&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;一般针对付费广告而言，指用户点击某个链接、页面、banner的次数，可重复累计。比如我在PC端看到一则新闻链接点进去看了一会就关了，过了一会又点进去看了一遍，这就算我为这篇新闻贡献两次点击。 9、avr.time 平均停留时长&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;指某个页面被用户访问，在页面停留时长的平均值，通常用来衡量一个页面内容的质量。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;avr.time=用户总停留时长 / 访客数量 10、CTR 点击率&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;指某个广告、Banner、URL被点击的次数和被浏览的总次数的比值。一般用来考核广告投放的引流效果。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CTR=点击数（click）/被用户看到的次数 11、Conversion rate 转化率&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;指用户完成设定的转化环节的次数和总会话人数的百分比，通常用来评价一个转化环节的好坏，如果转化率较低则急需优化该转化环节。转化率=转化会话数/总会话数 12、漏斗&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通常指产生目标转化前的明确流程，比如在淘宝购物，从点击商品链接到查看详情页，再到查看顾客评价、领取商家优惠券，再到填写地址、付款，每个环节都有可能流失用户，这就要求商家必须做好每一个转化环节，漏斗是评价转化环节优劣的指标。 13、投资回报率（ROI：Return On Investment ）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;反映投入和产出的关系，衡量我这个投资值不值得，能给到我多少价值的东西（非单单的利润），这个是站在投资的角度或长远生意上看的。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其计算公式为：投资回报率（ROI）=年利润或年均利润/投资总额×100%，通常用于评估企业对于某项活动的价值，ROI高表示该项目价值高。 14、重复购买率&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;指消费者在网站中的重复购买次数。 15、Referrer 引荐流量&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通常指将用户引导至目标页面的URL（超链接）。在百度统计中，引荐流量叫做“外部链接”。 16、流失分析（Churn Analysis/Attrition Analysis）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;描述哪些顾客可能停止使用公司的产品/业务，以及识别哪些顾客的流失会带来最大损失。流失分析的结果用于为可能要流失的顾客准备新的优惠。 17、顾客细分&amp;画像（Customer Segmentation &amp; Profiling）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;根据现有的顾客数据，将特征、行为相似的顾客归类分组。描述和比较各组。 18、顾客的生命周期价值 (Lifetime Value, LTV)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;顾客在他/她的一生中为一个公司产生的预期折算利润。 19、购物篮分析（Market Basket Analysis）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;识别在交易中经常同时出现的商品组合或服务组合，例如经常被一起购买的产品。此类分析的结果被用于推荐附加商品，为陈列商品的决策提供依据等。 20、实时决策（Real Time Decisioning, RTD）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;帮助企业做出实时（近乎无延迟）的最优销售/营销决策。比如，实时决策系统（打分系统）可以通过多种商业规则或模型，在顾客与公司互动的瞬间，对顾客进行评分和排名。 21、留存/顾客留存（Retention / Customer Retention)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;指建立后能够长期维持的客户关系的百分比。 22、社交网络分析（Social Network Analysis, SNA）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;描绘并测量人与人、组与组、机构与机构、电脑与电脑、URL与URL、以及其他种类相连的信息/知识实体之间的关系与流动。这些人或组是网络中的节点，而它们之间的连线表示关系或流动。SNA为分析人际关系提供了一种方法，既是数学的又是视觉的。 23、生存分析（Survival Analysis）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;估测一名顾客继续使用某业务的时间，或在后续时段流失的可能性。此类信息能让企业判断所要预测时段的顾客留存，并引入合适的忠诚度政策。 二、统计学名词解释1、绝对数和相对数&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;绝对数：是反应客观现象总体在一定时间、一定地点下的总规模、总水平的综合性指标，也是数据分析中常用的指标。比如年GDP，总人口等等。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;相对数：是指两个有联系的指标计算而得出的数值，它是反应客观现象之间的数量联系紧密程度的综合指标。相对数一般以倍数、百分数等表示。相对数的计算公式： 1相对数=比较值（比数）/基础值（基数） 2、百分比和百分点&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;百分比：是相对数中的一种，他表示一个数是另一个数的百分之几，也成为百分率或百分数。百分比的分母是100，也就是用1%作为度量单位，因此便于比较。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;百分点：是指不同时期以百分数的形式表示的相对指标的变动幅度，1%等于1个百分点。 3、频数和频率&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;频数：一个数据在整体中出现的次数。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;频率：某一事件发生的次数与总的事件数之比。频率通常用比例或百分数表示。 4、比例与比率&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;比例：是指在总体中各数据占总体的比重，通常反映总体的构成和比例，即部分与整体之间的关系。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;比率：是样本(或总体)中各不同类别数据之间的比值，由于比率不是部分与整体之间的对比关系，因而比值可能大于1。 5、倍数和番数&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;倍数：用一个数据除以另一个数据获得，倍数一般用来表示上升、增长幅度，一般不表示减少幅度。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;番数：指原来数量的2的n次方。 6、同比和环比&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;同比：指的是与历史同时期的数据相比较而获得的比值，反应事物发展的相对性。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;环比：指与上一个统计时期的值进行对比获得的值，主要反映事物的逐期发展的情况。 7、变量&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;变量来源于数学，是计算机语言中能储存计算结果或能表示值抽象概念。变量可以通过变量名访问。 8、连续变量&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在统计学中，变量按变量值是否连续可分为连续变量与离散变量两种。在一定区间内可以任意取值的变量叫连续变量，其数值是连续不断的，相邻两个数值可作无限分割，即可取无限个数值。如:年龄、体重等变量。 9、离散变量&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;离散变量的各变量值之间都是以整数断开的，如人数、工厂数、机器台数等，都只能按整数计算。离散变量的数值只能用计数的方法取得。 10、定性变量&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;又名分类变量：观测的个体只能归属于几种互不相容类别中的一种时，一般是用非数字来表达其类别，这样的观测数据称为定性变量。可以理解成可以分类别的变量，如学历、性别、婚否等。 11、均值&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;即平均值，平均数是表示一组数据集中趋势的量数，是指在一组数据中所有数据之和再除以这组数据的个数。 12、中位数&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于有限的数集，可以通过把所有观察值高低排序后找出正中间的一个作为中位数。如果观察值有偶数个，通常取最中间的两个数值的平均数作为中位数。 13、缺失值&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;它指的是现有数据集中某个或某些属性的值是不完全的。 14、缺失率&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;某属性的缺失率=数据集中某属性的缺失值个数/数据集总行数。 15、异常值&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;指一组测定值中与平均值的偏差超过两倍标准差的测定值，与平均值的偏差超过三倍标准差的测定值，称为高度异常的异常值。 16、方差&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;是在概率论和统计方差衡量随机变量或一组数据时离散程度的度量。概率论中方差用来度量随机变量和其数学期望（即均值）之间的偏离程度。统计中的方差（样本方差）是每个样本值与全体样本值的平均数之差的平方值的平均数。在许多实际问题中，研究方差即偏离程度有着重要意义。方差是衡量源数据和期望值相差的度量值。 17、标准差&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;中文环境中又常称均方差，是离均差平方的算术平均数的平方根，用σ表示。标准差是方差的算术平方根。标准差能反映一个数据集的离散程度。平均数相同的两组数据，标准差未必相同。 18、皮尔森相关系数&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;皮尔森相关系数是用来反映两个变量线性相关程度的统计量。相关系数用r表示，其中n为样本量，分别为两个变量的观测值和均值。r描述的是两个变量间线性相关强弱的程度。r的绝对值越大表明相关性越强。 19、相关系数&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;相关系数是最早由统计学家卡尔·皮尔逊设计的统计指标，是研究变量之间线性相关程度的量，一般用字母r表示。由于研究对象的不同，相关系数有多种定义方式，较为常用的是皮尔森相关系数。 20、特征值&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;特征值是线性代数中的一个重要概念。在数学、物理学、化学、计算机等领域有着广泛的应用。设A是向量空间的一个线性变换，如果空间中某一非零向量通过A变换后所得到的向量和X仅差一个常数因子，即AX=kX，则称k为A的特征值，X称为A的属于特征值k的特征向量或特征矢量。 三、数据分析名词解释A&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;聚合(Aggregation)：搜索、合并、显示数据的过程。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;算法(Algorithms)：可以完成某种数据分析的数学公式。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;分析法(Analytics)：用于发现数据的内在涵义。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;异常检测(Anomaly detection)：在数据集中搜索与预期模式或行为不匹配的数据项。除了“Anomalies”,用来表示异常的词有以下几种：outliers,exceptions,surprises,contaminants.他们通常可提供关键的可执行信息。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;匿名化(Anonymization)：使数据匿名，即移除所有与个人隐私相关的数据。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;分析型客户关系管理（Analytical CRM/aCRM）：用于支持决策，改善公司跟顾客的互动或提高互动的价值。针对有关顾客的知识，和如何与顾客有效接触的知识，进行收集、分析、应用。 B&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;行为分析法(Behavioural Analytics)：这种分析法是根据用户的行为如“怎么做”，“为什么这么做”，以及“做了什么”来得出结论，而不是仅仅针对人物和时间的一门分析学科，它着眼于数据中的人性化模式。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;批量处理（Batch processing）：尽管从大型计算机时代开始，批量处理就已经出现了。由于处理大型数据集，批量处理对大数据具有额外的意义。批量数据处理是处理一段时间内收集的大量数据的有效方式。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;商业智能（Business Intelligence）: 分析数据、展示信息以帮助企业的执行者、管理层、其他人员进行更有根据的商业决策的应用、设施、工具、过程。 C&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;分类分析(Classification analysis)：从数据中获得重要的相关性信息的系统化过程;这类数据也被称为元数据(meta data),是描述数据的数据。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;云计算(Cloud computing)：构建在网络上的分布式计算系统，数据是存储于机房外的（即云端）。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;集群计算（Cluster computing）：这是一个使用多个服务器集合资源的“集群”的计算术语。要想更技术性的话，就会涉及到节点，集群管理层，负载平衡和并行处理等概念。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;聚类分析(Clustering analysis)：它是将相似的对象聚合在一起，每类相似的对象组合成一个聚类(也叫作簇)的过程。这种分析方法的目的在于分析数据间的差异和相似性。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;冷数据存储(Cold data storage)：在低功耗服务器上存储那些几乎不被使用的旧数据。但这些数据检索起来将会很耗时。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对比分析(Comparative analysis)：在非常大的数据集中进行模式匹配时，进行一步步的对比和计算过程得到分析结果。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;相关性分析(Correlation analysis)：是一种数据分析方法，用于分析变量之间是否存在正相关，或者负相关。 D&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;仪表板(Dashboard)：使用算法分析数据，并将结果用图表方式显示于仪表板中。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据聚合工具(Data aggregation tools)：将分散于众多数据源的数据转化成一个全新数据源的过程。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据分析师(Data analyst)：从事数据分析、建模、清理、处理的专业人员。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据库(Database)：一个以某种特定的技术来存储数据集合的仓库。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据湖（Data lake）：数据湖是原始格式的企业级数据的大型存储库。与此同时我们可以涉及数据仓库，它在概念上是相似的，也是企业级数据的存储库，但在清理、与其他来源集成之后是以结构化格式。数据仓库通常用于常规数据（但不是专有的）。数据湖使得访问企业级数据更加容易，你需要明确你要寻找什么，以及如何处理它并明智地试用它。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;暗数据（Dark Data）：基本上指的是，由企业收集和处理的，但并不用于任何意义性目的的数据，因此它是“暗”的，可能永远不会被分析。它可以是社交网络反馈，呼叫中心日志，会议笔记等等。有很多人估计，所有企业数据中的 60-90％ 可能是“暗数据”，但谁又真正知道呢？ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据挖掘（Data mining)：数据挖掘是通过使用复杂的模式识别技术，从而找到有意义的模式，并得出大量数据的见解。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据中心(Data centre)：一个实体地点，放置了用来存储数据的服务器。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据清洗(Data cleaning)：对数据进行重新审查和校验的过程，目的在于删除重复信息、纠正存在的错误，并提供数据一致性。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据质量（Data Quality）：有关确保数据可靠性和实用价值的过程和技术。高质量的数据应该忠实体现其背后的事务进程，并能满足在运营、决策、规划中的预期用途。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据订阅(Data feed)：一种数据流，例如Twitter订阅和RSS。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据集市(Data Mart)：进行数据集买卖的在线交易场所。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据建模(Data modelling)：使用数据建模技术来分析数据对象，以此洞悉数据的内在涵义。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据集(Data set)：大量数据的集合。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据虚拟化(Data virtualization)：数据整合的过程，以此获得更多的数据信息，这个过程通常会引入其他技术，例如数据库，应用程序，文件系统，网页技术，大数据技术等等。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;判别分析(Discriminant analysis)：将数据分类，按不同的分类方式，可将数据分配到不同的群组，类别或者目录。是一种统计分析法，可以对数据中某些群组或集群的已知信息进行分析，并从中获取分类规则。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;分布式文件系统(Distributed File System)：提供简化的，高可用的方式来存储、分析、处理数据的系统。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;文件存贮数据库(Document Store Databases)：又称为文档数据库，为存储、管理、恢复文档数据而专门设计的数据库，这类文档数据也称为半结构化数据。 E&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;探索性分析(Exploratory analysis)：在没有标准的流程或方法的情况下从数据中发掘模式。是一种发掘数据和数据集主要特性的一种方法。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;提取-转换-加载(ETL:Extract,Transform and Load)：是一种用于数据库或者数据仓库的处理过程，天善学院有国内唯一的最全的ETL学习课程。即从各种不同的数据源提取(E)数据，并转换(T)成能满足业务需要的数据，最后将其加载(L)到数据库。 G&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;游戏化(Gamification)：在其他非游戏领域中运用游戏的思维和机制，这种方法可以以一种十分友好的方式进行数据的创建和侦测，非常有效。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;图形数据库(Graph Databases)：运用图形结构(例如，一组有限的有序对，或者某种实体)来存储数据，这种图形存储结构包括边缘、属性和节点。它提供了相邻节点间的自由索引功能，也就是说，数据库中每个元素间都与其他相邻元素直接关联。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;网格计算(Grid computing)：将许多分布在不同地点的计算机连接在一起，用以处理某个特定问题，通常是通过云将计算机相连在一起。 H&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Hadoop：一个开源的分布式系统基础框架，可用于开发分布式程序，进行大数据的运算与存储。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Hadoop数据库(HBase)：一个开源的、非关系型、分布式数据库，与Hadoop框架共同使用。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;HDFS：Hadoop分布式文件系统(Hadoop Distributed File System)；是一个被设计成适合运行在通用硬件(commodity hardware)上的分布式文件系统。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;高性能计算(HPC:High-Performance-Computing)：使用超级计算机来解决极其复杂的计算问题。 I&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;内存数据库(IMDB:In-memory)：一种数据库管理系统，与普通数据库管理系统不同之处在于，它用主存来存储数据，而非硬盘。其特点在于能高速地进行数据的处理和存取。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;物联网（IoT）：最新的流行语是物联网（IOT）。IOT通过互联网将嵌入式对象（传感器，可穿戴设备，汽车，冰箱等）中的计算设备进行互连，并且能够发送以及接收数据。IOT生成大量数据，提供了大量大数据分析的机会。 K&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;键值数据库(Key-Value Databases)：数据的存储方式是使用一个特定的键，指向一个特定的数据记录，这种方式使得数据的查找更加方便快捷。键值数据库中所存的数据通常为编程语言中基本数据类型的数据。 L&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;负载均衡(Load balancing)：将工作量分配到多台电脑或服务器上，以获得最优结果和最大的系统利用率。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;位置信息(Location data)：GPS信息，即地理位置信息。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;日志文件(Log file)：由计算机系统自动生成的文件，记录系统的运行过程。 M&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;M2M数据(Machine 2 Machine data)：两台或多台机器间交流与传输的内容。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;机器数据(Machine data)：由传感器或算法在机器上产生的数据。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;机器学习(Machine learning)：人工智能的一部分，指的是机器能够从它们所完成的任务中进行自我学习，通过长期的累积实现自我改进。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Map Reduce：是处理大规模数据的一种软件框架(Map:映射，Reduce:归纳)。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;大规模并行处理(MPP:Massivel yParallel Processing)：同时使用多个处理器(或多台计算机)处理同一个计算任务。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;元数据(Meta data)：被称为描述数据的数据，即描述数据数据属性(数据是什么)的信息。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;多维数据库(Multi-Dimensional Databases)：用于优化数据联机分析处理(OLAP)程序，优化数据仓库的一种数据库。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;多值数据库(MultiValue Databases)：是一种非关系型数据库(NoSQL),一种特殊的多维数据库：能处理3个维度的数据。主要针对非常长的字符串，能够完美地处理HTML和XML中的字串。 N&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;自然语言处理(Natural Language Processing)：是计算机科学的一个分支领域，它研究如何实现计算机与人类语言之间的交互。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;网络分析(Network analysis)：分析网络或图论中节点间的关系，即分析网络中节点间的连接和强度关系。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;NewSQL：一个优雅的、定义良好的数据库系统，比SQL更易学习和使用，比NoSQL更晚提出的新型数据库。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;NoSQL：顾名思义，就是“不使用SQL”的数据库。这类数据库泛指传统关系型数据库以外的其他类型的数据库。这类数据库有更强的一致性，能处理超大规模和高并发的数据。 O&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对象数据库(Object Databases)：(也称为面象对象数据库)以对象的形式存储数据，用于面向对象编程。它不同于关系型数据库和图形数据库，大部分对象数据库都提供一种查询语言，允许使用声明式编程(declarative programming)访问对象。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;基于对象图像分析(Object-based Image Analysis)：数字图像分析方法是对每一个像素的数据进行分析，而基于对象的图像分析方法则只分析相关像素的数据，这些相关像素被称为对象或图像对象。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;操作型数据库(Operational Databases)：这类数据库可以完成一个组织机构的常规操作，对商业运营非常重要，一般使用在线事务处理，允许用户访问、收集、检索公司内部的具体信息。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;优化分析(Optimization analysis)：在产品设计周期依靠算法来实现的优化过程，在这一过程中，公司可以设计各种各样的产品并测试这些产品是否满足预设值。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;本体论(Ontology）：表示知识本体，用于定义一个领域中的概念集及概念之间的关系的一种哲学思想。(译者注:数据被提高到哲学的高度，被赋予了世界本体的意义，成为一个独立的客观数据世界) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;异常值检测(Outlier detection)：异常值是指严重偏离一个数据集或一个数据组合总平均值的对象，该对象与数据集中的其他它相去甚远，因此，异常值的出现意味着系统发生问题，需要对此另加分析。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;联机分析处理（On-Line Analytical Processing，OLAP）：能让用户轻松制作、浏览报告的工具，这些报告总结相关数据，并从多角度分析。 P&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;模式识别(Pattern Recognition)：通过算法来识别数据中的模式，并对同一数据源中的新数据作出预测 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;平台即服务(PaaS:Platform-as-a-Service)：为云计算解决方案提供所有必需的基础平台的一种服务。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;预测分析(Predictive analysis)：大数据分析方法中最有价值的一种分析方法，这种方法有助于预测个人未来(近期)的行为，例如某人很可能会买某些商品，可能会访问某些网站，做某些事情或者产生某种行为。通过使用各种不同的数据集，例如历史数据，事务数据，社交数据，或者客户的个人信息数据，来识别风险和机遇。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;公共数据(Public data)：由公共基金创建的公共信息或公共数据集。 Q&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数字化自我(Quantified Self)：使用应用程序跟踪用户一天的一举一动，从而更好地理解其相关的行为。 R&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;R：是一种编程语言，在统计计算方面很出色。如果你不知道 R，你就称不上是数据科学家。R 是数据科学中最受欢迎的语言之一。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;再识别(Re-identification)：将多个数据集合并在一起，从匿名化的数据中识别出个人信息。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;回归分析(Regression analysis)：确定两个变量间的依赖关系。这种方法假设两个变量之间存在单向的因果关系(译者注：自变量，因变量，二者不可互换)。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;实时数据(Real-time data)：指在几毫秒内被创建、处理、存储、分析并显示的数据。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;推荐引擎(Recommendation engine)：推荐引擎算法根据用户之前的购买行为或其他购买行为向用户推荐某种产品。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;路径分析(Routing analysis)：–针对某种运输方法通过使用多种不同的变量分析从而找到一条最优路径，以达到降低燃料费用，提高效率的目的。 S&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;半结构化数据(Semi-structured data)：半结构化数据并不具有结构化数据严格的存储结构，但它可以使用标签或其他形式的标记方式以保证数据的层次结构。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;结构化数据(Structured data)：可以组织成行列结构，可识别的数据。这类数据通常是一条记录，或者一个文件，或者是被正确标记过的数据中的某一个字段，并且可以被精确地定位到。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;情感分析(Sentiment Analysis)：通过算法分析出人们是如何看待某些话题。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;信号分析(Signal analysis)：指通过度量随时间或空间变化的物理量来分析产品的性能。特别是使用传感器数据。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;相似性搜索(Similarity searches)：在数据库中查询最相似的对象，这里所说的数据对象可以是任意类型的数据。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;仿真分析(Simulation analysis)：仿真是指模拟真实环境中进程或系统的操作。仿真分析可以在仿真时考虑多种不同的变量，确保产品性能达到最优。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;软件即服务(SaaS:Software-as-a-Service)：基于Web的通过浏览器使用的一种应用软件。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;空间分析(Spatial analysis)：空间分析法分析地理信息或拓扑信息这类空间数据，从中得出分布在地理空间中的数据的模式和规律。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SQL：在关系型数据库中，用于检索数据的一种编程语言。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;流处理（Stream processing）：流处理旨在对有“连续”要求的实时和流数据进行处理。结合流分析，即在流内不间断地计算数学或统计分析的能力。流处理解决方案旨在对高流量进行实时处理。 T&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;时序分析(Time series analysis)：分析在重复测量时间里获得的定义良好的数据。分析的数据必须是良好定义的，并且要取自相同时间间隔的连续时间点。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;拓扑数据分析(Topological Data Analysis)：拓扑数据分析主要关注三点：复合数据模型、集群的识别、以及数据的统计学意义。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;交易数据(Transactional data)：随时间变化的动态数据 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;透明性(Transparency)：–消费者想要知道他们的数据有什么作用、被作何处理，而组织机构则把这些信息都透明化了。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;文本挖掘（Text Mining）：对包含自然语言的数据的分析。对源数据中词语和短语进行统计计算，以便用数学术语表达文本结构，之后用传统数据挖掘技术分析文本结构。 U&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;非结构化数据(Un-structured data)：非结构化数据一般被认为是大量纯文本数据，其中还可能包含日期，数字和实例。 V&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;价值(Value)：(译者注：大数据4V特点之一)所有可用的数据，能为组织机构、社会、消费者创造出巨大的价值。这意味着各大企业及整个产业都将从大数据中获益。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;可变性(Variability)：也就是说，数据的含义总是在（快速）变化的。例如，一个词在相同的推文中可以有完全不同的意思。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;多样(Variety)：(译者注：大数据4V特点之一)数据总是以各种不同的形式呈现，如结构化数据，半结构化数据，非结构化数据，甚至还有复杂结构化数据 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;高速(Velocity)：(译者注：大数据4V特点之一)在大数据时代，数据的创建、存储、分析、虚拟化都要求被高速处理。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;真实性(Veracity)：组织机构需要确保数据的真实性，才能保证数据分析的正确性。因此，真实性(Veracity)是指数据的正确性。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;可视化(Visualization)：只有正确的可视化，原始数据才可被投入使用。这里的“可视化”并非普通的图型或饼图，可视化指是的复杂的图表，图表中包含大量的数据信息，但可以被很容易地理解和阅读。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;大量(Volume)：(译者注：大数据4V特点之一)指数据量，范围从Megabytes至Brontobytes。 W&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;天气数据(Weather data)：是一种重要的开放公共数据来源，如果与其他数据来源合成在一起，可以为相关组织机构提供深入分析的依据。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;网络挖掘/网络数据挖掘（Web Mining / Web Data Mining)：使用数据挖掘技术从互联网站点、文档或服务中自动发现和提取信息。 X&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;XML数据库(XML Databases)：XML数据库是一种以XML格式存储数据的数据库。XML数据库通常与面向文档型数据库相关联，开发人员可以对XML数据库的数据进行查询，导出以及按指定的格式序列化。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;以上就是数据分析相关术语的盘点，看完别忘了收藏哟~ Sometimes you should force yourself, and then you can know what’s your ability which level is.]]></content>
      <categories>
        <category>数据仓库</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
        <tag>指标</tag>
        <tag>数据仓库</tag>
        <tag>术语</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[English Corner 20191115]]></title>
    <url>%2F2019%2F11%2F16%2F%E8%8B%B1%E8%AF%AD%E8%A7%92-20191115%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;When you enter this article, I think you are interested in English, or you are good at English. So I will tell you a good story you may like to you, check it out below. &nbsp;&nbsp;&nbsp;&nbsp;I am learning English Speaking, really love it, so I find a English Corner activity in the RenMin University whick is organized every Friday evening. If you are in Beijing, it’s best place for you to practive English speaking I have ever found. &nbsp;&nbsp;&nbsp;&nbsp;I met an interesting group which was consist of some teachers and some kids yesterday, these students were really exciting about the homework the teacher give them, the homework was selling the homemade coffee to other person who they didn’t know each other like me in the activiy. &nbsp;&nbsp;&nbsp;&nbsp;Firstly, they gave one coin to us who wanted to join this game for buying the coffee from their kids, actually the coin was not real RMB coin, but I was late for the game, so I didn’t have one game coin. &nbsp;&nbsp;&nbsp;&nbsp;When I went there, the game was started, I saw them practicing the game, every kid was trying to sell the hot coffee to the teacher one by one, I found that they were so cute, and the announce, the fluent of speaking was out of my expectation, but some of them was a little not, though, it doesn’t matter for them to sell the coffee. when one kid sold one cup of coffee to another one, there were applauses for him or her, in my opinion, it’s hilarious effective and meaningful for them. That’s the second round. &nbsp;&nbsp;&nbsp;&nbsp;The third step was selling hot Nest Coffee to us they didn’t know, I didn’t know how long it passed, I still talked with my partner who I met the second time, this guy is an Artificial Intelligence engineer in TsingHua University, he is a little fluent in English, and love his job, these days, he is annoying about his kids always messing the room up, he said there also were some interesting and happy things happening about his family, though. I said It’s great, lol. &nbsp;&nbsp;&nbsp;&nbsp;Time flied, one kid came to us suddenly, he talked to my partner: Hello, do you want a coffee? then they had a conversation and I just stood there to hear their words, my partner just had some questions about personal information and the school life for him, and gave him the coin, said goodbye to each other politely. &nbsp;&nbsp;&nbsp;&nbsp;And then, we talked about the game, the kids, and other things, another student came to me, and said: Hello, your friend have a coffee, I think you want a coffee too, do you want a coffee? So here was my conversation with the kid. &nbsp;&nbsp;&nbsp;&nbsp;I asked some information about the coffee to make sure the coffee is drinkable or is hot or not, I knew the coffee is drinkable, just made some conversation with him in english, actually his english is better than me. Then, I had some another questions, talked about his school life and the courses on school, he said that he had seven projects named chinese, math, english, history, biology, physics and politics, he just didn’t know how to say the politics, wow, it’s enough for the 9 years old kid. We also talked about the beijing’s weather and the difference between the chinese school and the foreign school, he likes chinese school. &nbsp;&nbsp;&nbsp;&nbsp;Any way, he asked a final question: So do you want a coffee? I said yes, but I don’t have a game coin, do you support Wechat pay or Alipay? He said yes, wow, that’s great, then he showed me the wechat code, and I scaned him, but there was some problem about the payment, he showed me the add friend code, so I taught him the way. In this situation, his teacher noticed it, and stood there seeing the whole operation for safety. Final he knew what we were doing, and corrected him that this was a game, he should not sell me the coffee for the real money, just gave him one game coin for the reward. haha, it’s really funny, I thought it’s ok to buy one coffee with two yuan and spoke with him in english, also meaningful for me. Finally I said goodbye to them and smiled, lol. &nbsp;&nbsp;&nbsp;&nbsp;I finished the talk, and have a break thinking about the funny game. Then I found another partner who is a mechanical robot engineer, he has gone aboard for working months, and told me about the foreign life, I really like the talk, let me know the life in UK and the difference between China and UK, I learned much. &nbsp;&nbsp;&nbsp;&nbsp;Wow, there are so many high educational people in this activity, last few times, I just met some fresh students in this university and other english lover came from other place, there is saler, sciencist, teacher and guard and so on. You can communciate with everyone here, and they are friendly and always talking, it’s so realx after the work. If you want it, just come with me, do it. &nbsp;&nbsp;&nbsp;&nbsp;It’s really wonderful day, see you next time. You should force yourself to do the thing that your heart think is right.]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>English</tag>
        <tag>英语角</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OCR识别是什么？]]></title>
    <url>%2F2019%2F11%2F16%2FOCR%E8%AF%86%E5%88%AB%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;OCR 是英文Optical Character Recognition的缩写，意思是光学字符识别，也可简单地称为文字识别，是文字自动输入的一种方法。它通过 扫描 和摄像等光学输入方式获取纸张上的文字图像信息，利用各种 模式 识别算法分析文字形态特征 可以将票据、报刊、书籍、文稿及其它印刷品转化为图像信息，再利用文字识别技术将图像信息转化为可以使用的计算机输入技术。可应用于银行票据、大量文字资料、档案卷宗、文案的录入和处理领域。适合于银行、税务等行业大量票据表格的自动扫描识别及长期存储。相对一般文本，通常以最终识别率、识别速度、版面理解正确率及版面还原满意度4个方面作为OCR技术的评测依据；而相对于表格及票据， 通常以识别率或整张通过率及识别速度为测定OCR技术的实用标准，随着人工智能的兴起，人们在追求让工作更简单化，ocr识别技术可以让从事文字工作的人更加轻松，以下是ocr在生活和工作中的应用。 1. 证件OCR识别&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;证件OCR识别技术一开始是基于PC的，近几年开始向移动端发展，主要有android，ios平台的SDK，目前成熟的有身份证识别，行驶证识别，驾驶证识别，护照识别等。 2. 银行卡OCR识别&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;银行卡OCR识别主要用于移动支付绑卡，是一项非常有技术含量的细分OCR技术，目前有一些APP已经在用，如支付宝，微信等。 3. 名片OCR识别&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;名片OCR识别这一类技术也非常成熟了，目前市场上名片管理的APP也非常多，多半已经使用这类技术。 4. 文档OCR识别&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其实OCR技术最早的时候就是用于识别文档的，基于扫描技术，主要针对图书，报刊等，把这些纸质文档进行电子化，目前中英文识别率也非常高。近几年也开始用于移动端的文档识别，扫一扫就可以识别。 5. 票据OCR识别&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;票据OCR识别顾名思义用于各式各样的票据识别，基于模板机制，需要针对不同的票据，定制不同的识别要素，这项技术也称要素识别OCR，最早的其实运用的是银行行业，现在企业、金融、电信机构都在使用。 6. 车牌OCR识别&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;车牌识别技术相信大家都不会觉得陌生，智能交通，小区停车场等，都有很好的应用，车牌识别的原理其实技术对车牌进行OCR识别，再进行比对的过程。也是相当成熟的技术。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们每天都被文字所环绕，像我们的工作文案、书本、证件、商品的介绍都是文字组成的，ocr技术的运用，可以让有些工作变得简单化、智能化，以后他将伴随着我们的生活，让我们的生活更加智能。 Challenge yourself.]]></content>
      <categories>
        <category>OCR</category>
      </categories>
      <tags>
        <tag>OCR</tag>
        <tag>智能识别</tag>
        <tag>金融</tag>
        <tag>科技</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据分析员一定要知道的分析指标]]></title>
    <url>%2F2019%2F11%2F13%2F%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%91%98%E4%B8%80%E5%AE%9A%E8%A6%81%E7%9F%A5%E9%81%93%E7%9A%84%E5%88%86%E6%9E%90%E6%8C%87%E6%A0%87%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;作为数据分析员，不管身处互联网公司，还是金融科技公司，都离不开一些指标的计算，下面来看看一般用到了哪些指标，这些指标有哪些参考价值。 PV: 浏览次数、点击次数、购买次数、服务次数等等； UV: 浏览人数、点击人数、购买人身、服务人数等等； GMV: 成交总金额、按分类成交总金额； 留存数：购买留存，浏览或查询留存； 留存率: 昨日留存、三日留存、七日留存、30日留存，反映产品对用户的吸引力； 转化率: 一般产品包含许多服务，而这些服务从开始的注册、登陆，到信息认证，申请，完成订单，需要许多步骤，而每一步都有可能筛选掉部分用户，此时用户质量就可以使用转化率来反映； 复购人数、复购次数、复购率； &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;本文会不定期更新，如有什么问题，欢迎指正。 Free your mind.]]></content>
      <categories>
        <category>数据仓库</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
        <tag>数据仓库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[发现了数据仓库的一个秘密]]></title>
    <url>%2F2019%2F11%2F12%2F%E5%8F%91%E7%8E%B0%E4%BA%86%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E7%9A%84%E4%B8%80%E4%B8%AA%E7%A7%98%E5%AF%86%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;那就是他的预测功能。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;基于历史数据发现一些规律，来预测接下来一段时间的数据可能发生的事，包括仓库本身（数据增长趋势，定时任务完成时间趋势等等来提醒你该做出相应的行动了）、数据集市（提供一些指标，来预测你的行动范围、风险承受能力在一个适当的范围之内，而不至于做出不正确的决策，给公司带来损失）。 You will be the king.]]></content>
      <categories>
        <category>数据仓库</category>
      </categories>
      <tags>
        <tag>数据仓库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[推荐一个sql学习网站]]></title>
    <url>%2F2019%2F11%2F10%2F%E6%8E%A8%E8%8D%90%E4%B8%80%E4%B8%AAsql%E5%AD%A6%E4%B9%A0%E7%BD%91%E7%AB%99%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;推荐一个sql学习网站，从入门到精通，有很好的例子提供给你练习，图片动画很容易理解。前提条件是有一定的英文基础，下面来看看吧。 增删改查：select/insert/update/delete; DDL：create/drop/alter table; 条件判断：like/between and/if/case when/limit/group by/order by; 聚合函数：sum/count/avg/max/min; 表关联：join/left join/right join/inner join/union/union all/cross join; 复杂查询； https://www.codecademy.com/learn/learn-sql The weakest goes to the wall.]]></content>
      <categories>
        <category>数据仓库</category>
      </categories>
      <tags>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ANSI-sql和SQL的区别]]></title>
    <url>%2F2019%2F11%2F10%2FANSI-sql%E5%92%8CSQL%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;相信大家都使用过SQL SERVER。今天给大家简单介绍一下Oracle SQL与ANSI SQL区别。其实，SQL SERVER与与ANSI SQL也有区别。 什么是ANSI&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ANSI：美国国家标准学会（American National Standards Institute）。当时，美国的许多企业和专业技术团体，已开始了标准化工作，但因彼此间没有协调，存在不少矛盾和问题。为了进一步提高效率，数百个科技学会、协会组织和团体，均认为有必要成立一个专门的标准化机构，并制订统一的通用标准。 ANSI SQL到底是什么（1）作为程序员开发者们应该知道，在使用那些非标准的SQL命令（比如Oracle、微软和MySQL等数据库系统）从跨平台和遵守标准的角度出发，你应该尽量采用ANSI SQL，它是一种和平台无关的数据库语言。其实为什么这么说了，很简单就是可能在Oracle能够运行的SQL语句不一定在SQL SERVER当中能够运行，那么在跨平台当中数据操作就会带来困难。（2）程序在开发的时候，如果使用SQL语句对数据进行操作。一般的建议不管你在使用哪种数据库系统，如果该数据库系统中的SQL完全支持ANSI SQL标准，那么请你尽量使用ANSI SQL。 一些写法区别&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;既然标准不一样，那么写法肯定有一些区别，具体可以参考这2篇文章(Presto查询系统使用的是ANSI-SQL语法)： Presto-SQL与Hive-SQL的区别与联系Presto基础用法介绍 No one can take away who you are.]]></content>
      <categories>
        <category>数据仓库</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>ANSI-SQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Presto-SQL与Hive-SQL的区别与联系]]></title>
    <url>%2F2019%2F11%2F09%2FPresto-SQL%E4%B8%8EHive-SQL%E7%9A%84%E5%8C%BA%E5%88%AB%E4%B8%8E%E8%81%94%E7%B3%BB%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Presto作为一个数据仓库后端的OLAP分析查询系统，提供了比Hive更快的查询速度，能及时提供用户的分析结果数据，深受OLAP用户喜爱，那么我们从hive迁移到presto时他们的查询方式有哪些区别与联系呢，下面来看看。 一、前言&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Presto使用ANSI SQL语法和语义，而Hive使用类似SQL的语言，称为HiveQL，它在MySQL（它本身与ANSI SQL有很多不同）之后进行了松散的建模。 二、使用下标来访问数组的动态索引而不是udf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SQL中的下标运算符支持完整表达式，与Hive（仅支持常量）不同。因此，您可以编写如下查询： 12SELECT my_array[CARDINALITY(my_array)] as last_elementFROM ... 三、避免超出阵列的边界访问&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;访问数组的超出边界元素将导致异常。您可以通过以下方式避免这种if情况： 12SELECT IF(CARDINALITY(my_array) &gt;= 3, my_array[3], NULL)FROM ... 四、对数组使用ANSI SQL语法&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数组从1开始索引，而不是从0开始： 12SELECT my_array[1] AS first_elementFROM ... &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;使用ANSI语法构造数组： 1SELECT ARRAY[1, 2, 3] AS my_array 五、对标识符和字符串使用ANSI SQL语法&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;字符串用单引号分隔，标识符引用双引号，而不是反引号： 123SELECT name AS &quot;User Name&quot;FROM &quot;7day_active&quot;WHERE name = &apos;foo&apos; 六、引用以数字开头的标识符&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;以数字开头的标识符在ANSI SQL中不合法，必须使用双引号引用： 12SELECT *FROM &quot;7day_active&quot; 七、使用标准字符串连接运算符&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;使用ANSI SQL字符串连接运算符： 12SELECT a || b || cFROM ... 八、使用CAST目标的标准类型&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CAST目标支持以下标准类型： 123456SELECT CAST(x AS varchar), CAST(x AS bigint), CAST(x AS double), CAST(x AS boolean)FROM ... &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;特别是，使用VARCHAR而不是STRING。 九、除以整数时使用CAST&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Presto遵循分割两个整数时执行整数除法的标准行为。例如，除以7由2将导致3，而不是3.5。要对两个整数执行浮点除法，请将其中一个转换为double： 1SELECT CAST(5 AS DOUBLE) / 2 十、使用WITH表示复杂的表达式或查询&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果要将复杂输出表达式重用为过滤器，请使用内联子查询或使用WITH子句将其分解： 1234567WITH a AS ( SELECT substr(name, 1, 3) x FROM ...)SELECT *FROM aWHERE x = &apos;foo&apos; 十一、使用UNNEST扩展数组和映射&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Presto支持UNNEST扩展阵列和地图。用UNNEST而不是。LATERAL VIEW explode()&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Hive查询： 123SELECT student, scoreFROM testsLATERAL VIEW explode(scores) t AS score; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Presto查询： 123SELECT student, scoreFROM testsCROSS JOIN UNNEST(scores) AS t (score); If I were you, I would do what I want.]]></content>
      <categories>
        <category>数据仓库</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>Hive</tag>
        <tag>Presto</tag>
        <tag>OLAP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[企业级数据仓库设计之业务库模型设计]]></title>
    <url>%2F2019%2F11%2F09%2F%E4%BC%81%E4%B8%9A%E7%BA%A7%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E8%AE%BE%E8%AE%A1%E4%B9%8B%E4%B8%9A%E5%8A%A1%E5%BA%93%E6%A8%A1%E5%9E%8B%E8%AE%BE%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据仓库的数据哪里来，业务库、日志文件、第三方提供、历史业务数据、外部爬取数据等，这些数据可能是结构化的，也可能是非结构化的，也可能是半结构化（json），我们放到数据仓库中，需要统一规范（字段、度量、维度等），那么我们如何去规范呢？ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;本公司业务库、业务日志等是目前我们能够控制的，在设计的初期去满足范式设计模式、实体设计模式，这样当数据实时或者按小时或者按天收集到数据仓库中，我们所做的清洗工作、主题业务模型设计、报表就会简单许多。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;想象一下，如果我们在获取某个业务指标时，如果表中即存在会员基本信息，也包含会员渠道信息，也包含会员订单信息（不符合实体设计模式），而且你在获取唯一关联id时，需要从某个字段中截取或者json格式中获取字段（不符合第一范式），一个指标下来，关联内嵌的表达到十几个，你是不是要疯了，而且后期维护起来特别麻烦，过几周再去查看sql时自己都看得费劲。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这样的结果是我们不愿看到的，出现这样的情况就是业务库在设计表结构时没有遵循相应的设计模式，不管是第三范式还是实体设计模式，应至少遵循其中一个设计模式，这样我们后期的集市建设、主题模型设计就不至于逻辑复杂、sql复杂了。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;希望大家能够知道这一点，这是我工作中亲身经历的情况。 这里有篇文章可以参考，讲解 范式设计模式和实体设计模式。 希望你能每遇到一个问题，多思考为什么会出现这个问题。]]></content>
      <categories>
        <category>数据仓库</category>
      </categories>
      <tags>
        <tag>数据仓库</tag>
        <tag>业务库</tag>
        <tag>模型设计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据仓库监控系统]]></title>
    <url>%2F2019%2F11%2F07%2F%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们如何知道仓库的运行情况呢？仓库监控系统可以看到。下面来看看仓库监控系统里面需要监控的内容。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据仓库监控系统可以监控的内容： 任务概览； 任务运行详情：日志、状态、开始时间、结束时间、运行时长等 任务endTime时间（结束时间与deadline时间比较）监控； 库表数概览； 数量； 数据量概览； 占用磁盘大小； 仓库层次概览； 仓库分层； 主题域概览； 模型设计； 数据集市概览； 对外业务集市； 对内业务集市； 外部导入集市； 数据源概览； 仓库数据来源； 调度系统报警机制：可设置报警人、报警组，和报警方式，如短信、企业邮箱、企业微信、钉钉等 超时任务：每个任务历史执行时长取方差，如果本次任务减去平均值超出方差范围，则报警； 失败任务； 数据重复； 开始时间预警； 结束时间预警； 多个任务设置报警SLA； SLA服务会根据组内的任务运行情况实时做一个统计，定时发送组内任务运行情况； 在配置任务的时候设置标签，属于哪个SLA组，可以根据仓库层次、数据集市进行分组配置任务运行概览； Time is money.]]></content>
      <categories>
        <category>数据仓库</category>
      </categories>
      <tags>
        <tag>数据仓库</tag>
        <tag>监控系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据仓库数据质量管理系统]]></title>
    <url>%2F2019%2F11%2F05%2F%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%95%B0%E6%8D%AE%E8%B4%A8%E9%87%8F%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;简单介绍下数据仓库数据质量管理系统，因为本人也是在不断学习建设当中，所以有什么问题请及时沟通。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;下面来看下质量管理系统需要做的工作： 数据表字段值范围预警； 导数据质量对比； 数据量； 数据值； 任务运行时间范围预警； When the going gets tough, the tough get going.]]></content>
      <categories>
        <category>数据仓库</category>
      </categories>
      <tags>
        <tag>数据仓库</tag>
        <tag>数据质量管理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据仓库元数据管理系统-1]]></title>
    <url>%2F2019%2F11%2F05%2F%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%85%83%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F-1%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;简单记录一下数据仓库元数据管理系统所包含的内容。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;基本包含的内容如下（因为本人也在不断的建设仓库学习当中，所以不定期更新）： 数据库、表结构信息 分区； 压缩格式； 文件路径； 列信息 库信息； 数据量 按照分区的大小增长曲线； 血缘关系 上游血缘； 下游血缘； Where there is a will, there is a way.]]></content>
      <categories>
        <category>数据仓库</category>
      </categories>
      <tags>
        <tag>数据仓库</tag>
        <tag>元数据管理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据挖掘流程介绍]]></title>
    <url>%2F2019%2F11%2F04%2F%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E6%B5%81%E7%A8%8B%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据挖掘其实是一种深层次的数据分析方法。数据挖掘可以描述为：按企业既定业务目标，对大量的企业数据进行探索和分析，揭示隐藏的、未知的或验证已知的规律性，并进一步将其模型化的先进有效的方法。下面来看下挖掘有价值的数据的基本流程。 概述&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;应用的技术包括：数据库技术、人工智能技术、数理统计、可视化技术、并行计算等方面。数据挖掘与传统的数据分析（如查询、报表、联机应用分析）的本质区别是数据挖掘是在没有明确假设的前提下去挖掘信息、发现知识。数据挖掘所得到的信息应具有先知、有效和可实用三个特征。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据挖掘的目标是从数据库中发现隐含的、有意义的知识，主要有以下五类功能。 自动预测趋势和行为数据挖掘自动在大型数据库中寻找预测性信息，以往需要进行大量手工分析的问题如今可以迅速直接由数据本身得出结论。 关联分析数据关联是数据库中存在的一类重要的可被发现的知识。若两个或多个变量的取值之间存在某种规律性，就称为关联。 聚类数据库中的记录可被划分为一系列有意义的子集，即聚类。 概念描述就是对某类对象的内涵进行描述，并概括这类对象的有关特征。概念描述分为特征性描述和区别性描述，前者描述某类对象的共同特征，后者描述不同类对象之间的区别。 偏差检测数据库中的数据常有一些异常记录，从数据库中检测这些偏差很有意义。 数据挖掘技术&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;包括关联分析、序列分析、分类、预测、聚类分析及时间序列分析等。 1.关联分析&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;主要用于发现不同事件之间的关联性，即一个事件发生的同时，另一个事件也经常发生。关联分析的重点在于快速发现那些有实用价值的关联发生的事件。其主要依据是事件发生的概率和条件概率应该符合一定的统计意义。 2．序列分析&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;序列分析技术主要用于发现一定时间间隔内接连发生的事件。这些事件构成一个序列，发现的序列应该具有普遍意义。 3．分类分析&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;分类分析通过分析具有类别的样本的特点，得到决定样本属于各种类别的规则或方法。主要方法有基于统计学的贝叶斯方法、神经网络方法、决策树方法及支持向量机。 4．聚类分析&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;聚类分析是根据物以类聚的原理，将本身没有类别的样本聚集成不同的组，并且对每一个这样的组进行描述的过程。其主要依据是聚到同一个组中的样本应该彼此相似，而属于不同组的样本应该足够不相似。 5．预测&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;预测与分类类似，但预测是根据样本的已知特征估算某个连续类型的变量的取值的过程，而分类则只是用于判别样本所属的离散类别而已。预测常用的技术是回归分析。 6．时间序列&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;分析时间序列分析的是随时间而变化的事件序列，目的是预测未来发展趋势，或者寻找相似发展模式或者是发现周期性发展规律。 数据挖掘的流程 1．明确需求&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;问题定义在开始数据挖掘之前，最先的也是最重要的要求就是熟悉背景知识，弄清用户的需求。缺少了背景知识，就不能明确定义要解决的问题，就不能为挖掘准备优质的数据，也很难正确地解释得到的结果。要想充分发挥数据挖掘的价值，必须对目标有一个清晰明确的定义，即决定到底想干什么。 2．建立数据挖掘库&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;要进行数据挖掘必须收集要挖掘的数据资源。一般建议把要挖掘的数据都收集到一个数据库中，而不是采用原有的数据库或数据仓库。这是因为大部分情况下需要修改要挖掘的数据，而且还会遇到采用外部数据的情况；另外，数据挖掘还要对数据进行各种纷繁复杂的统计分析，而数据仓库可能不支持这些数据结构。 3．分析数据&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;分析数据就是通常所进行的对数据深入调查的过程。从数据集中找出规律和趋势，用聚类分析区分类别，最终要达到的目的就是搞清楚多因素相互影响的、十分复杂的关系，发现因素之间的相关性。 4．调整数据&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过上述步骤的操作，对数据的状态和趋势有了进一步的了解，这时要尽可能对问题解决的要求能进一步明确化、进一步量化。针对问题的需求对数据进行增删，按照对整个数据挖掘过程的新认识组合或生成一个新的变量，以体现对状态的有效描述。 5．模型化&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在问题进一步明确，数据结构和内容进一步调整的基础上，就可以建立形成知识的模型。这一步是数据挖掘的核心环节，一般运用神经网络、决策树、数理统计、时间序列分析等方法来建立模型。 6．评价和解释&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上面得到的模式模型，有可能是没有实际意义或没有实用价值的，也有可能是其不能准确反映数据的真实意义，甚至在某些情况下是与事实相反的，因此需要评估，确定哪些是有效的、有用的模式。评估的一种办法是直接使用原先建立的挖掘数据库中的数据来进行检验，另一种办法是另找一批数据并对其进行检验，再一种办法是在实际运行的环境中取出新鲜数据进行检验。 The best way to escape from your problem is to solve it.]]></content>
      <categories>
        <category>数据仓库</category>
      </categories>
      <tags>
        <tag>数据仓库</tag>
        <tag>数据挖掘</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编译型、解释型、静态、动态语言概念及区别]]></title>
    <url>%2F2019%2F11%2F04%2F%E7%BC%96%E8%AF%91%E5%9E%8B%E3%80%81%E8%A7%A3%E9%87%8A%E5%9E%8B%E3%80%81%E9%9D%99%E6%80%81%E3%80%81%E5%8A%A8%E6%80%81%E8%AF%AD%E8%A8%80%E6%A6%82%E5%BF%B5%E5%8F%8A%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;作为程序员有好几年了，突然发现编译型、解释型、静态、静态类型、动态、动态类型、强类型、弱类型语言目前还没怎么分太清楚，下面来看看这些概念和区别吧。 一、编译型语言和解释型语言1、编译型语言&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;需通过编译器（compiler）将源代码编译成机器码，之后才能执行的语言。一般需经过编译（compile）、链接（linker）这两个步骤。编译是把源代码编译成机器码，链接是把各个模块的机器码和依赖库串连起来生成可执行文件。 优点：编译器一般会有预编译的过程对代码进行优化。因为编译只做一次，运行时不需要编译，所以编译型语言的程序执行效率高。可以脱离语言环境独立运行。 缺点：编译之后如果需要修改就需要整个模块重新编译。编译的时候根据对应的运行环境生成机器码，不同的操作系统之间移植就会有问题，需要根据运行的操作系统环境编译不同的可执行文件。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;代表语言：C、C++、Pascal、Object-C以及最近很火的苹果新语言swift 2、解释型语言&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;解释性语言的程序不需要编译，相比编译型语言省了道工序，解释性语言在运行程序的时候才逐行翻译。 优点：有良好的平台兼容性，在任何环境中都可以运行，前提是安装了解释器（虚拟机）。灵活，修改代码的时候直接修改就可以，可以快速部署，不用停机维护。 缺点：每次运行的时候都要解释一遍，性能上不如编译型语言。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;代表语言：JavaScript、Python、Erlang、PHP、Perl、Ruby 3、混合型语言&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;既然编译型和解释型各有缺点就会有人想到把两种类型整合起来，取其精华去其糟粕。就出现了半编译型语言。比如C#,C#在编译的时候不是直接编译成机器码而是中间码，.NET平台提供了中间语言运行库运行中间码，中间语言运行库类似于Java虚拟机。.net在编译成IL代码后，保存在dll中，首次运行时由JIT在编译成机器码缓存在内存中，下次直接执行（博友回复指出）。我个人认为抛开一切的偏见C#是这个星球上最好的编程语言。可惜微软的政策限制了C#的推广。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Java先生成字节码再在Java虚拟机中解释执行。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;严格来说混合型语言属于解释型语言。C#更接近编译型语言。 二、动态语言和静态语言1、动态语言&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;是一类在运行时可以改变其结构的语言：例如新的函数、对象、甚至代码可以被引进，已有的函数可以被删除或是其他结构上的变化。通俗点说就是在运行时代码可以根据某些条件改变自身结构。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;主要动态语言：Object-C、C#、JavaScript、PHP、Python、Erlang。 2、静态语言&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;与动态语言相对应的，运行时结构不可变的语言就是静态语言。如Java、C、C++。 3、注意：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;很多人认为解释型语言都是动态语言，这个观点是错的！Java是解释型语言但是不是动态语言，Java不能在运行的时候改变自己结构。反之成立吗？动态语言都是解释型语言。也是错的！Object-C是编译型语言，但是他是动态语言。得益于特有的run time机制（准确说run time不是语法特性是运行时环境，这里不展开）OC代码是可以在运行的时候插入、替换方法的。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;C#也是动态语言，通过C#的反射机制可以动态的插入一段代码执行。所以我说C#是这个星球最好的编程语言。 三、动态类型语言和静态类型语言1、动态类型语言&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;很多网上资料把动态类型语言和动态语言混为一谈，简直是误人子弟。动态类型语言和动态语言是完全不同的两个概念。动态类型语言是指在运行期间才去做数据类型检查的语言，说的是数据类型，动态语言说的是运行是改变结构，说的是代码结构。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;动态类型语言的数据类型不是在编译阶段决定的，而是把类型绑定延后到了运行阶段。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;主要语言：Python、Ruby、Erlang、JavaScript、swift、PHP、Perl。 2、静态类型语言&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;静态语言的数据类型是在编译其间确定的或者说运行之前确定的，编写代码的时候要明确确定变量的数据类型。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;主要语言：C、C++、C#、Java、Object-C。 3、注意：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;相当一部分程序员，也包括曾经的我，认为解释型语言都是动态类型语言，编译型语言都是静态类型语言。这个也是错的。swift是编译型语言但是它也是动态类型语言。C#和Java是解释型语言也是静态类型语言。 四、强类型语言和弱类型语言1、强类型语言：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;强类型语言，一旦一个变量被指定了某个数据类型，如果不经过强制类型转换，那么它就永远是这个数据类型。你不能把一个整形变量当成一个字符串来处理。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;主要语言：Java、C#、Python、Object-C、Ruby 2、弱类型语言：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据类型可以被忽略，一个变量可以赋不同数据类型的值。一旦给一个整型变量a赋一个字符串值，那么a就变成字符类型。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;主要语言：JavaScript、PHP、C、C++（C和C++有争议，但是确实可以给一个字符变量赋整形值，可能初衷是强类型，形态上接近弱类型） 3、注意：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;一个语言是不是强类型语言和是不是动态类型语言也没有必然联系。Python是动态类型语言，是强类型语言。JavaScript是动态类型语言，是弱类型语言。Java是静态类型语言，是强类型语言。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果我的理解有误，请在回复中指正，或者邮件我：chenzuoli709@163.com，谢谢。 I believe in me more than anything in this world.]]></content>
      <categories>
        <category>计算机语言</category>
      </categories>
      <tags>
        <tag>编译型语言</tag>
        <tag>解释型语言</tag>
        <tag>静态语言</tag>
        <tag>动态语言</tag>
        <tag>静态类型语言</tag>
        <tag>动态类型语言</tag>
        <tag>强类型语言</tag>
        <tag>弱类型语言</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java和Python语言的区别]]></title>
    <url>%2F2019%2F11%2F04%2FJava%E5%92%8CPython%E8%AF%AD%E8%A8%80%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;计算机语言届，Java和Python在应用占比方面算是前一二了，那么他们之间又有什么区别呢？应用到哪些场景中，下面来看下。 Python比Java简单，学习成本低，开发效率高； Java运行效率高于Python，尤其是纯python开发的程序，效率极低。IO密集型的应用程序中，效率差不多，参考文章：Python解释器GIL详解）； Java相关资料多，尤其是中文资料； Java版本比较稳定，Python2和3不兼容导致很多类库不可用； Java开发偏向于软件工程，团队协同，Python适合于小型开发； Java偏向于商业开发，Python偏向于数据分析； Java是一种静态类型语言，Python是一种动态类型语言； Java中的所有变量需要先声明（类型）再使用，Python变量不需要声明； Java程序编译之后才能运行，Python可以直接运行； Java中的块用大括号包围，Python以冒号和缩进来表示。其实所有语言都是通过缩进来标记块范围的，只是java为了程序看起来更加友好； Java中每行程序需要以分号结束，Python不需要； 实现同一功能时，Java敲键盘的次数比python多； Do you believe in fate?]]></content>
      <categories>
        <category>计算机语言</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用户级线程和内核级线程的区别]]></title>
    <url>%2F2019%2F11%2F03%2F%E7%94%A8%E6%88%B7%E7%BA%A7%E7%BA%BF%E7%A8%8B%E5%92%8C%E5%86%85%E6%A0%B8%E7%BA%A7%E7%BA%BF%E7%A8%8B%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;用户级线程（ULT：User Level Thread）和内核级线程（KLT：Kernel Level Thread）之间有什么关系和区别呢，跟进程呢？下面来看看详细介绍。 引言：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;本文涉及到操作系统的内核模式和用户模式，如果不太懂的话，可以参看看这篇文章：内核模式和用户模式，其中简单的进行了介绍。 1.进程和线程&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;首先说一下线程对于进程的优势，这其实就是线程出现的意义。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;进程是资源拥有的基本单位，进程切换需要保存进程状态，会造成资源的消耗。同一进程中的线程，共享进程获取的部分资源。在同一进程中，线程的切换不会引起进程切换，线程的切换需要的资源少于进程切换，可以提高效率。 2.内核级线程（Kemel-Level Threads, KLT）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;也有叫做内核支持的线程: 线程管理的所有工作（创建和撤销）由操作系统内核完成 操作系统内核提供一个应用程序设计接口API，供开发者使用KLT &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;纯内核级线程特点： 进程中的一个线程被阻塞，内核能调度同一进程的其他线程（就绪态）占有处理器运行 多处理器环境中，内核能同时调度同一进程的多线程，将这些线程映射到不同的处理器核心上，提高进程的执行效率。 应用程序线程在用户态运行，线程调度和管理在内核实现。线程调度时，控制权从一个线程改变到另一线程，需要模式切换，系统开销较大。 3.用户级线程（User-Level Threads ULT）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;用户空间运行线程库，任何应用程序都可以通过使用线程库被设计成多线程程序。线程库是用于用户级线程管理的一个例程包，它提供多线程应用程序的开发和运行支撑环境，包含：用于创建和销毁线程的代码、在线程间传递数据和消息的代码、调度线程执行的代码以及保存和恢复线程上下文的代码。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;所以线程的创建，消息传递，调度，保存/恢复上下文都有线程库来完成。内核感知不到多线程的存在。内核继续以进程为调度单位，并且给该进程指定一个执行状态（就绪、运行、阻塞等）。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;纯用户级线程的特点： 线程切换不需要内核模式，能节省模式切换开销和内核资源。 允许进程按照特定的需要选择不同的调度算法来调度线程。调度算法需要自己实现。 由于其不需要内核进行支持，所以可以跨OS运行。 不能利用多核处理器有点，OS调度进程，每个进程仅有一个ULT能执行 一个ULT阻塞，将导致整个进程的阻塞。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;jacketing技术可以解决ULT一个线程阻塞导致整个进程阻塞。 jacketing的目标是把一个产生阻塞的系统调用转化成一个非阻塞的系统调用。例如,当进程中的一个线程调用IO中断钱，先调用一个应用级的I/O jacket例程，而不是直接调用一个系统I/O。让这个jacket例程检查并确定I/O设备是否忙。如果忙，则jacketing将控制权交给该进程的线程调度程序，决定该线程进入阻塞状态并将控制权传送给另一个线程（若无就绪态线程咋可能执行进程切换）。 4.线程实现的组合策略&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;可以看出，用户级线程和内核级线程都有各自的优点和缺点，在应用上主要表现为： 用户级多线程对于处理逻辑并行性问题有很好的效果。不擅长于解决物理并发问题。 内核级多线程适用于解决物理并行性问题。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;组合策略：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由操作系统内核支持内核级多线程，由操作系统的程序库来支持用户级多线程，线程创建完全在用户空间创建，现成的调度也在应用程序内部进行，然后把用户级多线程映射到（或者说是绑定到）一些内核级多线程。编程人员可以针对不同的应用特点调节内核级线程的数目来达到物理并行性和逻辑并行性的最佳方案。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;附上三种线程模式的图，帮助理解（来自网络，版权不可考，如发现出处可以联系作者删除或者增加版权说明）: 其实这里还有一个轻进程的概念，但是我觉得不写在本文中反而更好理解。另外如果有什么地方写的不好，欢迎大家评论交流。 Believe yourself.]]></content>
      <categories>
        <category>计算机系统</category>
      </categories>
      <tags>
        <tag>用户级线程</tag>
        <tag>内核级线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[什么是IO密集型与CPU密集型？]]></title>
    <url>%2F2019%2F11%2F03%2F%E4%BB%80%E4%B9%88%E6%98%AFIO%E5%AF%86%E9%9B%86%E5%9E%8B%E4%B8%8ECPU%E5%AF%86%E9%9B%86%E5%9E%8B%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;简单来说，不同的应用程序，有的计算量特别大，然后将结果（很小，甚至一个数值结果）写入磁盘，有的读写频率非常高，那么此时我们就应该根据不同的程序类型来设计不同的程序编写方式，选择不同类型的服务器、数据库等。下面来看下IO密集型和CPU密集型的解释吧。 1.CPU密集型（CPU-bound）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CPU密集型也叫计算密集型，指的是系统的硬盘、内存性能相对CPU要好很多，此时，系统运作大部分的状况是CPU Loading 100%，CPU要读/写I/O(硬盘/内存)，I/O在很短的时间就可以完成，而CPU还有许多运算要处理，CPU Loading很高。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在多重程序系统中，大部分时间用来做计算、逻辑判断等CPU动作的程序称之CPU bound。例如一个计算圆周率至小数点一千位以下的程序，在执行的过程当中绝大部分时间用在三角函数和开根号的计算，便是属于CPU bound的程序。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CPU bound的程序一般而言CPU占用率相当高。这可能是因为任务本身不太需要访问I/O设备，也可能是因为程序是多线程实现因此屏蔽掉了等待I/O的时间。 2.IO密集型（I/O bound）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;IO密集型指的是系统的CPU性能相对硬盘、内存要好很多，此时，系统运作，大部分的状况是CPU在等I/O (硬盘/内存) 的读/写操作，此时CPU Loading并不高。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;I/O bound的程序一般在达到性能极限时，CPU占用率仍然较低。这可能是因为任务本身需要大量I/O操作，而pipeline做得不是很好，没有充分利用处理器能力。 3.CPU密集型 vs IO密集型&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们可以把任务分为计算密集型和IO密集型。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;计算密集型任务的特点是要进行大量的计算，消耗CPU资源，比如计算圆周率、对视频进行高清解码等等，全靠CPU的运算能力。这种计算密集型任务虽然也可以用多任务完成，但是任务越多，花在任务切换的时间就越多，CPU执行任务的效率就越低，所以，要最高效地利用CPU，计算密集型任务同时进行的数量应当等于CPU的核心数。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;计算密集型任务由于主要消耗CPU资源，因此，代码运行效率至关重要。Python这样的脚本语言运行效率很低，完全不适合计算密集型任务。对于计算密集型任务，最好用C语言编写。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第二种任务的类型是IO密集型，涉及到网络、磁盘IO的任务都是IO密集型任务，这类任务的特点是CPU消耗很少，任务的大部分时间都在等待IO操作完成（因为IO的速度远远低于CPU和内存的速度）。对于IO密集型任务，任务越多，CPU效率越高，但也有一个限度。常见的大部分任务都是IO密集型任务，比如Web应用。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;IO密集型任务执行期间，99%的时间都花在IO上，花在CPU上的时间很少，因此，用运行速度极快的C语言替换用Python这样运行速度极低的脚本语言，完全无法提升运行效率。对于IO密集型任务，最合适的语言就是开发效率最高（代码量最少）的语言，脚本语言是首选，C语言最差。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;总之，计算密集型程序适合C语言多线程，I/O密集型适合脚本语言开发的多线程。 A man becomes learned by asking questions.]]></content>
      <categories>
        <category>计算机系统</category>
      </categories>
      <tags>
        <tag>IO</tag>
        <tag>CPU</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java多线程与Python多线程性能比较]]></title>
    <url>%2F2019%2F11%2F03%2FJava%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8EPython%E5%A4%9A%E7%BA%BF%E7%A8%8B%E6%80%A7%E8%83%BD%E6%AF%94%E8%BE%83%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于多核的Linux服务器，同一环境下，Java多线程和Python多线程同时进行IO密集型操作，哪个程序会更快一些呢？ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;无论是Java还是Python他们都是真实的线程，那么也就是调用了操作系统的api来创建线程，至于是创建的是内核级还是用户级线程，不得而知。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;下面这张图来看下用户级和内核级线程配置策略： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果是用户级线程，那么线程对于cpu不可见，由jvm或者python解释器来控制线程获取cpu资源的权限。所以说对于同一个进程下创建的多个线程，同一时刻只有一个线程获取了cpu资源，也就是线程只能并发不能并行（关于并发和并行的解释可以参考文章：并发与并行）。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;那么我们平时所说的Java多线程并行处理又是怎么回事？如果多线程只能并发，不能并行，那么无论是单核还是多核，java和python在io密集型的环境下应用性能是差不多的。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果是内核级线程，同一进程下多个线程同一时刻可以获得多个cpu资源，这样就实现了并发，这个就可以说通了，而python解释器GIL（关于python GIL的解释，可以参考文章：Python解释器GIL详解）中的线程锁将python线程控制，使得同一时刻只有一个线程获得cpu资源的权限，所以，在cpu密集型的环境或者是多核系统下，java多线程明显比python多线程性能强。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;IO密集型和Cpu密集型的介绍可以参考文章：什么是IO密集型与CPU密集型？ Good health is over wealth.]]></content>
      <categories>
        <category>计算机语言</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Python</tag>
        <tag>多线程</tag>
        <tag>IO密集型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一般情况下为什么Python多线程比单线程还慢？]]></title>
    <url>%2F2019%2F11%2F03%2F%E4%B8%80%E8%88%AC%E6%83%85%E5%86%B5%E4%B8%8B%E4%B8%BA%E4%BB%80%E4%B9%88Python%E5%A4%9A%E7%BA%BF%E7%A8%8B%E6%AF%94%E5%8D%95%E7%BA%BF%E7%A8%8B%E8%BF%98%E6%85%A2%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Python中的多线程之间没有很好的并发操作，导致了单线程比多线程快。下面来看看具体详解。 1.GIL是什么&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;首先需要明确的一点是GIL并不是Python的特性，它是在实现Python解析器(CPython)时所引入的一个概念。就好比C++是一套语言（语法）标准，但是可以用不同的编译器来编译成可执行代码。有名的编译器例如GCC，INTEL C++，Visual C++等。Python也一样，同样一段代码可以通过CPython，PyPy，Psyco等不同的Python执行环境来执行。像其中的JPython就没有GIL。然而因为CPython是大部分环境下默认的Python执行环境。所以在很多人的概念里CPython就是Python，也就想当然的把GIL归结为Python语言的缺陷。所以这里要先明确一点：GIL并不是Python的特性，Python完全可以不依赖于GIL。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;那么CPython实现中的GIL又是什么呢？GIL全称Global Interpreter Lock为了避免误导，我们还是来看一下官方给出的解释： In CPython, the global interpreter lock, or GIL, is a mutex that prevents multiple native threads from executing Python bytecodes at once. This lock is necessary mainly because CPython’s memory management is not thread-safe. (However, since the GIL exists, other features have grown to depend on the guarantees that it enforces.) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;好吧，是不是看上去很糟糕？一个防止多线程并发执行机器码的一个Mutex，乍一看就是个BUG般存在的全局锁嘛！别急，我们下面慢慢的分析。 2.为什么会有GIL&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由于物理上的限制，各CPU厂商在核心频率上的比赛已经被多核所取代。为了更有效的利用多核处理器的性能，就出现了多线程的编程方式，而随之带来的就是线程间数据一致性和状态同步的困难。即使在CPU内部的Cache也不例外，为了有效解决多份缓存之间的数据同步时各厂商花费了不少心思，也不可避免的带来了一定的性能损失。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Python当然也逃不开，为了利用多核，Python开始支持多线程。而解决多线程之间数据完整性和状态同步的最简单方法自然就是加锁。 于是有了GIL这把超级大锁，而当越来越多的代码库开发者接受了这种设定后，他们开始大量依赖这种特性（即默认python内部对象是thread-safe的，无需在实现时考虑额外的内存锁和同步操作）。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;慢慢的这种实现方式被发现是蛋疼且低效的。但当大家试图去拆分和去除GIL的时候，发现大量库代码开发者已经重度依赖GIL而非常难以去除了。有多难？做个类比，像MySQL这样的“小项目”为了把Buffer Pool Mutex这把大锁拆分成各个小锁也花了从5.5到5.6再到5.7多个大版为期近5年的时间，本且仍在继续。MySQL这个背后有公司支持且有固定开发团队的产品走的如此艰难，那又更何况Python这样核心开发和代码贡献者高度社区化的团队呢？ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;所以简单的说GIL的存在更多的是历史原因。如果推到重来，多线程的问题依然还是要面对，但是至少会比目前GIL这种方式会更优雅。 3.GIL的影响&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从上文的介绍和官方的定义来看，GIL无疑就是一把全局排他锁。毫无疑问全局锁的存在会对多线程的效率有不小影响。甚至就几乎等于Python是个单线程的程序。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;那么读者就会说了，全局锁只要释放的勤快效率也不会差啊。只要在进行耗时的IO操作的时候，能释放GIL，这样也还是可以提升运行效率的嘛。或者说再差也不会比单线程的效率差吧。理论上是这样，而实际上呢？Python比你想的更糟。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;下面我们就对比下Python在多线程和单线程下得效率对比。测试方法很简单，一个循环1亿次的计数器函数。一个通过单线程执行两次，一个多线程执行。最后比较执行总时间。测试环境为双核的Mac pro。注：为了减少线程库本身性能损耗对测试结果带来的影响，这里单线程的代码同样使用了线程。只是顺序的执行两次，模拟单线程。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;顺序执行的单线程(single_thread.py) 1234567891011121314151617181920212223#! /usr/bin/python from threading import Threadimport time def my_counter(): i = 0 for _ in range(100000000): i = i + 1 return True def main(): thread_array = &#123;&#125; start_time = time.time() for tid in range(2): t = Thread(target=my_counter) t.start() t.join() end_time = time.time() print(&quot;Total time: &#123;&#125;&quot;.format(end_time - start_time)) if __name__ == &apos;__main__&apos;: main() &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;同时执行的两个并发线程(multi_thread.py) 12345678910111213141516171819202122232425#! /usr/bin/python from threading import Threadimport time def my_counter(): i = 0 for _ in range(100000000): i = i + 1 return True def main(): thread_array = &#123;&#125; start_time = time.time() for tid in range(2): t = Thread(target=my_counter) t.start() thread_array[tid] = t for i in range(2): thread_array[i].join() end_time = time.time() print(&quot;Total time: &#123;&#125;&quot;.format(end_time - start_time)) if __name__ == &apos;__main__&apos;: main() &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;下图就是测试结果: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;可以看到python在多线程的情况下居然比单线程整整慢了45%。按照之前的分析，即使是有GIL全局锁的存在，串行化的多线程也应该和单线程有一样的效率才对。那么怎么会有这么糟糕的结果呢？ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;让我们通过GIL的实现原理来分析这其中的原因。 4.当前GIL设计的缺陷1.基于pcode数量的调度方式&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;按照Python社区的想法，操作系统本身的线程调度已经非常成熟稳定了，没有必要自己搞一套。所以Python的线程就是C语言的一个pthread，并通过操作系统调度算法进行调度（例如linux是CFS）。为了让各个线程能够平均利用CPU时间，python会计算当前已执行的微代码数量，达到一定阈值后就强制释放GIL。而这时也会触发一次操作系统的线程调度（当然是否真正进行上下文切换由操作系统自主决定）。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;伪代码: 123456while True: acquire GIL for i in 1000: do something release GIL /* Give Operating System a chance to do thread scheduling */ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这种模式在只有一个CPU核心的情况下毫无问题。任何一个线程被唤起时都能成功获得到GIL（因为只有释放了GIL才会引发线程调度）。但当CPU有多个核心的时候，问题就来了。从伪代码可以看到，从release GIL到acquire GIL之间几乎是没有间隙的。所以当其他在其他核心上的线程被唤醒时，大部分情况下主线程已经又再一次获取到GIL了。这个时候被唤醒执行的线程只能白白的浪费CPU时间，看着另一个线程拿着GIL欢快的执行着。然后达到切换时间后进入待调度状态，再被唤醒，再等待，以此往复恶性循环。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;PS：当然这种实现方式是原始而丑陋的，Python的每个版本中也在逐渐改进GIL和线程调度之间的互动关系。例如先尝试持有GIL在做线程上下文切换，在IO等待时释放GIL等尝试。但是无法改变的是GIL的存在使得操作系统线程调度的这个本来就昂贵的操作变得更奢侈了。 2.关于GIL影响的扩展阅读&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了直观的理解GIL对于多线程带来的性能影响，这里直接借用的一张测试结果图（见下图）。图中表示的是两个线程在双核CPU上得执行情况。两个线程均为CPU密集型运算线程。绿色部分表示该线程在运行，且在执行有用的计算，红色部分为线程被调度唤醒，但是无法获取GIL导致无法进行有效运算等待的时间。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由图可见，GIL的存在导致多线程无法很好的立即多核CPU的并发处理能力。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;那么Python的IO密集型线程能否从多线程中受益呢？我们来看下面这张测试结果。颜色代表的含义和上图一致。白色部分表示IO线程处于等待。可见，当IO线程收到数据包引起终端切换后，仍然由于一个CPU密集型线程的存在，导致无法获取GIL锁，从而进行无尽的循环等待。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;简单的总结下就是：Python的多线程在多核CPU上，只对于IO密集型计算产生正面效果；而当有至少有一个CPU密集型线程存在，那么多线程效率会由于GIL而大幅下降。 5.如何避免受到GIL的影响&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;说了那么多，如果不说解决方案就仅仅是个科普帖，然并卵。GIL这么烂，有没有办法绕过呢？我们来看看有哪些现成的方案。 1.用multiprocess替代Thread&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;multiprocess库的出现很大程度上是为了弥补thread库因为GIL而低效的缺陷。它完整的复制了一套thread所提供的接口方便迁移。唯一的不同就是它使用了多进程而不是多线程。每个进程有自己的独立的GIL，因此也不会出现进程之间的GIL争抢。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当然multiprocess也不是万能良药。它的引入会增加程序实现时线程间数据通讯和同步的困难。就拿计数器来举例子，如果我们要多个线程累加同一个变量，对于thread来说，申明一个global变量，用thread.Lock的context包裹住三行就搞定了。而multiprocess由于进程之间无法看到对方的数据，只能通过在主线程申明一个Queue，put再get或者用share memory的方法。这个额外的实现成本使得本来就非常痛苦的多线程程序编码，变得更加痛苦了。具体难点在哪有兴趣的读者可以扩展阅读这篇文章 2.用其他解析器&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;之前也提到了既然GIL只是CPython的产物，那么其他解析器是不是更好呢？没错，像JPython和IronPython这样的解析器由于实现语言的特性，他们不需要GIL的帮助。然而由于用了Java/C#用于解析器实现，他们也失去了利用社区众多C语言模块有用特性的机会。所以这些解析器也因此一直都比较小众。毕竟功能和性能大家在初期都会选择前者，Done is better than perfect。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;所以没救了么？ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当然Python社区也在非常努力的不断改进GIL，甚至是尝试去除GIL。并在各个小版本中有了不少的进步。有兴趣的读者可以扩展阅读这个Slide 另一个改进Reworking the GIL: 将切换颗粒度从基于opcode计数改成基于时间片计数 避免最近一次释放GIL锁的线程再次被立即调度 新增线程优先级功能（高优先级线程可以迫使其他线程释放所持有的GIL锁） 6.总结&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Python GIL其实是功能和性能之间权衡后的产物，它尤其存在的合理性，也有较难改变的客观因素。从本分的分析中，我们可以做以下一些简单的总结： 因为GIL的存在，只有IO Bound场景下得多线程会得到较好的性能 如果对并行计算性能较高的程序可以考虑把核心部分也成C模块，或者索性用其他语言实现 GIL在较长一段时间内将会继续存在，但是会不断对其进行改进。 You should ask why more.]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>thread</tag>
        <tag>GIL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[闪电网络是如何提高比特币网络的交易速度的？]]></title>
    <url>%2F2019%2F10%2F31%2F%E9%97%AA%E7%94%B5%E7%BD%91%E7%BB%9C%E6%98%AF%E5%A6%82%E4%BD%95%E6%8F%90%E9%AB%98%E6%AF%94%E7%89%B9%E5%B8%81%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BA%A4%E6%98%93%E9%80%9F%E5%BA%A6%E7%9A%84%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;闪电网络是依附在区块链主链上的区块链状态通道，类似于主链的网络缓存，通过智能合约功能以支持跨参与者网络的即时付款，同时利用区块链的特性消除将资金托管给第三方带来的风险。主要用于即时、高频的支付，它的设计初衷就是希望比特币的交易可以提升效率，快如闪电。 一、缘起扩容&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;比特币在设计之初，每个区块容量的大小被限定为1MB，而1MB的比特币区块每秒大约可以处理7笔交易，这个处理速度，就当时比特币的交易量而言，1MB的区块容量是足够使用的。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;但从2013年开始，随着比特币价格的水涨船高，比特币的用户规模日益扩大，1MB的区块容量已经无法支持比特币网络进行快速交易确认，造成比特币网络拥堵。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;试想，你去便利店买桶泡面，你把泡面泡好了，吃完了，支付都还没有完成，这是一种怎样的体验？更离谱的是，当很多人跟你一样都在进行支付时，你还得支付非常高昂的手续费。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;因为矿工是根据手续费的高低来决定打包支付的优先级的，如果你给的手续费太低，别说吃完泡面的时候能不能支付完成，等到第二天吃泡面的时候，再看看有没有完成吧。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2017年底，因为比特币币价大涨，导致交易量激增，那时的手续费最高曾达到过每笔55美元，折合人民币300多块。也就是说，你买了一个8块钱的泡面，为了付钱，还要另外支付300块钱给矿工。可能，你也吃不下这碗泡面了！&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;所以，人们开始讨论如何给比特币“扩容”的问题。 二、比特币的扩容方案1.链上扩容&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;什么意思呢？就是直接扩大区块容量，1MB不够，那就2MB，2MB不够，那就10MB，管够为止。只是这样的扩容方式，会对网络的存储和运输带来很大的压力，区块是够了，宽带和硬盘容量不够了。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;举个例子，有一家叫做比特币的银行，由于人手有限（区块容量小），一秒钟只能处理七笔业务，而这家银行的生意又很好，每天都有成千上万笔业务需要处理，然后这家银行又有一个规定，每笔业务都必须经过柜台操作，导致效率很低，每一位想要办理业务的人通常都要等待很久，长此以往，肯定会有人不愿意等那么久，致使比特币银行的生意受到影响。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;于是，比特币银行开始考虑如何提升效率？最简单粗暴的办法，就是招人（区块扩容），扩大组织规模，把1个柜台变成8个。不够用？那就32个，100个。可是招那么多人，办公室不够用了。 2.链下扩容（相当于节流）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;就是把一部分交易区分出来，这些交易的交易过程不再直接记录在区块链之中，而是以缓存的方式进行记录，区块链中只记录其交易结果。以此来减少每一个区块需要记录的信息量。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;还是比特币银行的例子，柜台有很多要处理的业务，导致效率低下，然后银行引入了ATM机，规定低于两万的业务，大家可以在ATM机上自助办理小额业务，需要到柜台办理的业务减少了，效率自然也就提升了。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;闪电网络就是比特币链下扩容方案的一种，从理论上来说，利用这项技术，可以提升比特币的交易速度。 三、技术实现&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;那么，闪电网络是如何实现交易提速的呢？ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里我们需要搞清楚两个关键性的技术：序列到期可撤销合约RSMC（Revocable Sequence Maturity Contract）和哈希时间锁定合约HTLC（Hashed Timelock Contract）。 1.RSMC ：Revocable Sequence Maturity Contract，序列到期可撤销合约&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;RSMC，按照专业术语来讲，是一个资金池的机制。这有点像两个人私下记账，而且不能违反承诺，否则会受到惩罚。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;举个例子，在比特币银行里，张三和李四有一本共有的存折（资金池），为了交易便捷，他们两人都会先预存100元到这个存折里，这时这张存折的最大交易金额就是200元。每一次发生交易，张三和李四都需要对这200元钱的分配结果共同进行签字（数字签名和时间戳）确认，同时把之前交易后的分配方案作废。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;示例如下： 第一笔交易：张三支付给李四50元，则分配结果为张三50元，李四150元，然后张三和李四共同对这一分配结果在存折上签字确认。 第二笔交易：李四支付给张三20元，则分配结果为张三70元，李四130元，然后张三和李四共同对这一分配结果在存折上签字确认，同时废除第一笔交易的分配结果。 第三笔交易：张三再支付给李四10元，则分配结果为张三60元，李四140元，然后张三和李四共同对这一分配结果在存折上签字确认，同时废除第二笔交易的分配结果。 之后，如果张三或者李四需要把最后的分配结果进行提现，就需要把经他们双方共同签字确认过的分配结果拿去柜台登记(录入区块链)。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这一整个过程，只有在提现的时候，才需要把张三和李四的余额结果录入到区块链当中，既减少了柜台的业务量，还减免了每一次在柜台办理业务的手续费。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;需要注意的是，任何一笔交易的分配结果都需要经过双方的签名确认才算是合法的，而在提现时，需要提供最后一笔交易，也就是时间戳最新的一笔交易的双方签名。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这样的设置，是为了防止其中一方拿着已经作废的交易结果去提现。比如说，张三和李四，明明进行了第三笔交易（张三支付了10元给李四），但是张三为了多提现10元，把第二笔交易的交易结果拿去柜台要求提现。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这时，比特币银行会先将李四的钱打到李四的个人账户上，并发送短信提示到账金额为130元，然后李四就会知道张三在提现时动了手脚。于是，李四拿着第三笔交易（张三余额60元，李四余额140元）的双方签名提交给比特币银行，比特币银行就会判定张三作恶，将归张三所有的60元余额罚归李四所有。这就是闪电网络“先提现后到账”以及“作恶罚没”的设计。 2.HTLC：Hashed Timelock Contract，哈希时间锁定合约&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;RSMC解决了两个人或者说是两个点之间的交易往来，那么，如果是10个人，100个人需要进行交易，那不可能每一个人都要准备100个与另一个人共有的存折吧。于是，HTLC出现了。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;HTLC是一个限时转账的机制，就是转账方先把转出的钱进行冻结，再提供给收款方一个哈希值，在一定时间之内，收款方通过能提供出与这个哈希值相匹配的值，从而获得这笔钱。怎么用呢？&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在比特币银行里，张三和李四有一个共有的存折，李四和王五也有一个共有的存折，这时，张三需要给王五转账，可以由张三先转给李四，再由李四转给王五，那么张三和王五就不用再开一个共有存折了。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;张三通过智能合约将要转给王五的1000元进行冻结，并发送一个哈希值给王五，然后张三在朋友圈发布公告，“如果谁能给我一个暗语（与哈希值相匹配的值）”，我就给谁一笔钱。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;李四收到了张三的公告，也在朋友圈公告，“如果谁能给我一个暗语（与哈希值相匹配的值）”，我就给谁一笔钱。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这时，王五把暗语提供给李四后，李四就需要转1000元给王五，然后王五再拿着这个暗语去找张三拿钱。从而实现张三和王五的转账。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这个过程当中，李四充当了一个中转站的作用。当然在这个过程当中，充当了中转站的李四需要得到一定的激励，这个过程才能得以实现。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;现在清楚了吧，如果有什么不懂的地方，欢迎邮件我：chenzuoli709@163.com Happy Halloween Festival.]]></content>
      <categories>
        <category>比特币</category>
      </categories>
      <tags>
        <tag>比特币</tag>
        <tag>闪电网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Solidity中函数修改器modifier详解]]></title>
    <url>%2F2019%2F10%2F30%2FSolidity%E4%B8%AD%E5%87%BD%E6%95%B0%E4%BF%AE%E6%94%B9%E5%99%A8modifier%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;modifier即函数的修改器，可以用来改变一个函数的行为，控制函数的逻辑。修改器是一种合约属性，可以被继承和重写。 1.函数修改器&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;下面以代码为例进行介绍（代码来源于CryptoKitties项目KittyAccessControl.sol合约，详细代码可以查看https://github.com/dapperlabs/cryptokitties-bounty) 12345678modifier onlyCLevel() &#123; require( msg.sender == cooAddress || msg.sender == ceoAddress || msg.sender == cfoAddress ); _;&#125; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;前面一段代码是一个修改器，声明了一个约束onlyClevel：仅当当前的地址为ceoAddress或者cfoAddress或者cooAddress时，可以执行后续代码。下划线_是一个占位符，代表了执行函数接下来的代码。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;有时你还会看到上面那段代码写成如下形式： 12345678modifier onlyCLevel() &#123; if( msg.sender != cooAddress &amp;&amp; msg.sender != ceoAddress &amp;&amp; msg.sender != cfoAddress ) throw; _;&#125; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其实两段代码是等价的，if() throw的写法是较为老式的写法，现在使用require()的写法较多。 123function pause() external onlyClevel whenNotPaused &#123; paused = true; &#125; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;接下来一段代码声明了一个函数pause()，用于暂停合约，这里使用了onlyClevel约束，表明该函数的执行必须要满足onlyClevel条件。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;此外函数修改器也支持传入参数，和函数的参数类似，例如： 1234567891011121314pragma solidity ^0.4.0;contract parameter&#123; uint balance = 10; modifier lowerLimit(uint _balance, uint __withdraw)&#123; if( _withdraw &lt; 0 || _withdraw &gt; _balance) throw; _; &#125; function f(uint withdraw) lowerLimit(balance, withdraw) returns (uint)&#123; return balance; &#125;&#125; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在上面这段代码中，修改器lowerLimit传入两个参数，执行修改器的逻辑。函数的执行与否取决于两个参数：_withdraw和_balance。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;函数的修改器参数支持表达式，例如： 12345678910111213141516pragma solidity ^0.4.0;contract parameterExpression&#123; modifier m(uint a)&#123; if(a &gt; 0) _; &#125; function add(uint a, uint b) private returns(uint)&#123; return a + b; &#125; function f() m(add(1, 1)) returns(uint)&#123; return 1; &#125;&#125; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于上面的这段代码，修改器m的参数传入了一个表达式：add(a+b)，add()表达式在合约中定义了。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Return用在函数中表示返回值，如果函数有返回值标志return，但是由于修改器限制，判断不成功，无法执行函数体内的代码，那么将会返回返回值类型的默认值，例如： 1234567891011121314151617pragma solidity ^0.4.0;contract Return&#123; modifier a()&#123; if(false) _; &#125; function uintReturn() A returns(uint)&#123; uint a = 0; return uint(1); &#125; function stringReturn() A returns(string)&#123; return &quot;Hello World&quot;; &#125;&#125; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于上面的代码，由于修改器A永远判断不成功，所以uintReturn和stringReturn两个函数的函数体永远无法执行，那么返回值分别是uint和string的默认值：0和空串。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;函数修改器允许使用return;来中断当前流程，但是不允许明确的return值，也就是说在修改器中只能存在return； &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于函数修改器，下划线代表函数体，当执行到下划线;这一行的时候，就跳到函数体，执行函数体内的语句，执行完函数体内语句其实还要回到函数修改器，执行下划线_;后面的语句，例如： 123456789101112131415161718pragma solidity ^0.4.0;contract processFlow&#123; mapping(bool =&gt; uint) public mapp; modifier A(mapping(bool =&gt; uint) mapp)&#123; if(mapp[true] == 0)&#123; mapp[true] = 1; _; mapp[true] = 3; &#125; &#125; function f() A(mapp) returns(uint)&#123; mapp[true] = 2; return mapp[true]; &#125;&#125; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于上面的函数f()，先运行修改器，判断权限，通过权限，则执行修改器判断语句后面的代码：map[true] = 1;，当运行到下划线;这一行的时候，跳到函数体内执行map[true] = 2;，然后retrun map[true]的值。执行完函数体的代码会回到函数修改器下划线；这一行后面的代码，这里还有代码，接着执行map[true] = 3。所以最终map[true]的值为3。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于一个函数可以有多个修改器限制，在函数定义的时候依次写上，并用加空格分隔，执行的时候也是依次执行。多个修改器是同时限制，也就是说必须满足所有修改器的权限，才可以执行函数体的代码，例如： 12345678910111213141516171819pragma solidity ^0.4.0;contract multiModifier&#123; address owner = msg.sender; modifier onlyOwner&#123; if(msg.sender != owner) throw; _; &#125; modifier inState(bool state)&#123; if(!state) throw; _; &#125; function f(bool state) onlyOwner inState(state) returns(uint)&#123; return 1; &#125;&#125; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上面这段代码中两个修改器onlyOwner和inState同时作用于函数f()，只有当两个修改器的权限同时满足的时候，才会执行函数体内的代码，retrun 1。 2.重写修改器&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们知道合约是可以继承的，在子合约中我们还可以对父合约中的修改器进行重写覆盖，例如： 12345678910111213141516171819pragma solidity ^0.4.0;contract bank&#123; modifier transferLimit(uint _withdraw)&#123; if(_withdraw &gt; 100) throw; _; &#125;&#125;contract ModifierOverride is bank&#123; modifier transferLimit(uint _withdraw)&#123; if(_withdraw &gt; 10) throw; _; &#125; function f(uint withdraw) transferLimit(withdraw) returns(uint)&#123; return withdraw; &#125;&#125; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上面这段代码中，子合约ModifierOverride继承了父合约bank，那么同样有父合约中的transferLimit修改器，之后在子合约中再次定义transferLimit修改器，就重写了该修改器并覆盖了原修改器，在子合约中transferLimit的限制条件由子合约中重写的条件决定。 Never regret a day in your life, good days give you happiness, bad days give you experiences.]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>智能合约</tag>
        <tag>solidity</tag>
        <tag>modifier</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Solidity中now和block.timestamp的区别]]></title>
    <url>%2F2019%2F10%2F28%2FSolidity%E4%B8%ADnow%E5%92%8Cblock-timestamp%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在智能合约中，我们如何获取当前时间呢？下面来看看。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;now 的别称是 block.timestamp ，哈哈，他们是一个意思。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;答案可以参考这里：Ethereum StackexchangeSolidity Readthedocs Failure is the mother of the success.]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>Solidity</tag>
        <tag>now</tag>
        <tag>block</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Solidity中transfer和send的区别]]></title>
    <url>%2F2019%2F10%2F28%2FSolidity%E4%B8%ADtransfer%E5%92%8Csend%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;智能合约中我们可以根据不同的场景来使用这2中方式。下面来看看解释。 1. address.transfer() throws on failure // 转账失败会抛出异常 forwards 2,300 gas stipend (not adjustable), safe against reentrancy should be used in most cases as it’s the safest way to send ether 2. address.send() returns false on failure // 转账失败会返回false forwards 2,300 gas stipend (not adjustable), safe against reentrancy should be used in rare cases when you want to handle failure in the contract 3. address.call.value().gas()() returns false on failure forwards all available gas (adjustable), not safe against reentrancy should be used when you need to control how much gas to forward when sending ether or to call a function of another contract &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;函数返回值就是transfer和send的最主要区别，生产中用transfer比较多。 Where there is a life, there is a hope.]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>Solidity</tag>
        <tag>transfer</tag>
        <tag>send</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive经典sql之销售数据统计]]></title>
    <url>%2F2019%2F10%2F28%2FHive%E7%BB%8F%E5%85%B8sql%E4%B9%8B%E9%94%80%E5%94%AE%E6%95%B0%E6%8D%AE%E7%BB%9F%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;现在有这样一个需求：一张表sales中记录了公司每个业务销售的每月销售数据，来分析下2个案例。 1.sales表结构12345678910| month | user_id | amount |+--------+---------+--------+| 201601 | 1 | 500 || 201601 | 2 | 300 || 201601 | 3 | 100 || 201602 | 1 | 1000 || 201602 | 2 | 800 || 201603 | 2 | 1000 || 201603 | 3 | 500 || 201604 | 1 | 1000 | 2.需求一：求每个月的销售额及累计销售额开窗函数实现： 123456SELECT month, SUM(amount) AS cur_month_amount, SUM(SUM(amount)) OVER (ORDER BY month) FROM sales GROUP BY month ORDER BY month; 或者 12345SELECT month, SUM(amount) OVER(PARTITION BY month) AS cur_month_amount, SUM(amount) OVER(ORDER BY month) AS sum_amountFROM sales; 3.需求二：每个月的销售冠军和销售额开窗函数实现： 12345SELECT month, user_id, MAX(SUM(amount)) OVER(PARTITION BY month, user_id) AS cur_month_top_amountFROM sales; 或者 12345678910111213SELECT month, user_id, amountFROM ( SELECT month, user_id, amount, ROW_NUMBER() OVER(PARTITION BY month ORDER BY amount DESC) AS rank FROM sales) awhere rank = 1; How time flies.]]></content>
      <categories>
        <category>数据仓库</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive经典sql之花费当月费用与总费用]]></title>
    <url>%2F2019%2F10%2F28%2FHive%E7%BB%8F%E5%85%B8sql%E4%B9%8B%E8%8A%B1%E8%B4%B9%E5%BD%93%E6%9C%88%E8%B4%B9%E7%94%A8%E4%B8%8E%E6%80%BB%E8%B4%B9%E7%94%A8%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;现在有这样一个需求：一张表orders中记录了每个用户消费的明细（时间、金额），现在需要求出每个用户当月的消费总金额和总的消费总金额。如何求呢？ 1.orders表结构123name stringdt dateamount double 2.分析&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;统计的当月总金额这个汇总值，那么对name和month进行分组sum(amount)即可得到。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;统计总金额，那么只需对name进行分组即可。 方法一分别求出上面2个汇总值，然后进行join就行： 123456789101112131415161718192021select name, month_amount, sum_amountfrom ( select name, month(dt) as cur_month, sum(amount) as month_amount from orders group by name, month(dt);) t1join( select name, sum(amount) as sum_amount from orders group by name) t2on t1.name = t2.name; 方法二使用开窗函数，不在需要表join了： 12345select name, sum(amount) over(partition by name, month(dt)) as month_amount, sum(amount) over(partition by name) as sum_amountfrom orders &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;开窗函数是不是简单多了？ If somethings in life brings you joy, then you simple have to do it, regardless of what people say.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;—《海蒂与爷爷》]]></content>
      <categories>
        <category>数据仓库</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[电影推荐]]></title>
    <url>%2F2019%2F10%2F27%2F%E7%94%B5%E5%BD%B1%E6%8E%A8%E8%8D%90%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;下面简单介绍下我看过的好看的电影，推荐给你们，我会不定期更新列表。 驭风少年&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;讲了一个面对饥荒，饿殍遍野，面对生存危机，对知识渴望的少年，爱思考的少年，凭借着学校学到的知识，加上自己不断努力，给家人，给国家的人民带来水，带来电，带来粮食，挽救生命的故事。 绿皮书&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;20世纪60年代的美国，种族歧视严重，面对这种情况，顶级黑人高级钢琴师不顾侮辱，忍受不一样的对待，巡演美国，想借此机会改变美国人对待黑人的态度，聘请了半个美国人的司机，一路遇到麻烦，解决麻烦，他的态度从嘲讽转变到崇拜，从反感转变到友善。 绿里奇迹&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;善有善报，恶有恶报。原来美国也有这样的文化。 恶人传&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;警察与黑帮团结一起寻找杀人狂：恶人其实是好人。 罗小黑战纪&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;人与妖的战争：和平共处方能维持生态平衡，暴力解决不了问题。 传奇的诞生&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;球王是怎样练成的：不要在乎他人的看法，用成语邯郸学步很贴切，做自己才是最重要的。 触不可及（法国）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;黑人与很富有的残疾人的故事。 飓风营救（1/2/3）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;老爸救女儿的故事：主演连姆·尼森一生只做一件事，那就是保护家人。 海蒂与爷爷&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;爷爷与孙女的故事：小女孩的微笑超甜美，你要活得像她一样开心哟。 人生就像一场电影，而你就是主角，请开始你的表演。]]></content>
      <categories>
        <category>娱乐</category>
      </categories>
      <tags>
        <tag>电影</tag>
        <tag>English</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Solidity常用关键字详解]]></title>
    <url>%2F2019%2F10%2F27%2FSolidity%E5%B8%B8%E7%94%A8%E5%85%B3%E9%94%AE%E5%AD%97%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;最近在学习公链中的智能合约语言Solidity，下面来看看自带关键字的一些解释吧。 1.require&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;若require函数的第一个参数执行结果为false，则终止执行，撤销所有对状态和以太坊余额的改动，在旧版的EVM中会消耗所有gas，但现在不会了，你也可以在函数的第二个参数中对错误进行解释。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;使用require来检查函数是否被正确调用，是一个好习惯。 1234require( msg.sender == chairman, &quot;Only chairman can give the right to vote.&quot;); 2.payable&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;函数修饰符：payable关键字，如果一个函数需要进行货币操作，必须要带上payable关键字，这样才能正常接收msg.value。 3.msg.sender/msg.owner/tx.origin&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;参考下这篇文章：Difference between msg.owner and msg.sender and tx.origin 4.msg.value&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;默认为给合约转账的金额。 5.this.balance&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当前合约中的余额。 6.modifier&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;参考下这篇文章：Solidity中函数修改器modifier详解 7.now/block.timestamp&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;参考下这篇文章：Solidity中now和block-timestamp的区别 The best way to escape from your problem is to solve it.]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>智能合约</tag>
        <tag>solidity</tag>
        <tag>关键字</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[智能合约语言solidity中msg.sender,msg.owner和tx.origin区别]]></title>
    <url>%2F2019%2F10%2F27%2F%E6%99%BA%E8%83%BD%E5%90%88%E7%BA%A6%E8%AF%AD%E8%A8%80solidity%E4%B8%ADmsg-sender-msg-owner%E5%92%8Ctx-origin%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;智能合约solidity语言中自带的变量msg.sender, msg.owner, tx.origin是什么意思呢？来看看。 1.Difference between msg.owner and msg.sender?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当部署合约时，msg.sender是合约的所有者，如果合约中定义了一个名为“owner”的变量，则可以为其分配值（地址）msg.sender： 1address owner = msg.sender; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;此时，变量“owner”将始终具有最初部署合约的人的地址，意味着是合约的所有者。分析这样一行合约代码：owner.transfer(msg.value)，这里如果调用了回退函数（fallback函数），那么msg.value将在owner的地址传输。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;一个合约的msg.sender是当前与合约交互的地址，可以是用户也可以是另一个合约。所以，如果是一个用户和合约交互，msg.sender是该用户的地址；相反，如果是另一个合约B与该合约交互，msg.sender则是合约B的地址。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;总的来说，msg.sender将会是当前与某合约交互的用户或另一个合约的地址。 2.Difference between msg.owner and tx.origin?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;例如： 123function setOwner() &#123; owner = msg.sender;&#125; VS 123function setOwner() &#123; owner = tx.origin;&#125; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;可以这样说，msg.sender的owner可以是一个合约；而tx.origin的owner不可能是一个合约。在一个简单的调用链中，A-&gt;B-&gt;C-&gt;D，D里面的msg.sender将会是C，而tx.origin将一直是A。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;但是仔细考虑一下是否真的需要使用tx.origin。因为你可能不是合约的唯一用户。其他人可能想要使用你的合约，并希望通过其他的合约与之交互。因为在Solidity中，tx.origin它遍历整个调用栈并返回最初发送调用（或事务）的帐户的地址。在智能合约中使用此变量进行身份验证会使合约容易受到类似网络钓鱼的攻击。 3.参考Difference between msg.owner and msg.sender and tx.origin I adopt a dog today, it’s great.]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>智能合约</tag>
        <tag>msg</tag>
        <tag>sender</tag>
        <tag>owner</tag>
        <tag>origin</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HQL经典sql之课程总数与总价格]]></title>
    <url>%2F2019%2F10%2F26%2FHQL%E7%BB%8F%E5%85%B8sql%E4%B9%8B%E8%AF%BE%E7%A8%8B%E6%80%BB%E6%95%B0%E4%B8%8E%E6%80%BB%E4%BB%B7%E6%A0%BC%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;有这样一个问题，关于培训班学生的统计分析数据，一起来看看吧。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;有三张表： t1为学生课程表：sid（学生ID）、courseid（课程ID）、date（报课时间） t2为学生基本信息表：sid、name、age…… t3为课程基本信息表：courseid、price、date现在想要查看一个培训学校目前每个学生的所报的所有课程数和花费的总金额，如何求呢？12345678910select sid, count(courseid) over(partition by sid) as count, sum(price) over(partition by sid) as sumpricefrom ( select sid, t1.courseid as courseid, price from t1, t3 where t1.courseid = t3.courseid) a; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;分析： 首先关联出学生所报课程的价格； 1234select sid, t1.courseid as courseid, pricefrom t1, t3where t1.courseid = t3.courseid 使用开窗函数对sid分组得到学生报的每门课程和没门课程对应的价格，然后使用count和sum求得想要的指标； I wish i had two dogs, one would be given to my parents, one would be with me.]]></content>
      <categories>
        <category>数据仓库</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>Hive</tag>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive行列互转]]></title>
    <url>%2F2019%2F10%2F25%2FHive%E8%A1%8C%E5%88%97%E4%BA%92%E8%BD%AC%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;列转行用到的是concat_ws和collect_list(collect_set)，行转列用到的是lateral view explode和split。 1.列转行测试数据 12345678910111213hive&gt; select * from col_row limit 10;OKcol_row.user_id col_row.order_id104399 1715131104399 2105395104399 1758844104399 981085104399 2444143104399 1458638104399 968412104400 1609001104400 2986088104400 1795054 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;把相同user_id的order_id按照逗号转为一行 123456select user_id, concat_ws(&apos;,&apos;, collect_list(order_id)) as order_value from col_rowgroup by user_idlimit 10; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;结果(简写): 12user_id order_value104399 1715131,2105395,1758844,981085,2444143 总结&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;使用函数：concat_ws(‘,’, collect_set(column))&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;说明：collect_list 不去重，collect_set 去重。 column的数据类型要求是string 2.行转列测试数据 1234567891011121314hive&gt; select * from row_col;OKrow_col.user_id row_col.order_value104408 2909888,2662805,2922438,674972,2877863,190237104407 2982655,814964,1484250,2323912,2689723,2034331,1692373,677498,156562,2862492,338128104406 1463273,2351480,1958037,2606570,3226561,3239512,990271,1436056,2262338,2858678104405 153023,2076625,1734614,2796812,1633995,2298856,2833641,3286778,2402946,2944051,181577,464232104404 1815641,108556,3110738,2536910,1977293,424564104403 253936,2917434,2345879,235401,2268252,2149562,2910478,375109,932923,1989353104402 3373196,1908678,291757,1603657,1807247,573497,1050134,3402420104401 814760,213922,2008045,3305934,2130994,1602245,419609,2502539,3040058,2828163,3063469104400 1609001,2986088,1795054,429550,1812893104399 1715131,2105395,1758844,981085,2444143,1458638,968412Time taken: 0.065 seconds, Fetched: 10 row(s) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;将order_value的每条记录切割为单元素 1234567select user_id, order_value, order_idfrom row_collateral view explode(split(order_value, &apos;,&apos;)) num as order_idlimit 10; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;解析： split会将order_value字段按照指定的字符进行分割成集合； explode将集合进行展开； lateral view会根据explode结果生成一个侧视图，与row_col进行笛卡尔积操作；&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;结果如下:123456789101112user_id order_value order_id104408 2909888,2662805,2922438,674972,2877863,190237 2909888104408 2909888,2662805,2922438,674972,2877863,190237 2662805104408 2909888,2662805,2922438,674972,2877863,190237 2922438104408 2909888,2662805,2922438,674972,2877863,190237 674972104408 2909888,2662805,2922438,674972,2877863,190237 2877863104408 2909888,2662805,2922438,674972,2877863,190237 190237104407 2982655,814964,1484250,2323912,2689723,2034331,1692373,677498,156562,2862492,338128 2982655104407 2982655,814964,1484250,2323912,2689723,2034331,1692373,677498,156562,2862492,338128 814964104407 2982655,814964,1484250,2323912,2689723,2034331,1692373,677498,156562,2862492,338128 1484250104407 2982655,814964,1484250,2323912,2689723,2034331,1692373,677498,156562,2862492,338128 2323912Time taken: 0.096 seconds, Fetched: 10 row(s) You have to believe yourself, that’s the secret of the success.]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive经典SQL]]></title>
    <url>%2F2019%2F10%2F25%2FHive%E7%BB%8F%E5%85%B8SQL%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;下面来看下数据仓库数据分析过程中使用到比较经典的Hive SQL的使用。 有十万个淘宝店铺，每个顾客访问任意一个店铺时都会生成一条访问日志，访问日志表为visit，其中uid为用户id，store为店铺名称，统计店铺的uv； 1select store, count(distinct uid) from visit group by store; 有一亿个用户存储在user表中，有字段uid，age（年龄），total_consume（消费总金额），使用hive sql或者spark sql按照用户年龄大小降序排序，如果年龄相同按照消费总金额升序排列； 1select uid, age, total_consume from `user` order by age desc, total_consume; Just do it.]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[程序员节]]></title>
    <url>%2F2019%2F10%2F24%2F%E7%A8%8B%E5%BA%8F%E5%91%98%E8%8A%82%2F</url>
    <content type="text"><![CDATA[Talk is cheap, show me the code.]]></content>
      <categories>
        <category>工作</category>
      </categories>
      <tags>
        <tag>1024</tag>
        <tag>程序员节</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据仓库功能]]></title>
    <url>%2F2019%2F10%2F23%2F%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%8A%9F%E8%83%BD%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;刚开始接触数据仓库时，觉得这个东西就是一堆数据的集合，可以是一个数据库，可以是多个数据库，其它没什么。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;做了一年之后，大概了解了它的结构，可以提供历史变化的数据，可以写一些sql统计不同的指标，做数据分析，给各小组查看产品的运营情况。也进行了业务系统和仓库之间的数据质量对比、元数据同步等功能。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;再然后重新去找数据仓库的工作，发现其实不然，数据仓库除了上述的一些特点之外，我们还缺元数据管理，去管理仓库中的数据。也缺OLAP分析系统，分析人员可以使用不同的维度组合进行数据分析，使得数据发挥最大的价值。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;工作没找着，反而发现之前的数据仓库做的很浅很浅，其实更强大的功能是数据挖掘，除了提供基本的数据分析功能外，能否从当前已有的数据中挖掘出更有价值的东西来，才是数据仓库最大的最有价值的功能。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对不同的部门，做不同的数据特征，用户画像，用户标签，才是王道。 I might say that success is won by three things: first, effort; second, effort; third, still more effort.]]></content>
      <categories>
        <category>数据仓库</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
        <tag>数据仓库</tag>
        <tag>数据挖掘</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用户连续活跃的最大天数HQL]]></title>
    <url>%2F2019%2F10%2F23%2F%E7%94%A8%E6%88%B7%E8%BF%9E%E7%BB%AD%E6%B4%BB%E8%B7%83%E7%9A%84%E6%9C%80%E5%A4%A7%E5%A4%A9%E6%95%B0HQL%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;最近经常碰到这样的问题，每天每个城市播放最多的10首歌，某月每支股票连续下跌/上涨的最大天数，用户连续活跃的最大天数，初步看起来都和分析函数相关，考验逻辑思维和写复杂SQL的能力。 一、用户连续活跃的最大天数&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;以Oracle的分析函数语法说明，同理hive sql，首先模拟一些用户活跃的数据： 12345678910111213141516171819202122-- 建表语句DROP TABLE sigin;create table sigin( userid int, sigindate varchar2(20) ); -- 模拟数据插入insert into sigin values(1,&apos;2017-01-01&apos;);insert into sigin values(1,&apos;2017-01-02&apos;);insert into sigin values(1,&apos;2017-01-03&apos;);insert into sigin values(1,&apos;2017-01-04&apos;);insert into sigin values(2,&apos;2017-01-01&apos;);insert into sigin values(2,&apos;2017-01-02&apos;);insert into sigin values(2,&apos;2017-01-03&apos;);insert into sigin values(1,&apos;2017-01-10&apos;);insert into sigin values(2,&apos;2017-01-10&apos;);insert into sigin values(1,&apos;2017-01-11&apos;);insert into sigin values(2,&apos;2017-01-11&apos;);insert into sigin values(1,&apos;2017-01-12&apos;);insert into sigin values(2,&apos;2017-01-12&apos;); 大体思路如下： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;首先根据userid进行分组，将用户所有活跃的记录依次按照时间排序并标上序号。根据时间有序的特点，将所有时间减去它对应的序号，获取连续活跃时间唯一的时间点。如 1232017-01-01 12017-01-02 22017-01-03 3 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;时间减去序号，得唯一时间2016-12-31。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;根据userid和这个连续活跃时间唯一的时间点进行分组，计算连续活跃天数。 1234567891011121314-- 每个用户的几段连续活跃的天数select userid, to_date(sigindate,&apos;yyyy-mm-dd&apos;)-sigin_rank as date_rank, count(1) as sigincountfrom ( select userid, sigindate, row_number() over(partition by userid order by sigindate) as sigin_rank from sigin ) c group by userid, to_date(sigindate,&apos;yyyy-mm-dd&apos;)-sigin_rank; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;得到结果1如下： 12345USERID DATE_RANK SIGINCOUNT1 2017/1/5 32 2017/1/6 31 2016/12/31 42 2016/12/31 3 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上述方法可以找到每个用户的连续活跃天数，但用户中间有中断时程序就无法满足，一个用户出现了多条记录，分别为用户的多段连续活跃所产生。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们最终的目标是得到用户连续活跃的最大天数，可利用上述方法所得到的结果，在外面再嵌套一层，针对userid进行group by，得到每个用户的最大活跃天数。 123456789101112131415161718select d.userid, Max(d.sigincount) as max_sigincount from ( select userid, to_date(sigindate,&apos;yyyy-mm-dd&apos;)-sigin_rank as date_rank, count(1) as sigincount from ( select userid, sigindate, row_number() over(partition by userid order by sigindate) as sigin_rank from sigin ) c group by userid, to_date(sigindate,&apos;yyyy-mm-dd&apos;)-sigin_rank ) d group by d.userid; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;得到结果2如下： 123USERID MAX_SIGINCOUNT1 42 3 二、如果还需要得到用户连续活跃最大天数中这一段的首次活跃时间，可以把以上两个结果进行关联得到1234567891011121314151617181920212223242526272829303132333435363738394041-- 每个用户连续活跃的最大天数和连续活跃的第一天的时间select f.userid, g.date_rank+1, f.max_sigincount from ( select d.userid, Max(d.sigincount) as max_sigincount from ( select userid, to_date(sigindate,&apos;yyyy-mm-dd&apos;)-sigin_rank as date_rank, count(1) as sigincount from ( select userid, sigindate, row_number() over(partition by userid order by sigindate) as sigin_rank from sigin ) c group by userid ,to_date(sigindate,&apos;yyyy-mm-dd&apos;)-sigin_rank ) d group by d.userid) f inner join ( select userid, to_date(sigindate,&apos;yyyy-mm-dd&apos;)-sigin_rank as date_rank, count(1) as sigincount from ( select userid, sigindate, row_number() over(partition by userid order by sigindate) as sigin_rank from sigin ) c group by userid,to_date(sigindate,&apos;yyyy-mm-dd&apos;)-sigin_rank ) g on f.userid = g.userid and f.max_sigincount = g.sigincount; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;得到结果3如下: 1234USERID G.DATE_RANK+1 MAX_SIGINCOUNT2 2017/1/7 31 2017/1/1 42 2017/1/1 3 三、问题&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;结果3还存在一个问题，如果用户有两段连续活跃的天数相同且最大，则第二段连续活跃的首次活跃时间是不对的，这个问题怎么解决呢？欢迎留言你的解决方案。 Success often depends upon knowing how long it will take to succeed.]]></content>
      <categories>
        <category>数据仓库</category>
      </categories>
      <tags>
        <tag>HQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HQL经典分析语句]]></title>
    <url>%2F2019%2F10%2F23%2FHQL%E7%BB%8F%E5%85%B8%E5%88%86%E6%9E%90%E8%AF%AD%E5%8F%A5%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;下面这篇文章会不定期更新数据仓库中常用到的数据分析场景中的hql实现，一起来看看。 1.用户连续活跃的最大天数hql详解 Only those who have the patience to do simple things perfectly ever acquire the skill to do diffcult things easily. 2.行列互转hql详解 3.学生总课程数与总金额hql详解 4.学生课程数据统计hql详解 5.销售数据统计hql详解 You should bear the motto in mind.]]></content>
      <categories>
        <category>数据仓库</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[国人买车欲望为什么急剧下降？]]></title>
    <url>%2F2019%2F10%2F22%2F%E5%9B%BD%E4%BA%BA%E4%B9%B0%E8%BD%A6%E6%AC%B2%E6%9C%9B%E4%B8%BA%E4%BB%80%E4%B9%88%E6%80%A5%E5%89%A7%E4%B8%8B%E9%99%8D%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;中国车市未老先衰，中低收入者没钱买车。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;中国的人口结构财富是一个三角形，大面积的人口都是中低收入人口，只有少量人口才算是富裕人口。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;随着房价不断上涨以及生活必需品价格的不断上涨，事实上对于整个中低收入影响是非常大的，他们购车欲望降低。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;蔚来、长安等类似车企，销量急剧下滑，蔚来18年亏损甚至达200亿，股价一度跌近90%。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;来看下世界各国汽车人均占比图：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;看似中国汽车市场有很大增长空间，但是实际收入相比美国还是甚远，2018年中国9770美元，美国6.26万美元，相差7倍。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;看到这个状况，你有什么想法吗？ If you want to succeed, you should use persistence as your good friend, experience as your perference, prudence as your brother and hope as your sentry.]]></content>
      <categories>
        <category>汽车</category>
      </categories>
      <tags>
        <tag>汽车</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java集合对比]]></title>
    <url>%2F2019%2F10%2F17%2FJava%E9%9B%86%E5%90%88%E5%AF%B9%E6%AF%94%2F</url>
    <content type="text"><![CDATA[java中集合分List、Set、Map，下面来看看他们的实现类、底层结构及什么情境下使用。 1.List1.1 ArrayList底层结构是数组： 线程不安全，相对于Vector效率高； 查找快，增删慢； 1.2 Vector底层结构是数组： 线程安全，相对于ArrayList效率低； 查找快，增删慢； 1.3 LinkedList底层结构是链表： 线程不安全，效率高； 增删快，查找慢； 2.Set2.1 HashSet底层结构是哈希表： 无序、唯一； 查找快，增删快； 2.2 LinkedHashSet底层结构是链表+哈希表： 有序、唯一； 查找快，增删快； 2.3 TreeSet底层结构是红黑树： 有序、唯一； 查找快，增删快； 3.Map3.1 HashMap底层结构是哈希表： 线程不安全，相比Hashtable，效率高； 无序； 查找快，增删快； 父类AbstractMap； k、v允许为null值 3.2 Hashtable底层结构是哈希表： 线程安全，相比HashMap，效率低； 无序； 查找快，增删快； 父类Dictionary； k、v不允许为null值； 3.3 CurrentHashMap底层结构是哈希表： 线程安全，相比Hashtable做了优化，增删不会锁定整张表，原理是将整个map分成了n个segment； 无序； 查找快，增删快； 3.3 TreeMap底层结构是红黑树： 有序； 查找快，增删快； One is never too old to learn.]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>集合</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cut命令常用方法总结]]></title>
    <url>%2F2019%2F10%2F16%2Fcut%E5%91%BD%E4%BB%A4%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[Linux：cut命令详解，cut能干什么呢？ 文件内容查看； 显示行中的指定部分，删除文件中指定字段； 显示文件的内容，类似于下的type命令； 1.说明&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;该命令有两项功能，其一是用来显示文件的内容，它依次读取由参数file所指明的文件，将它们的内容输出到标准输出上；其二是连接两个或多个文件，如cut fl f2 &gt; f3将把文件fl和几的内容合并起来，然后通过输出重定向符“&gt;”的作用，将它们放入文件f3中。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当文件较大时，文本在屏幕上迅速闪过（滚屏），用户往往看不清所显示的内容。因此，一般用more等命令分屏显示。为了控制滚屏，可以按Ctrl+S键，停止滚屏；按Ctrl+Q键可以恢复滚屏。按Ctrl+C（中断键可以终止该命令的执行，并且返回Shell提示符状态。 2.语法cut(选项)(参数) 2.1 选项123456789-b：仅显示行中指定直接范围的内容；-c：仅显示行中指定范围的字符；-d：指定字段的分隔符，默认的字段分隔符为“TAB”；-f：显示指定字段的内容；-n：与“-b”选项连用，不分割多字节字符；--complement：补足被选择的字节、字符或字段；--out-delimiter=&lt;字段分隔符&gt;：指定输出内容是的字段分割符；--help：显示指令的帮助信息；--version：显示指令的版本信息； 2.2 参数&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;文件：指定要进行内容过滤的文件。 3.实例3.1.例如有一个学生报表信息，包含No、Name、Mark、Percent：123456cat test.txtNo Name Mark Percent01 tom 69 9102 jack 71 8703 alex 68 98 使用 -f 选项提取指定字段： 1cut -f 1 test.txt 结果： 1234No010203 1cut -f2,3 test.txt 结果： 1234Name Marktom 69jack 71alex 68 –complement 选项提取指定字段之外的列（打印除了第二列之外的列）： 1cut -f2 --complement test.txt 结果： 1234No Mark Percent01 69 9102 71 8703 68 98 3.2.使用 -d 选项指定字段分隔符：123456cat test2.txtNo;Name;Mark;Percent01;tom;69;9102;jack;71;8703;alex;68;98 1cut -f2 -d&quot;;&quot; test2.txt 结果： 1234Nametomjackalex 3.3.指定字段的字符或者字节范围&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;cut命令可以将一串字符作为列来显示，字符字段的记法： 123N-：从第N个字节、字符、字段到结尾；N-M：从第N个字节、字符、字段到第M个（包括M在内）字节、字符、字段；-M：从第1个字节、字符、字段到第M个（包括M在内）字节、字符、字段； &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上面是记法，结合下面选项将摸个范围的字节、字符指定为字段： 123-b 表示字节；-c 表示字符；-f 表示定义字段； 4.例：1234567cat test.txtabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz 打印第1个到第3个字符： 1cut -c1-3 test.txt 结果： 12345abcabcabcabcabc 打印前2个字符： 1cut -c-2 test.txt 结果： 12345ababababab 打印从第5个字符开始到结尾： 1cut -c5- test.txt 结果： 12345efghijklmnopqrstuvwxyzefghijklmnopqrstuvwxyzefghijklmnopqrstuvwxyzefghijklmnopqrstuvwxyzefghijklmnopqrstuvwxyz You need to know what you need.]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>cut</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[grep命令常用方法总结]]></title>
    <url>%2F2019%2F10%2F16%2Fgrep%E5%91%BD%E4%BB%A4%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;grep 是一种强大的文本搜索工具，它能使用正则表达式搜索文本，并把匹配的行打印出来。通常grep有三种版本grep、egrep（等同于grep -E）和fgrep。egrep为扩展的grep，fgrep则为快速grep（固定的字符串来对文本进行搜索，不支持正则表达式的引用但是查询极为快速）。grep是Linux文本处理三剑客之一。 1.正则表达式1234grep -E &apos;l\&#123;2,\&#125;&apos; 2.txtgrep -E &apos;h(ell|a)o&apos; test.txtgrep &apos;[a-z]\&#123;5,\&#125;&apos; test.txtgrep -E &apos;^(h|y)&apos; test.txt //-E支持扩展正则，相当于egrep 2.操作文件12grep -xf a.txt b.txt //查询a与b共同存在的行grep -vxf b.txt a.txt //查询a的行不在b文件中面的,就是a比b多出来的行 3.简单查询1234grep -v &apos;^h&apos; test.txt //显示非以h开发的grep -r &quot;hello&quot; xx //在xx文件夹查找grep -c //统计匹配的行数grep -n //显示行号 4.查询匹配到的上下文123grep -A 1 &apos;hello&apos; test.txt //除了显示匹配到的行也显示它之后的一行grep -B 1 &apos;hello&apos; test.txt //除了显示匹配到的行也显示它之前的一行grep -C 1 &apos;hello&apos; test.txt //除了显示匹配到的行也显示它之前和之后的一行 5.参数解释[options]主要參数：－c：仅仅输出匹配行的计数。－I：不区分大 小写(仅仅适用于单字符)。－h：查询多文件时不显示文件名称。－l：查询多文件时仅仅输出包括匹配字符的文件名称。－n：显示匹配行及 行号。－s：不显示不存在或无匹配文本的错误信息。－v：显示不包括匹配文本的全部行。 6.命令正則表達式123456789 . 匹配单个字符 如..X yiX能查出来 ^ 匹配行首 ^d 每行第一个字符为d $ 匹配行尾 T$ 每行最后一个字符为T * 匹配随意字符串 \ 屏蔽特殊字符的含义 A&#123;2&#125;B 字母A出现两次 A&#123;2,&#125;B 至少出现两次 A&#123;2,4&#125;B 出现2到4次[0-9]&#123;4&#125;xx[0-9]&#123;4&#125; 前四个是数字，中间是xx,后四个是数字 7.pattern正則表達式主要參数12345678910111213\： 忽略正則表達式中特殊字符的原有含义;^：匹配正則表達式的開始行;$: 匹配正則表達式的结束行;&lt;：从匹配正则表达式的行開始;锚定单词的開始。如:/\&lt;love/匹配包括以love开头的单词的行;&gt;：到匹配正則表達式的行结束;[ ]：单个字符。如[A]即A符合要求;[ - ]：范围。如[A-Z]，即A、B、C一直到Z都符合要求;.：全部的单个字符;* ：有字符，长度能够为0[^]: 匹配一个不在指定范围内的字符。如：/[^A-RT-Z]ed/匹配不包括A-R和T-Z的一个字母开头。紧跟ed的行;+: 匹配前面的子表达式一次或多次。比如，&apos;zo+&apos; 能匹配 &quot;zo&quot; 以及 &quot;zoo&quot;，但不能匹配 &quot;z&quot;。+ 等价于 &#123;1,&#125;;?: 匹配前面的子表达式零次或一次。比如。&quot;do(es)?&quot; 能够匹配 &quot;do&quot; 或 &quot;does&quot; 中的&quot;do&quot; 。? 等价于 &#123;0,1&#125;; You need to know what you want.]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>grep</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Impala和Hive的对比]]></title>
    <url>%2F2019%2F10%2F15%2FImpala%E5%92%8CHive%E7%9A%84%E5%AF%B9%E6%AF%94%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Impala和Hive两者都是构建在Hadoop之上的大数据查询分析工具，他们之间有什么区别和联系呢？ 1.从客户端来看&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Impala和Hive有许多相同之处，如数据表元数据、ODBC/JDBC驱动、SQL语法等，但是他们还是有不同之处的，Hive适合长时间的批处理查询分析，而Impala适合实时交互式查询分析。Impala给数据分析人员提供了快速实验、验证想法的大数据分析工具，可以先使用hive进行数据转换处理，然后使用Impala在Hive处理后的结果集之上进行快速的数据分析。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;下图可以看下Impala和Hive的区别与联系： 2.从技术上来看 执行计划：Impala没有使用Mr进行并行计算，mr是面向批处理的方式，而Impala是面向交互式的并行计算。Impala把整个查询解析成执行计划树，在分发执行计划后，使用拉式获取计算结果，把结果数据组成按执行树流式传递汇集，减少了中间结果写入磁盘的步骤，再从磁盘读取数据的开销； 执行方式：Impala使用服务的方式，避免每次执行查询都要启动的开销，即相比Hive节省了启动mapreduce的开销； 数据存储：都支持把数据存储在HDFS和HBase； 元数据：使用相同的元数据； Sql解析：比较相似，都是通过语法分析生成的执行计划； 数据流： Hive：采用推的方式，每一个计算节点task执行完成之后将计算结果一起推送给下一个task； Impala：采用拉的方式，后续节点通过getNext，主动向前面的节点拉取数据，以此方式获取结果以流的形式返回给客户端，只要有1条结果数据，就会立即展现在客户端，不用等到全部处理完，更符合交互式查询使用； 3.总结&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;所以，Hive适合复杂的批处理查询，数据转换任务，而Impala适合实时数据分析，配合Hive使用，对hive的结果数据进行实时分析。 Heath is certainly more important than money.]]></content>
      <categories>
        <category>OLAP</category>
      </categories>
      <tags>
        <tag>Hive</tag>
        <tag>Impala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Presto和Hive的对比]]></title>
    <url>%2F2019%2F10%2F14%2FPresto%E5%92%8CHive%E7%9A%84%E5%AF%B9%E6%AF%94%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Presto和Hive作为大数据分析领域OLAP工具，他们之间有什么区别和联系呢？下面来看看。 1.本质区别&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Hive是把一个查询转化成多个MapReduce任务，然后一个接一个执行。执行的中间结果通过对磁盘的读写来同步。然而，Presto没有使用MapReduce，它是通过一个定制的查询和执行引擎来完成的。它的所有的查询处理是在内存中，这也是它的性能很高的一个主要原因。 2.执行速度&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;presto由于是基于内存的，而hive是在磁盘上读写的，因此presto比hive快很多，但是由于是基于内存的当多张大表关联操作时易引起内存溢出错误 3.处理json类型的数据presto处理如下： 123select json_extract_scalar(xx[&apos;custom&apos;],&apos;$.position&apos;)from table hive处理如下： 123select get_json_object(xx[&apos;custom&apos;],&apos;$.position&apos;)from table &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;此外Presto还有一个函数json_extract是直接返回一个json串，根据需要自己需要选择函数 4.列转行Hive: 1select student, score from tests lateral view explode(split(scores, &apos;,&apos;)) t as score; Presto: 1select student, score from tests cross json unnest(split(scores, &apos;,&apos;) as t (score) Where there is a will, there is a way.]]></content>
      <categories>
        <category>OLAP</category>
      </categories>
      <tags>
        <tag>Hive</tag>
        <tag>Presto</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据仓库设计终极目标]]></title>
    <url>%2F2019%2F10%2F14%2F%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E8%AE%BE%E8%AE%A1%E7%BB%88%E6%9E%81%E7%9B%AE%E6%A0%87%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们设计的数据仓库，最基本的功能是服务对象需要什么数据，我们就能很简单地提供什么数据，什么是数据仓库的终极目标呢，即什么样的数据仓库是完美的数据仓库？ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;完美的数据仓库：一个企业中的所有业务，根据数据仓库提供的分析模型，我们可以知道产品的好坏、产品的发展情况，根据这些分析的数据，指导管理者做出正确的决策，引导他们做出一个更好的产品。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;因此，我们在设计数据仓库的时候，应该设计出好的模型，从操作型数据库设计开始，应该提供什么样的数据，应该提供什么样的功能，才是我们应该重点着手去做的事情。 Where there’s a will, there’s a way.]]></content>
      <categories>
        <category>数据仓库</category>
      </categories>
      <tags>
        <tag>数据仓库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mr&Tez&Spark引擎的区别]]></title>
    <url>%2F2019%2F10%2F14%2FMr%26Tez%26Spark%E5%BC%95%E6%93%8E%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;spark号称比mr快100倍，tez也号称比mr快100倍，那么他们之间为什么有这么大的差距呢？下面来看看。 为什么性能远超mr？ spark和tez都是基于dag方式处理数据 tez源于mr，核心思想是将map和reduce两个操作进一步拆分，即map被拆分Input、Processor、Sort、Merge和Output，reduce被拆分为Input、Shuffle、Sort、Merge和Output，这样分解后的元数据操作就可以随意组合，产生新的操作，这些操作经过一些程序组装后，形成一个大的DAG作业，可以将多个map依赖合并为1个，这样只需写一次hdfs；mr的话，有多少个依赖map就有多少个任务； spark直接可以将job输出的中间结果写入内存中；&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mr和tez对比图如下： 使用场景的区别 spark是一个通用计算引擎，提供实时、离线、机器学习等多种计算方式，适合迭代计算； tez作为一个框架工具，特定为hive和pig提供计算； 优势 spark属于内存计算，map阶段计算结果可以缓存在内存中。支持多种运行模式，可以跑在standalone、yarn、memos等资源调度框架上，而tez只能跑在yarn上； tez能够及时地释放资源，重启container，节省调度时间，对内存的资源要求率不高，资源利用率高，属于细粒度资源调度；而spark如果存在迭代计算时，container会一直占用着资源，资源利用率不高，属于粗粒度资源调度； 总结&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;tez与spark两者并不矛盾，不存在冲突，在实际生产中，如果数据需要快速处理而且资源充足，则可以选择spark；如果资源是瓶颈，则可以使用tez；可以根据不同场景不同数据层次做出选择；这个总结同样也适合spark与mr的比较； Asking cost nothing.]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>mr</tag>
        <tag>tez</tag>
        <tag>spark</tag>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive优化之小文件合并]]></title>
    <url>%2F2019%2F10%2F13%2FHive%E4%BC%98%E5%8C%96%E4%B9%8B%E5%B0%8F%E6%96%87%E4%BB%B6%E5%90%88%E5%B9%B6%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当Hive输入由很多个小文件组成，由于每个小文件都会启动一个map任务，如果文件过小，以至于map任务启动和初始化的时间大于逻辑处理的时间，会造成资源浪费，甚至OOM。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为此，当我们启动一个任务，发现输入数据量小但任务数量多时，需要注意在Map前端进行输入合并。当然，在我们向一个表写数据时，也需要注意输出文件大小。 问题1.通常情况下，作业会通过input的目录产生一个或者多个map任务。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;主要的决定因素有： input的文件总个数，input的文件大小，集群设置的文件块大小(目前为128M, 可在hive中通过set dfs.block.size;命令查看到，该参数不能自定义修改)； 2.举例：a) 假设input目录下有1个文件a,大小为780M,那么hadoop会将该文件a分隔成7个块（6个128m的块和1个12m的块），从而产生7个map数b) 假设input目录下有3个文件a,b,c,大小分别为10m，20m，130m，那么hadoop会分隔成4个块（10m,20m,128m,2m）,从而产生4个map数,即如果文件大于块大小(128m),那么会拆分，如果小于块大小，则把该文件当成一个块。 3.是不是map数越多越好？&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;答案是否定的。如果一个任务有很多小文件（远远小于块大小128m）,则每个小文件也会被当做一个块，用一个map任务来完成，而一个map任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的map数是受限的。 4.是不是保证每个map处理接近128m的文件块，就高枕无忧了？&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;答案也是不一定。比如有一个127m的文件，正常会用一个map去完成，但这个文件只有一个或者两个小字段，却有几千万的记录，如果map处理的逻辑比较复杂，用一个map任务去做，肯定也比较耗时。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;针对上面的问题3和4，我们需要采取两种方式来解决：即减少map数和增加map数； 5.如何合并小文件，减少map数？&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;假设一个SQL任务： 1Select count(1) from popt_tbaccountcopy_mes where pt = ‘2012-07-04’; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;该任务的inputdir /group/p_sdo_data/p_sdo_data_etl/pt/popt_tbaccountcopy_mes/pt=2012-07-04 共有194个文件，其中很多是远远小于128m的小文件，总大小9G，正常执行会用194个map任务。Map总共消耗的计算资源： SLOTS_MILLIS_MAPS= 623,020 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我通过以下方法来在map执行前合并小文件，减少map数： 1234set mapred.max.split.size=100000000;set mapred.min.split.size.per.node=100000000;set mapred.min.split.size.per.rack=100000000;set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;再执行上面的语句，用了74个map任务，map消耗的计算资源：SLOTS_MILLIS_MAPS= 333,500, 对于这个简单SQL任务，执行时间上可能差不多，但节省了一半的计算资源。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;大概解释一下，100000000表示100M, set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;这个参数表示执行前进行小文件合并，前面三个参数确定合并文件块的大小，大于文件块大小128m的，按照128m来分隔，小于128m,大于100m的，按照100m来分隔，把那些小于100m的（包括小文件和分隔大文件剩下的），进行合并,最终生成了74个块。 6.如何适当的增加map数？&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当input的文件都很大，任务逻辑复杂，map执行非常慢的时候，可以考虑增加Map数，来使得每个map处理的数据量减少，从而提高任务的执行效率。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;假设有这样一个任务： 12345678Select data_desc, count(1), count(distinct id), sum(case when …), sum(case when ...), sum(…)from a group by data_desc; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果表a只有一个文件，大小为120M，但包含几千万的记录，如果用1个map去完成这个任务，肯定是比较耗时的，这种情况下，我们要考虑将这一个文件合理的拆分成多个，这样就可以用多个map任务去完成: 123456set mapred.reduce.tasks=10;create table a_1 as select * from a distribute by rand(123); &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这样会将a表的记录，随机的分散到包含10个文件的a_1表中，再用a_1代替上面sql中的a表，则会用10个map任务去完成。每个map任务处理大于12M（几百万记录）的数据，效率肯定会好很多。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;看上去，貌似这两种有些矛盾，一个是要合并小文件，一个是要把大文件拆成小文件，这点正是重点需要关注的地方，根据实际情况，控制map数量需要遵循两个原则：使大数据量利用合适的map数；使单个map任务处理合适的数据量； 1. Map输入合并小文件&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对应参数： 1234set mapred.max.split.size=256000000; #每个Map最大输入大小set mapred.min.split.size.per.node=100000000; #一个节点上split的至少的大小 set mapred.min.split.size.per.rack=100000000; #一个交换机下split的至少的大小set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat; #执行Map前进行小文件合并 在开启了org.apache.hadoop.hive.ql.io.CombineHiveInputFormat后，一个data node节点上多个小文件会进行合并，合并文件数由 123mapred.max.split.size #限制的大小决定mapred.min.split.size.per.node #决定了多个data node上的文件是否需要合并~mapred.min.split.size.per.rack #决定了多个交换机上的文件是否需要合并~ 2. 输出合并1234set hive.merge.mapfiles=true; #在Map-only的任务结束时合并小文件set hive.merge.mapredfiles=true; #在Map-Reduce的任务结束时合并小文件set hive.merge.size.per.task=256*1000*1000; #合并文件的大小set hive.merge.smallfiles.avgsize=16000000; #当输出文件的平均大小小于该值时，启动一个独立的map-reduce任务进行文件merge]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
        <tag>优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive文件压缩格式对比]]></title>
    <url>%2F2019%2F10%2F13%2FHive%E6%96%87%E4%BB%B6%E5%8E%8B%E7%BC%A9%E6%A0%BC%E5%BC%8F%E5%AF%B9%E6%AF%94%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Hive的文件压缩格式应该在不同的应用场景下使用不同的方式，例如cpu资源足够，但是硬盘容量不足时，可以使用bzip2方式。也跟文件格式有关，对列式存储的文件进行压缩，会得到一个可观的压缩比例，我们在上一篇文章中讲解了 Hive文件格式的对比。 一、对比&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们来看下不同的压缩方式的对比： 1234567/*压缩：可以最小化所需要的磁盘存储空间，以及减小磁盘和网络I/O操作，但是文件压缩和解压过程会增加CPU开销。因此，对于压缩密集型的job最好使用压缩，特别是有额外的CPU资源或者磁盘存储空间比较稀缺的情况。*/-- BZip2压缩率最高，但是消耗最多的CPU开销-- GZip是压缩率和压缩/解压速度上的下一个选择-- LZO和Snappy压缩率低于前两者，但是速度快-- Bzip2和LZO可以支持块级别的压缩，另外两者不支持，如果用后两者，可以将文件分割成期望值的大小。 二、配置1.配置文件（永久有效）123456789101112131415[hadoop@hadoop004 hadoop]$ vim core-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop004:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;io.compression.codecs&lt;/name&gt; &lt;value&gt;org.apache.hadoop.io.compress.GzipCodec, org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.BZip2Codec, org.apache.hadoop.io.compress.SnappyCodec &lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 1234567891011121314151617181920212223242526[hadoop@hadoop004 hadoop]$ vim mapred-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置 Map段输出的压缩,snappy--&gt; &lt;property&gt; &lt;name&gt;mapreduce.map.output.compress&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.map.output.compress.codec&lt;/name&gt; &lt;value&gt;org.apache.hadoop.io.compress.SnappyCodec&lt;/value&gt; &lt;/property&gt; &lt;!--开启MapReduce输出文件压缩--&gt; &lt;property&gt; &lt;name&gt;mapreduce.output.fileoutputformat.compress&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.output.fileoutputformat.compress.codec&lt;/name&gt; &lt;value&gt;org.apache.hadoop.io.compress.BZip2Codec&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 1.1 先看看snappy的效果12[hadoop@hadoop004 data]$ hdfs dfs -du -s -h /user/hive/warehouse/page_views/page_views.dat18.1 M 18.1 M /user/hive/warehouse/page_views/page_views.dat 1234567891011121314151617181920hive&gt; SET hive.exec.compress.output;hive.exec.compress.output=falsehive&gt; &gt; create table page_views( &gt; track_times string, &gt; url string, &gt; session_id string, &gt; referer string, &gt; ip string, &gt; end_user_id string, &gt; city_id string &gt; ) row format delimited fields terminated by &apos;\t&apos;;OKTime taken: 0.741 seconds hive&gt; load data local inpath &apos;/home/hadoop/data/page_views.dat&apos; overwrite into table page_views;Loading data to table default.page_viewsTable default.page_views stats: [numFiles=1, numRows=0, totalSize=19014993, rawDataSize=0]OKTime taken: 0.595 seconds 因为我在mapred-site.xml里面设置默认的解压缩格式为snappy 1234567891011121314151617181920212223242526272829hive&gt; &gt; set hive.exec.compress.output=true;hive&gt; set hive.exec.compress.output;hive.exec.compress.output=truehive&gt; set mapreduce.output.fileoutputformat.compress.codec;mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.SnappyCodechive&gt; create table page_views_snappy as select * from page_views;Query ID = hadoop_20190419141818_561ede29-e964-4655-9e48-1e4f5d6eeb5cTotal jobs = 3Launching Job 1 out of 3Number of reduce tasks is set to 0 since there&apos;s no reduce operatorStarting Job = job_1555643336639_0002, Tracking URL = http://hadoop004:8088/proxy/application_1555643336639_0002/Kill Command = /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/bin/hadoop job -kill job_1555643336639_0002Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 02019-04-19 14:30:50,994 Stage-1 map = 0%, reduce = 0%2019-04-19 14:30:57,450 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 2.7 secMapReduce Total cumulative CPU time: 2 seconds 700 msecEnded Job = job_1555643336639_0002Stage-4 is selected by condition resolver.Stage-3 is filtered out by condition resolver.Stage-5 is filtered out by condition resolver.Moving data to: hdfs://hadoop004:9000/user/hive/warehouse/.hive-staging_hive_2019-04-19_14-30-44_413_8511625915872870114-1/-ext-10001Moving data to: hdfs://hadoop004:9000/user/hive/warehouse/page_views_snappyTable default.page_views_snappy stats: [numFiles=1, numRows=100000, totalSize=8814444, rawDataSize=18914993]MapReduce Jobs Launched:Stage-Stage-1: Map: 1 Cumulative CPU: 2.7 sec HDFS Read: 19018292 HDFS Write: 8814535 SUCCESSTotal MapReduce CPU Time Spent: 2 seconds 700 msecOKTime taken: 15.354 seconds 123456[hadoop@hadoop004 data]$ hdfs dfs -ls /user/hive/warehouse/page_views_snappyFound 1 items-rwxr-xr-x 1 hadoop supergroup 8814444 2019-04-19 14:30 /user/hive/warehouse/page_views_snappy/000000_0.snappy [hadoop@hadoop004 data]$ hdfs dfs -du -s -h /user/hive/warehouse/page_views_snappy8.4 M 8.4 M /user/hive/warehouse/page_views_snappy 1.2 下面采用bzip2瞅瞅12345hive&gt; &gt; set mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.BZip2Codec; hive&gt; set mapreduce.output.fileoutputformat.compress.codec;mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.BZip2Codec 1234567891011121314151617181920212223hive&gt; create table page_views_bzip2 as select * from page_views;Query ID = hadoop_20190419143939_53d67293-92de-4697-a791-f9a1afe7be01Total jobs = 3Launching Job 1 out of 3Number of reduce tasks is set to 0 since there&apos;s no reduce operatorStarting Job = job_1555643336639_0003, Tracking URL = http://hadoop004:8088/proxy/application_1555643336639_0003/Kill Command = /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/bin/hadoop job -kill job_1555643336639_0003Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 02019-04-19 14:44:16,992 Stage-1 map = 0%, reduce = 0%2019-04-19 14:44:25,471 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 5.19 secMapReduce Total cumulative CPU time: 5 seconds 190 msecEnded Job = job_1555643336639_0003Stage-4 is selected by condition resolver.Stage-3 is filtered out by condition resolver.Stage-5 is filtered out by condition resolver.Moving data to: hdfs://hadoop004:9000/user/hive/warehouse/.hive-staging_hive_2019-04-19_14-44-09_564_3458580979229190548-1/-ext-10001Moving data to: hdfs://hadoop004:9000/user/hive/warehouse/page_views_bzip2Table default.page_views_bzip2 stats: [numFiles=1, numRows=100000, totalSize=3815195, rawDataSize=18914993]MapReduce Jobs Launched:Stage-Stage-1: Map: 1 Cumulative CPU: 5.19 sec HDFS Read: 19018291 HDFS Write: 3815285 SUCCESSTotal MapReduce CPU Time Spent: 5 seconds 190 msecOKTime taken: 17.289 seconds 123456[hadoop@hadoop004 hadoop]$ hdfs dfs -ls /user/hive/warehouse/page_views_bzip2Found 1 items-rwxr-xr-x 1 hadoop supergroup 3815195 2019-04-19 14:44 /user/hive/warehouse/page_views_bzip2/000000_0.bz2 [hadoop@hadoop004 hadoop]$ hdfs dfs -du -s -h /user/hive/warehouse/page_views_bzip2/*3.6 M 3.6 M /user/hive/warehouse/page_views_bzip2/000000_0.bz2 1.3 再来看看gzip的12345hive&gt; &gt; set mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.GzipCodec; hive&gt; set mapreduce.output.fileoutputformat.compress.codec;mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.GzipCodec 123456789101112131415161718192021222324hive&gt; &gt; create table page_views_gzip as select * from page_views;Query ID = hadoop_20190419143939_53d67293-92de-4697-a791-f9a1afe7be01Total jobs = 3Launching Job 1 out of 3Number of reduce tasks is set to 0 since there&apos;s no reduce operatorStarting Job = job_1555643336639_0004, Tracking URL = http://hadoop004:8088/proxy/application_1555643336639_0004/Kill Command = /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/bin/hadoop job -kill job_1555643336639_0004Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 02019-04-19 14:48:10,606 Stage-1 map = 0%, reduce = 0%2019-04-19 14:48:18,019 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 3.29 secMapReduce Total cumulative CPU time: 3 seconds 290 msecEnded Job = job_1555643336639_0004Stage-4 is selected by condition resolver.Stage-3 is filtered out by condition resolver.Stage-5 is filtered out by condition resolver.Moving data to: hdfs://hadoop004:9000/user/hive/warehouse/.hive-staging_hive_2019-04-19_14-48-03_436_7531556390065383047-1/-ext-10001Moving data to: hdfs://hadoop004:9000/user/hive/warehouse/page_views_gzipTable default.page_views_gzip stats: [numFiles=1, numRows=100000, totalSize=5550655, rawDataSize=18914993]MapReduce Jobs Launched:Stage-Stage-1: Map: 1 Cumulative CPU: 3.29 sec HDFS Read: 19018290 HDFS Write: 5550744 SUCCESSTotal MapReduce CPU Time Spent: 3 seconds 290 msecOKTime taken: 15.866 seconds 123456[hadoop@hadoop004 hadoop]$ hdfs dfs -ls /user/hive/warehouse/page_views_gzipFound 1 items-rwxr-xr-x 1 hadoop supergroup 5550655 2019-04-19 14:48 /user/hive/warehouse/page_views_gzip/000000_0.gz [hadoop@hadoop004 hadoop]$ hdfs dfs -du -s -h /user/hive/warehouse/page_views_gzip/*5.3 M 5.3 M /user/hive/warehouse/page_views_gzip/000000_0.gz 2.配置参数（当前session有效）2.1. Hive中间数据压缩 hive.exec.compress.intermediate：默认该值为false，设置为true为激活中间数据压缩功能。HiveQL语句最终会被编译成Hadoop的Mapreduce job，开启Hive的中间数据压缩功能，就是在MapReduce的shuffle阶段对mapper产生的中间结果数据压缩。在这个阶段，优先选择一个低CPU开销的算法。 mapred.map.output.compression.codec：该参数是具体的压缩算法的配置参数，SnappyCodec比较适合在这种场景中编解码器，该算法会带来很好的压缩性能和较低的CPU开销。设置如下： 123set hive.exec.compress.intermediate=true;set mapred.map.output.compression.codec= org.apache.hadoop.io.compress.SnappyCodec;set mapred.map.output.compression.codec=com.hadoop.compression.lzo.LzoCodec; 2.2. Hive最终数据压缩 hive.exec.compress.output：用户可以对最终生成的Hive表的数据通常也需要压缩。该参数控制这一功能的激活与禁用，设置为true来声明将结果文件进行压缩。 mapred.output.compression.codec：将hive.exec.compress.output参数设置成true后，然后选择一个合适的编解码器，如选择SnappyCodec。设置如下： 12set hive.exec.compress.output=true;set mapred.output.compression.codec=org.apache.hadoop.io.compress.SnappyCodec; 2.3. 建表：Text: 12345678$&#123;建表语句&#125;stored as textfile;##########################################插入数据########################################set hive.exec.compress.output=true; --启用压缩格式set mapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec; --指定输出的压缩格式为Gzip set mapred.output.compress=true; set io.compression.codecs=org.apache.hadoop.io.compress.GzipCodec; insert overwrite table textfile_table select * from T_Name; Sequence 123456789$&#123;建表语句&#125;SORTED AS SEQUENCEFILE; --将Hive表存储定义成SEQUENCEFILE##########################################插入数据########################################set hive.exec.compress.output=true; --启用压缩格式set mapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec; --指定输出的压缩格式为Gzip set mapred.output.compression.type=BLOCK; --压缩选项设置为BLOCKset mapred.output.compress=true; set io.compression.codecs=org.apache.hadoop.io.compress.GzipCodec; insert overwrite table textfile_table select * from T_Name; RCFile 123456789$&#123;建表语句&#125;stored as rcfile;-插入数据操作：set hive.exec.compress.output=true; set mapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec; set mapred.output.compress=true; set io.compression.codecs=org.apache.hadoop.io.compress.GzipCodec; insert overwrite table rcfile_table select * from T_Name; Parquet 1set parquet.compression=snappy; 1create table mytable(a int,b int) stored as parquet tblproperties(&apos;parquet.compression&apos;=&apos;snappy&apos;); 1alter table mytable set tblproperties (&apos;parquet.compression&apos;=&apos;snappy&apos;); Trobule is Friend.]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
        <tag>Bzip2</tag>
        <tag>Gzip</tag>
        <tag>Snappy</tag>
        <tag>Lzo</tag>
        <tag>压缩格式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive中的文件格式对比]]></title>
    <url>%2F2019%2F10%2F12%2FHive%E4%B8%AD%E7%9A%84%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F%E5%AF%B9%E6%AF%94%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Hive表数据实际存储在HDFS文件系统中，而不同的文件格式，会有不同的特性，我们在数据仓库建设中，如何根据仓库不同层次特点设计不同的文件格式呢？下面来看下。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;常见的hive文件存储格式包括以下几类：TEXTFILE、SEQUENCEFILE、RCFILE、ORC。其中TEXTFILE为默认格式，建表时默认为这个格式，导入数据时会直接把数据文件拷贝到hdfs上不进行处理。SequenceFile、RCFile、ORC格式的表不能直接从本地文件导入数据，数据要先导入到TextFile格式的表中，然后再从TextFile表中用insert导入到SequenceFile、RCFile表中。 一、hive中的file_format SEQUENCEFILE：行存储，生产中绝对不会用，k-v格式，二进制文件，比源文本格式占用磁盘更多； TEXTFILE：行存储，生产中用的多，行式存储。最简单的数据格式，便于和其他工具（Pig, grep, sed, awk）共享数据，便于查看和编辑； RCFILE：行列混合存储，生产中用的少； ORC：行列式存储，生产中最常用，OCFILE的升级版，查询效率最高。 PARQUET：列式存储，生产中最常用，更高效的压缩和编码； AVRO：生产中几乎不用，不用考虑 JSONFILE：生产中几乎不用，不用考虑 INPUTFORMAT：生产中几乎不用，不用考虑注意hive默认的文件格式是TextFile，可通过 set hive.default.fileformat 进行配置 二、行式存储与列式存储&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;行式存储与列式存储数据物理底层存储区别 结论：由上图可知&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;行式存储一定会把同一行数据存到同一个块中，在select查询的时候，是对所有字段的查询，不可以单独查询某一行&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;列式存储同一列数据一定是存储到同一个块中，换句话说就是不同的列可以放到不同块中，在进行select查询的时候可以单独查询某一列。 三、优缺点1.列式存储&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;优点：当查询某个或者某几个字段的时候，只需要查看存储这几个字段的这几个block就可以了，大大的减少了数据的查询范围，提高了查询效率&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;缺点：当进行全字段查询的时候，数据需要重组，比单独查一行要慢 2.行式存储&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;优点：全字段查询比较快&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;缺点：当查询一张表里的几个字段的时候，底层依旧是读取所有的字段，这样查询效率降低，并且会造成不必要的资源浪费，而且，生产中很少会出现需要全字段查询的场景 四、hive文件格式配置实现以及对比1.创建原始表默认TEXTFILE Hive数据表的默认格式，存储方式：行存储。 可以使用Gzip压缩算法，但压缩后的文件不支持split 在反序列化过程中，必须逐个字符判断是不是分隔符和行结束符，因此反序列化开销会比SequenceFile高几十倍。12345678910111213CREATE EXTERNAL TABLE g6_access ( cdn string, region string, level string, time string, ip string, domain string, url string, traffic bigint)ROW FORMAT DELIMITEDFIELDS TERMINATED BY &apos;\t&apos;LOCATION &apos;/g6/hadoop/access/clear/test/&apos;; 导入测试数据 123[hadoop@hadoop001 data]$ ll-rw-r--r-- 1 hadoop hadoop 68051224 Apr 17 17:37 part-r-00000[hadoop@hadoop001 data]$ hadoop fs -put part-r-00000 /g6/hadoop/access/clear/test/ 通过hue查看数据的大小 64.9MB 2.创建以 SEQUENCEFILE格式储存的表g6_access_seq，并使用g6_access中的数据 压缩数据文件可以节省磁盘空间，但Hadoop中有些原生压缩文件的缺点之一就是不支持分割。支持分割的文件可以并行的有多个mapper程序处理大数据文件，大多数文件不支持可分割是因为这些文件只能从头开始读。Sequence File是可分割的文件格式，支持Hadoop的block级压缩。 Hadoop API提供的一种二进制文件，以key-value的形式序列化到文件中。存储方式：行存储。 sequencefile支持三种压缩选择：NONE，RECORD，BLOCK。Record压缩率低，RECORD是默认选项，通常BLOCK会带来较RECORD更好的压缩性能。 优势是文件和hadoop api中的MapFile是相互兼容的123create table g6_access_seqstored as SEQUENCEFILEas select * from g6_access; 查看数据大小 71.8MB &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;结论：比默认的TEXTFILE格式的文件还要大，生产上基本上是不会用的 3.创建RCFILE数据存储格式表，，并使用g6_access中的数据存储方式：数据按行分块，每块按列存储。结合了行存储和列存储的优点： 首先，RCFile 保证同一行的数据位于同一节点，因此元组重构的开销很低 其次，像列存储一样，RCFile 能够利用列维度的数据压缩，并且能跳过不必要的列读取 数据追加：RCFile不支持任意方式的数据写操作，仅提供一种追加接口，这是因为底层的 HDFS当前仅仅支持数据追加写文件尾部。 行组大小：行组变大有助于提高数据压缩的效率，但是可能会损害数据的读取性能，因为这样增加了 Lazy 解压性能的消耗。而且行组变大会占用更多的内存，这会影响并发执行的其他MR作业。 考虑到存储空间和查询效率两个方面，Facebook 选择 4MB 作为默认的行组大小，当然也允许用户自行选择参数进行配置。123create table g6_access_rcstored as RCFILEas select * from g6_access; 查看数据大小 61.6MB &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;结论：存储减少了3M左右，微不足道，读写性能也没有什么优势，生产也没有用他的理由 4.创建ORCFILE数据存储格式表，，并使用g6_access中的数据，默认是使用zlib压缩，支持zlib和snappy 存储方式：数据按行分块，每块按照列存储。 压缩快，快速列存取。效率比rcfile高，是rcfile的改良版本。123create table g6_access_orcstored as ORCas select * from g6_access; 查看数据大小 17.0MB 5.创建ORCFILE数据存储格式表，不使用压缩，并使用g6_access中的数据123create table g6_access_orc_nonestored as ORC tblproperties (&quot;orc.compress&quot;=&quot;NONE&quot;)as select * from g6_access; 查看数据大小 51.5MB 6.创建PARQUET数据存储格式表，不使用压缩，并使用g6_access中的数据 列式存储，生产中最常用，更高效的压缩和编码；123create table g6_access_parstored as PARQUETas select * from g6_access; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;结论：ORC文件不压缩，比源文件少了10多MB，ORC文件采用默认压缩，文件只有源文件的四分之一查看数据大小 58.3MB 7.创建PARQUET数据存储格式表，设置使用gzip压缩格式，并使用g6_access中的数据1234set parquet.compression=gzip;create table g6_access_par_zipstored as PARQUETas select * from g6_access; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;结论：parquet格式文件大小是源文件的1/4左右。生产上也是好的选择 8.小结TextFile默认格式，加载速度最快，可以采用Gzip进行压缩，压缩后的文件无法split，即并行处理。SequenceFile压缩率最低，查询速度一般，将数据存放到sequenceFile格式的hive表中，这时数据就会压缩存储。三种压缩格式NONE，RECORD，BLOCK。是可分割的文件格式。RCfile压缩率最高，查询速度最快，数据加载最慢。相比TEXTFILE和SEQUENCEFILE，RCFILE由于列式存储方式，数据加载时性能消耗较大，但是具有较好的压缩比和查询响应。数据仓库的特点是一次写入、多次读取，因此，整体来看，RCFILE相比其余两种格式具有较明显的优势。在hive中使用压缩需要灵活的方式，如果是数据源的话，采用RCFile+bz或RCFile+gz的方式，这样可以很大程度上节省磁盘空间；而在计算的过程中，为了不影响执行的速度，可以浪费一点磁盘空间，建议采用RCFile+snappy的方式，这样可以整体提升hive的执行速度。至于lzo的方式，也可以在计算过程中使用，只不过综合考虑（速度和压缩比）还是考虑snappy适宜。Parquet格式，列式存储，生产中最常用，更高效的压缩和编码； 五、读取数据量对比&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;直接执行 select 查询，观察日志 尾部HDFS Read: 190XXX ,就可知道读取数据量了。 六、总结&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;行存储：查询满足条件的一整行数据的时候，列存储则需要去每个聚集的字段找到对应的每个列的值，行存储只需要找到其中一个值，其余的值都在相邻地方，所以此时行存储查询的速度更快。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;列存储：因为每个字段的数据聚集存储，在查询只需要少数几个字段的时候，能大大减少读取的数据量；每个字段的数据类型一定是相同的，列式存储可以针对性的设计更好的设计压缩算法。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据仓库层次建设中： stage层：数据是从业务库直接拉取过来的，没有做任何改动，后续的ETL操作会读取该层的全部字段数据，而且适合更新、查看，与其他组件进行交互，所以，该层设置为行存储较为合适，常见为Text格式； ods层：从stage层获取数据，经过ETL清洗过程，得到格式统一（度量、维度等）的明细数据，后续事实表、中间表的建设会从这一层中获取某一部分数据，查询的是部分字段，因此，该层设置为列式存储比较合适，常见的文件格式为ORC、Parquet； dwd层：有些事实表、中间表字段非常地多，我们会把他们拆分为许多分组，然后汇总在一起，此时，各个分组可以设置为行存储，而汇总后的事实表、中间表设置为列存储会比较合适； report层/dws层：这一层数据量非常地小，主要作为报表层，数据集市在这里产生，分发给不同的小组或者部门查看分析用，故他们会查看所有的数据，这层设置为行存储会比较合适； &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上面是总结的一些经验，如果有异议，大家可以提出来一起讨论，数据仓库建设的路很漫长，我们一起一步一个脚印👣走起来。欢迎留言：chenzuoli709@163.com. Doing the things you want to do.]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
        <tag>文件格式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive开窗函数总结]]></title>
    <url>%2F2019%2F10%2F12%2FHive%E5%BC%80%E7%AA%97%E5%87%BD%E6%95%B0%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[背景： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;平常我们使用 hive 或者 mysql 时，一般聚合函数用的比较多。但对于某些偏分析的需求，group by可能很费力，子查询很多，这个时候就需要使用窗口分析函数了~ 注：hive、oracle提供开窗函数，mysql8之前版本不提供，但Oracle发布的 MySQL 8.0版本支持窗口函数（over）和公用表表达式（with）这两个重要的功能！ 一、介绍&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;分析函数用于计算基于组的某种聚合值，它和聚合函数的不同之处是：对于每个组返回多行，而聚合函数对于每个组只返回一行。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;开窗函数指定了分析函数工作的数据窗口大小，这个数据窗口大小可能会随着行的变化而变化！到底什么是数据窗口？后面举例会详细讲到！ 1.基础结构：1分析函数（如:sum(),max(),row_number()...） + 窗口子句（over函数） 2.over函数写法： over (partition by cookieid order by createtime) 先根据cookieid字段分区，相同的cookieid分为一区，每个分区内根据createtime字段排序（默认升序） 注：不加 partition by 的话则把整个数据集当作一个分区，不加 order by 的话会对某些函数统计结果产生影响，如sum() 3.测试数据：测试表test1只有三个字段 cookieid、createtime、pv 4.窗口含义：1234567SELECT cookieid,createtime,pv, SUM(pv) OVER(PARTITION BY cookieid ORDER BY createtime) AS pv1, -- 默认为从起点到当前行 SUM(pv) OVER(PARTITION BY cookieid ORDER BY createtime ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS pv2, --从起点到当前行，结果同pv1 SUM(pv) OVER(PARTITION BY cookieid ORDER BY createtime ROWS BETWEEN 3 PRECEDING AND CURRENT ROW) AS pv3, --当前行+往前3行 SUM(pv) OVER(PARTITION BY cookieid ORDER BY createtime ROWS BETWEEN 3 PRECEDING AND 1 FOLLOWING) AS pv4, --当前行+往前3行+往后1行 SUM(pv) OVER(PARTITION BY cookieid ORDER BY createtime ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING) AS pv5 ---当前行+往后所有行 FROM test1; 结果： cookieid createtime pv pv1 pv2 pv3 pv4 pv5 a 2017-12-01 3 3 3 3 3 3 b 2017-12-00 3 3 3 3 3 3 cookie1 2017-12-10 1 1 1 1 6 26 cookie1 2017-12-11 5 6 6 6 13 25 cookie1 2017-12-12 7 13 13 13 16 20 cookie1 2017-12-13 3 16 16 16 18 13 cookie1 2017-12-14 2 18 18 17 21 10 cookie1 2017-12-15 4 22 22 16 20 8 cookie1 2017-12-16 4 26 26 13 13 4 cookie2 2017-12-12 7 7 7 7 13 14 cookie2 2017-12-16 6 13 13 13 14 7 cookie2 2017-12-24 1 14 14 14 14 1 cookie3 2017-12-22 5 5 5 5 5 5 注：这些窗口的划分都是在分区内部！超过分区大小就无效了 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;相信大家看了后就会明白，如果不指定ROWS BETWEEN,默认统计窗口为从起点到当前行;如果不指定ORDER BY，则将分组内所有值累加; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;关键是理解 ROWS BETWEEN 含义,也叫做window子句：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;PRECEDING：往前&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;FOLLOWING：往后&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CURRENT ROW：当前行&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UNBOUNDED：无边界，UNBOUNDED PRECEDING 表示从最前面的起点开始， UNBOUNDED FOLLOWING：表示到最后面的终点&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其他AVG，MIN，MAX，和SUM用法一样 二、SUM 函数123select cookieid,createtime,pv, sum(pv) over(PARTITION BY cookieid ORDER BY createtime) as pv1 FROM test1; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;首先 PARTITION BY cookieid，根据cookieid分区，各分区之间默认根据字典顺序排序，ORDER BY createtime，指定的是分区内部的排序，默认为升序 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们可以清晰地看到，窗口函数和聚合函数的不同，sum()函数可以根据每一行的窗口返回各自行对应的值，有多少行记录就有多少个sum值，而group by只能计算每一组的sum，每组只有一个值！ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其中sum()计算的是分区内排序后一个个叠加的值，和order by有关！ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果不加 order by会咋样： 123select cookieid,createtime,pv, sum(pv) over(PARTITION BY cookieid) as pv1 FROM test1; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;可以看到，如果没有order by，不仅分区内没有排序，sum()计算的pv也是整个分区的pv &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;注：max()函数无论有没有order by 都是计算整个分区的最大值 三、NTILE 函数&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;NTILE(n)，用于将分组数据按照顺序切分成n片，返回当前切片值 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;注1：如果切片不均匀，默认增加第一个切片的分布&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;注2：NTILE不支持ROWS BETWEEN 12345SELECT cookieid,createtime,pv, NTILE(2) OVER(PARTITION BY cookieid ORDER BY createtime) AS ntile1, --分组内将数据分成2片 NTILE(3) OVER(PARTITION BY cookieid ORDER BY createtime) AS ntile2, --分组内将数据分成3片 NTILE(4) OVER(PARTITION BY cookieid ORDER BY createtime) AS ntile3 --将所有数据分成4片FROM test1; 用法举例：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;统计一个cookie，pv数最多的前1/3的天： 123SELECT cookieid,createtime,pv, NTILE(3) OVER(PARTITION BY cookieid ORDER BY pv DESC) AS ntile FROM test1; 取 ntile = 1 的记录，就是我们想要的结果！ 四、ROW_NUMBER 函数&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ROW_NUMBER() 从1开始，按照顺序，生成分组内记录的序列 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ROW_NUMBER() 的应用场景非常多，比如获取分组内排序第一的记录、获取一个session中的第一条refer等。 123SELECT cookieid,createtime,pv, ROW_NUMBER() OVER(PARTITION BY cookieid ORDER BY pv desc) AS rn FROM test1; 五、RANK 和 DENSE_RANK 函数&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;RANK() 生成数据项在分组中的排名，排名相等会在名次中留下空位&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;DENSE_RANK() 生成数据项在分组中的排名，排名相等会在名次中不会留下空位 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们把 rank、dense_rank、row_number三者对比，这样比较清晰： 12345SELECT cookieid,createtime,pv, RANK() OVER(PARTITION BY cookieid ORDER BY pv desc) AS rank1, DENSE_RANK() OVER(PARTITION BY cookieid ORDER BY pv desc) AS d_rank2, ROW_NUMBER() OVER(PARTITION BY cookieid ORDER BY pv DESC) AS rn3 FROM test1; 六、CUME_DIST 函数&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;cume_dist 返回小于等于当前值的行数/分组内总行数 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;比如，我们可以统计小于等于当前薪水的人数，所占总人数的比例 1234SELECT cookieid,createtime,pv, round(CUME_DIST() OVER(ORDER BY pv),2) AS cd1, round(CUME_DIST() OVER(PARTITION BY cookieid ORDER BY pv),2) AS cd2 FROM test1; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;注：cd1没有partition,所有数据均为1组！ 七、PERCENT_RANK 函数&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;percent_rank 分组内当前行的RANK值-1/分组内总行数-1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;注：一般不会用到该函数，可能在一些特殊算法的实现中可以用到吧 123SELECT cookieid,createtime,pv, PERCENT_RANK() OVER(ORDER BY pv) AS rn1 from test1; 八、LAG 和 LEAD 函数&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LAG(col,n,DEFAULT) 用于统计窗口内往上第n行值 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第一个参数为列名，第二个参数为往上第n行（可选，默认为1），第三个参数为默认值（当往上第n行为NULL时候，取默认值，如不指定，则为NULL） 12345SELECT cookieid,createtime,pv, ROW_NUMBER() OVER(PARTITION BY cookieid ORDER BY createtime) AS rn, LAG(createtime,1,&apos;1970-01-01&apos;) OVER(PARTITION BY cookieid ORDER BY createtime) AS lag1, LAG(createtime,2) OVER(PARTITION BY cookieid ORDER BY createtime) AS lag2 FROM test1; LEAD 函数则与 LAG 相反： LEAD(col,n,DEFAULT) 用于统计窗口内往下第n行值 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第一个参数为列名，第二个参数为往下第n行（可选，默认为1），第三个参数为默认值（当往下第n行为NULL时候，取默认值，如不指定，则为NULL） 九、FIRST_VALUE 和 LAST_VALUE 函数&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;FIRST_VALUE 取分组内排序后，截止到当前行，第一个值 1234SELECT cookieid,createtime,pv, ROW_NUMBER() OVER(PARTITION BY cookieid ORDER BY createtime) AS rn, FIRST_VALUE(pv) OVER(PARTITION BY cookieid ORDER BY createtime) AS first FROM test1; LAST_VALUE 函数则相反： LAST_VALUE 取分组内排序后，截止到当前行，最后一个值 这两个函数还是经常用到的（往往和排序配合使用），比较实用！ It’s never too late to learn.]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
        <tag>开窗函数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据仓库的现状及优化改进意见]]></title>
    <url>%2F2019%2F10%2F09%2F%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E7%9A%84%E7%8E%B0%E7%8A%B6%E5%8F%8A%E4%BC%98%E5%8C%96%E6%94%B9%E8%BF%9B%E6%84%8F%E8%A7%81%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;刚离职的这家公司，数据仓库可以提供基本的功能：基于时间的历史数据查询，多维展示报表，决策支持、数据质量监控等，但是也有许多问题，下面来看看这些问题及解决方案，不知道同行们有没有遇到过。 介绍&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;首先介绍下具体业务场景，产品是金融类app，有贷款、理财2条业务线，有贷款、理财后台、前端、风控等主体业务，数据仓库进行了分层设计：db、ods、dwd、dws、report、es，其中： db、ods主要作为数据仓库详细数据层次，主要是一些ETL的工作，用sqoop增量拉取数据后做一些加解密、字段转换、度量单位转换等清洗工作； dwd是数据中间层，有一些数据大宽表在这个层次； dws和report作为数据集市层，对外提供数据结果； es对等report，直接将hive的数据导入到es的，没有做任何操作，主要是es查询快，提供给可视化工具和API使用； &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;没有OLAP系统，因为数据量没达到量级，跟企业级的数据仓库还是有一些差距的。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;airflow作为调度系统，目前没什么大问题，遇到一个实际使用slot数超过pool设置的size限制bug，临时clear掉是可以解决问题的，但不知道怎么彻底解决。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;报表工具使用开源的cboard，当数据量级上到10万，查询es就有些慢了，大家有什么好的工具推荐吗？ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这是目前的数据仓库结构及运行模式，下面来看下一些问题及解决方案。 问题及解决方案1.拉链表运行缓慢问题：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果对该仓库结构不清楚的人，真没法下手维护。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由于业务的原因，有些业务表有更新操作，于是我们仓库选择拉链表来对付更新表。拉链表的逻辑就是使用生效日期作为hive表的分区字段，然后每次更新数据的时候，与新增数据进行join，得到最新的数据，然后使用动态分区方式insert overwrite到目标拉链表中。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;开始设计的时候，所有表（有更新的和无更新的）都做拉链表，没有想到，数据量达到千万级后，做拉链表非常地慢，千万级的表做完拉链表需要3小时，严重影响后面维度表、报表的运行。 解决方案：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a.对业务表进行分组，有更新的表才做拉链表，无更新的表做完清洗工作即放入目标表即可;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;b.使用失效日期作为拉链表分区字段，那么就不需要每次操作所有的分区，而只是操作最后一个分区和当天分区就可以了，速度可以提高十几倍。 2.数据结构混乱问题：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;做拉链表需要用到2个中间表tmp1、tmp2，目前跟目标拉链表在同一个库下，导致有三分之二的表是中间临时表，而业务人员去使用的时候，就会问我应该使用那张表的问题。 解决方案：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;将中间临时表放入另外一个库下，不对外开放权限。 3.数据查找混乱问题：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;无元数据管理，当需要用到某部分结果数据时，它对应哪个库哪个表，表字段属性注释等等信息，当数据表非常多时，单凭记忆是无法满足条件的，这个时候元数据的管理就至关重要了。 解决方案：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;建立元数据管理系统，目前使用的较多的开源框架为grafana，集元数据管理、血缘关系、数据质量监控于一体，开源解决这个问题。 4.调度系统卡住问题：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们使用airflow的pool功能进行把控任务的调度，实现数据仓库层次的建设工作，每天按时跑。但是pool大小是有限制的，系统总的资源也是有限的（相信大部分公司资源都不是很足），分层设计之后，每层的任务会依赖上一层次的任务，上层任务运行成功后才会运行下层任务，在上层任务未运行或者运行失败的情况下，下层任务一直处于wait for状态，这些wait for任务都占用着资源，会导致那些上层任务运行成功的下层任务因为没有系统资源而无法运行。 解决方案：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;可以重组airflow dags，合并仓库层次airflow dag脚本，保持仓库层次依然清晰，减少wait for任务的数量。 优化1.增加元数据管理功能&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;开源的工具可以选择Grafana，另外Kettle还具有ETL的功能，商业软件可以IBM的Datastage，另一个就是Informatica，具体如下： 2.增加OLAP分析系统&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如何选择OLAP工具，可以参考这篇文章：开源OLAP系统对比 3.仓库层次结构分组&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;临时表、中间表、db、ods、dwd、dws、report、es应层次分明，在不同的库存储。]]></content>
      <categories>
        <category>数据仓库</category>
      </categories>
      <tags>
        <tag>OLAP</tag>
        <tag>数据仓库</tag>
        <tag>元数据管理</tag>
        <tag>数据质量</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[开源OLAP系统对比]]></title>
    <url>%2F2019%2F10%2F09%2F%E5%BC%80%E6%BA%90OLAP%E7%B3%BB%E7%BB%9F%E5%AF%B9%E6%AF%94%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;OLAP系统，基于数据仓库，为数据价值而生，提供复杂的数据分析操作，及直观易懂的查询结果，目前市场上有许多开源的OLAP框架，如Impala、Presto、Druid、Kylin、Spark SQL等，他们的关系又是怎样的呢，我们应该怎样选择框架？ 1.Presto&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;facebook开源的一个java写的分布式数据查询框架，原生集成了hive、hbase和关系型数据库，Presto背后所使用的执行模式与hive有本质的不同，它没有使用MapReduce，比hive快一个数据量级，其中的关键是所有的计算都在内存中完成。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;缺点就是容易造成oom。 2.Druid&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Druid是一个实时处理时序数据的OLAP系统，采用预计算的方式，因为它的索引首先是按照时间的方式分片，查询的时候也是按照时间片去路由索引的。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;缺点就是sql支持不太好，需要使用它自己的方言来实现。 3.Spark SQL&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;基于spark平台的一个OLAP框架，本质上也是基于DAG的MPP方案，基本思路是增加机器来提高并行计算能力，从而提高查询响应速度。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;缺点就是只能使用sql去即时查询，一次查询结果不能复用。 4.Kylin&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Ebay开源框架，支持大数据生态圈的数据分析业务，核心是Cube，cube是一种预计算技术，主要是通过预计算的方式将用户预定义的多维立方体cube缓存起来，达到提高查询速度的目的，应用场景是针对复杂的sql join操作情况。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;用空间来换时间，存储空间一般是数据仓库的10倍+。 5.Impala&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Cloudera公司开源，能查询HDFS和Hbase中的数据，同Hive一样，也是一种SQL on Hadoop解决方案，它抛弃了MapReduce，而是使用传统的MPP数据库方式来提高查询速度。 You never know what’s coming for you, so do it and don’t care about the result.]]></content>
      <categories>
        <category>数据仓库</category>
      </categories>
      <tags>
        <tag>OLAP</tag>
        <tag>数据仓库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据仓库设计]]></title>
    <url>%2F2019%2F10%2F09%2F%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E8%AE%BE%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据仓库是一个面向主题的、集成的、稳定的、反映历史变化的数据集合，这篇文章讨论数据仓库设计的一个思路，不谈设计模式、分层设计思想等。 创新性的技术：建立所谓的创新性索引或创造性概要文件如下图：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;创造性索引是当数据由操作型环境转移到数据仓库环境时建立起来的。由于在任何情况下都要对每个数据单元进行处理，所以就这一点来说，计算或建立索引只需要很少的开销。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;创造性索引为最终用户感兴趣的项目建立一个概要文件描述，比如最大的购买额，最不活跃的账户，最近发出的货物，等等。如果在将数据传到数据仓库的时候，对于管理活动有价值的需求能够预见得到（不得不承认，这在很多情况下是不能的），那么建造创造性索引就很有意义了。 要明确的设计技术：参照完整性的管理&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在数据仓库环境中，参照完整性以“人工关系”的方式出现。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在操作型环境中，参照完整性表现为数据表之间的动态连接。由于在数据仓库环境中的数据量很大、数据仓库是不更新的、仓库按照时间描述数据、关系不是静态的，因此，应采取不同的方式表示参照完整性。换句话说，数据的关系在数据仓库环境中采用人工关系表示。这意味着有些数据要复制，有些数据要删除，而其他数据要仍然保留在数据仓库中。总之，试图在数据仓库环境中复制参照完整性显然是一种不正确的做法。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;看下图： We’re meant to lose the people we love, how else would we know how important they are to us?]]></content>
      <categories>
        <category>数据仓库</category>
      </categories>
      <tags>
        <tag>数据仓库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一致性共识算法PBFT简介]]></title>
    <url>%2F2019%2F10%2F07%2F%E4%B8%80%E8%87%B4%E6%80%A7%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95PBFT%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;PBFT是Practical Byzantine Fault Tolerance的缩写，即：实用拜占庭容错算法。主要用于联盟链，下面来看下介绍吧。 摘要&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;PBFT是Practical Byzantine Fault Tolerance的缩写，即：实用拜占庭容错算法。该算法是Miguel Castro（卡斯特罗）和Barbara Liskov（利斯科夫）在1999年提出来的，解决了原始拜占庭容错算法效率不高的问题，算法的时间复杂度是O(n^2)，使得在实际系统应用中可以解决拜占庭容错问题。该论文发表在1999年的操作系统设计与实现国际会议上（OSDI99）。其中Barbara Liskov就是提出了著名的里氏替换原则（LSP）的人，2008年图灵奖得主。以下拜占庭容错问题简称BFT。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;BFT是区块链共识算法中，需要解决的一个核心问题，以比特币和以太访为代表的POW，EOS为代表的DPOS，以及今后以太访逐渐替换的共识算法POS，这些都是公链算法，解决的是共识节点众多情况下的BFT。而PBFT是在联盟链共识节点较少的情况下BFT的一种解决方案。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;网上已经有很多关于PBFT算法的描述，但是写的都不是很明白，本文以一种更为清晰易懂的方法，彻底讲明白PBFT算法原理。下一篇文章将会结合fabric-0.6.0-preview 中的代码，讲解超级账本项目是如何实现PBFT算法的。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;本文假设读者已经理解什么是BFT问题。 PBFT算法流程&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;PBFT算法前提，采用密码学算法保证节点之间的消息传送是不可篡改的。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;PBFT容忍无效或者恶意节点数：f，为了保障整个系统可以正常运转，需要有2f+1个正常节点，系统的总节点数为：|R| = 3f + 1。也就是说，PBFT算法可以容忍小于1/3个无效或者恶意节点，该部分的原理证明请参考PBFT论文，下文有链接地址。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;PBFT是一种状态机副本复制算法，所有的副本在一个视图（view）轮换的过程中操作，主节点通过视图编号以及节点数集合来确定，即：主节点 p = v mod |R|。v：视图编号，|R|节点个数，p：主节点编号。 PBFT算法主体实现流程图如下： PBFT算法流程 以下详细说明，每个主体流程内容： 1. REQUEST：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;客户端c向主节点p发送&lt;REQUEST, o, t, c&gt;请求。o: 请求的具体操作，t: 请求时客户端追加的时间戳，c：客户端标识。REQUEST: 包含消息内容m，以及消息摘要d(m)。客户端对请求进行签名。 2. PRE-PREPARE：主节点收到客户端的请求，需要进行以下交验： a. 客户端请求消息签名是否正确。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;非法请求丢弃。正确请求，分配一个编号n，编号n主要用于对客户端的请求进行排序。然后广播一条&lt;&lt;PRE-PREPARE, v, n, d&gt;, m&gt;消息给其他副本节点。v：视图编号，d客户端消息摘要，m消息内容。&lt;PRE-PREPARE, v, n, d&gt;进行主节点签名。n是要在某一个范围区间内的[h, H]，具体原因参见垃圾回收章节。 3. PREPARE：副本节点i收到主节点的PRE-PREPARE消息，需要进行以下交验： a. 主节点PRE-PREPARE消息签名是否正确。 b. 当前副本节点是否已经收到了一条在同一v下并且编号也是n，但是签名不同的PRE-PREPARE信息。 c. d与m的摘要是否一致。 d. n是否在区间[h, H]内。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;非法请求丢弃。正确请求，副本节点i向其他节点包括主节点发送一条&lt;PREPARE, v, n, d, i&gt;消息, v, n, d, m与上述PRE-PREPARE消息内容相同，i是当前副本节点编号。&lt;PREPARE, v, n, d, i&gt;进行副本节点i的签名。记录PRE-PREPARE和PREPARE消息到log中，用于View Change过程中恢复未完成的请求操作。 4. COMMIT：主节点和副本节点收到PREPARE消息，需要进行以下交验： a. 副本节点PREPARE消息签名是否正确。 b. 当前副本节点是否已经收到了同一视图v下的n。 c. n是否在区间[h, H]内。 d. d是否和当前已收到PRE-PPREPARE中的d相同 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;非法请求丢弃。如果副本节点i收到了2f+1个验证通过的PREPARE消息，则向其他节点包括主节点发送一条&lt;COMMIT, v, n, d, i&gt;消息，v, n, d, i与上述PREPARE消息内容相同。&lt;COMMIT, v, n, d, i&gt;进行副本节点i的签名。记录COMMIT消息到日志中，用于View Change过程中恢复未完成的请求操作。记录其他副本节点发送的PREPARE消息到log中。 5. REPLY：主节点和副本节点收到COMMIT消息，需要进行以下交验： a. 副本节点COMMIT消息签名是否正确。 b. 当前副本节点是否已经收到了同一视图v下的n。 c. d与m的摘要是否一致。 d. n是否在区间[h, H]内。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;非法请求丢弃。如果副本节点i收到了2f+1个验证通过的COMMIT消息，说明当前网络中的大部分节点已经达成共识，运行客户端的请求操作o，并返回&lt;REPLY, v, t, c, i, r&gt;给客户端，r：是请求操作结果，客户端如果收到f+1个相同的REPLY消息，说明客户端发起的请求已经达成全网共识，否则客户端需要判断是否重新发送请求给主节点。记录其他副本节点发送的COMMIT消息到log中。 总结&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;PBFT算法由于每个副本节点都需要和其他节点进行P2P的共识同步，因此随着节点的增多，性能会下降的很快，但是在较少节点的情况下可以有不错的性能，并且分叉的几率很低。PBFT主要用于联盟链，但是如果能够结合类似DPOS这样的节点代表选举规则的话也可以应用于公链，并且可以在一个不可信的网络里解决拜占庭容错问题，TPS应该是远大于POW的。 参考拜占庭容错(Byzantine Fault Tolerance) WIKI: BFT-Wikipedia PBFT论文地址：PBFT论文 The shortest answer is doing.]]></content>
      <categories>
        <category>共识算法</category>
      </categories>
      <tags>
        <tag>共识算法</tag>
        <tag>PBFT</tag>
        <tag>区块链</tag>
        <tag>联盟链</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式一致性共识算法Paxos、Raft、ZAB、ETCD之间的关系]]></title>
    <url>%2F2019%2F10%2F01%2F%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95Paxos%E3%80%81Raft%E3%80%81ZAB%E3%80%81ETCD%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在分布式系统中，多个节点之间达成共识成为最重要的组成部分，在发展过程中，出现许多类型的算法，他们从最基本的达成共识，到易于理解，更接近于实践应用等等方面，做到了极致，下面来看看Paxos、Raft、ZAB、Etcd之间的不同。 为了解决分布式系统中的一致性问题，科学家们首先提出了Paxos算法，但是Paxos流程太过繁杂，不易于理解，应用主要有Chubby、libpaxos； 斯坦福大学的2个人以易于理解为目标，又能实现Paxos所解决的问题，于是实现了Raft算法，到现在已经有了十多种语言的Raft算法实现框架，较为出名的有etcd，Google的Kubernetes也是用了etcd作为他的服务发现框架； Zab与Paxos不同，但是有些地方是从Paxos那里学习过来的，比如Leader发送心跳、议案、决定等给Follower；Leader在提交（commit）议案之前需要经过一定量的Follower的确认；算法实现最经典的应用为zookeeper； 但Etcd和Zab不适合分布式大数据存储，主要做分布式元数据的存储； when your ability don’t support your ambition, then you should learn down-to-earth.]]></content>
      <categories>
        <category>共识算法</category>
      </categories>
      <tags>
        <tag>共识算法</tag>
        <tag>分布式</tag>
        <tag>Paxos</tag>
        <tag>Raft</tag>
        <tag>Zab</tag>
        <tag>Etcd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一致性共识算法ETCD解析]]></title>
    <url>%2F2019%2F10%2F01%2F%E4%B8%80%E8%87%B4%E6%80%A7%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95ETCD%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;前一段时间的项目里用到了 Etcd， 所以研究了一下它的源码以及实现。网上关于 Etcd 的使用介绍的文章不少，但分析具体架构实现的文章不多，同时 Etcd v3的文档也非常稀缺。本文通过分析 Etcd 的架构与实现，了解其优缺点以及瓶颈点，一方面可以学习分布式系统的架构，另外一方面也可以保证在业务中正确使用 Etcd，知其然同时知其所以然，避免误用。最后介绍 Etcd 周边的工具和一些使用注意事项。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;阅读对象：分布式系统爱好者，正在或者打算在项目中使用Etcd的开发人员。 Etcd 按照官方介绍1Etcd is a distributed, consistent key-value store for shared configuration and service discovery &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;是一个分布式的，一致的 key-value 存储，主要用途是共享配置和服务发现。Etcd 已经在很多分布式系统中得到广泛的使用，本文的架构与实现部分主要解答以下问题： Etcd是如何实现一致性的？ Etcd的存储是如何实现的？ Etcd的watch机制是如何实现的？ Etcd的key过期机制是如何实现的？ 为什么需要 Etcd ？&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;所有的分布式系统，都面临的一个问题是多个节点之间的数据共享问题，这个和团队协作的道理是一样的，成员可以分头干活，但总是需要共享一些必须的信息，比如谁是 leader， 都有哪些成员，依赖任务之间的顺序协调等。所以分布式系统要么自己实现一个可靠的共享存储来同步信息（比如 Elasticsearch ），要么依赖一个可靠的共享存储服务，而 Etcd 就是这样一个服务。 Etcd 提供什么能力？&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Etcd 主要提供以下能力，已经熟悉 Etcd 的读者可以略过本段。 提供存储以及获取数据的接口，它通过协议保证 Etcd 集群中的多个节点数据的强一致性。用于存储元信息以及共享配置。 提供监听机制，客户端可以监听某个key或者某些key的变更（v2和v3的机制不同，参看后面文章）。用于监听和推送变更。 提供key的过期以及续约机制，客户端通过定时刷新来实现续约（v2和v3的实现机制也不一样）。用于集群监控以及服务注册发现。 提供原子的CAS（Compare-and-Swap）和 CAD（Compare-and-Delete）支持（v2通过接口参数实现，v3通过批量事务实现）。用于分布式锁以及leader选举。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;更详细的使用场景不在这里描述，有兴趣的可以参看文末infoq的一篇文章。 Etcd 如何实现一致性的？&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;说到这个就不得不说起raft协议。但这篇文章不是专门分析raft的，篇幅所限，不能详细分析，有兴趣的建议看文末原始论文地址以及raft协议的一个动画。便于看后面的文章，我这里简单做个总结： raft通过对不同的场景（选主，日志复制）设计不同的机制，虽然降低了通用性（相对paxos），但同时也降低了复杂度，便于理解和实现。 raft内置的选主协议是给自己用的，用于选出主节点，理解raft的选主机制的关键在于理解raft的时钟周期以及超时机制。 理解 Etcd 的数据同步的关键在于理解raft的日志同步机制。 Etcd 实现raft的时候，充分利用了go语言CSP并发模型和chan的魔法，想更进行一步了解的可以去看源码，这里只简单分析下它的wal日志。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;wal日志是二进制的，解析出来后是以上数据结构LogEntry。其中第一个字段type，只有两种，一种是0表示Normal，1表示ConfChange（ConfChange表示 Etcd 本身的配置变更同步，比如有新的节点加入等）。第二个字段是term，每个term代表一个主节点的任期，每次主节点变更term就会变化。第三个字段是index，这个序号是严格有序递增的，代表变更序号。第四个字段是二进制的data，将raft request对象的pb结构整个保存下。Etcd 源码下有个tools/etcd-dump-logs，可以将wal日志dump成文本查看，可以协助分析raft协议。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;raft协议本身不关心应用数据，也就是data中的部分，一致性都通过同步wal日志来实现，每个节点将从主节点收到的data apply到本地的存储，raft只关心日志的同步状态，如果本地存储实现的有bug，比如没有正确的将data apply到本地，也可能会导致数据不一致。 Etcd v2 与 v3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Etcd v2 和 v3 本质上是共享同一套 raft 协议代码的两个独立的应用，接口不一样，存储不一样，数据互相隔离。也就是说如果从 Etcd v2 升级到 Etcd v3，原来v2 的数据还是只能用 v2 的接口访问，v3 的接口创建的数据也只能访问通过 v3 的接口访问。所以我们按照 v2 和 v3 分别分析。 Etcd v2 存储，Watch以及过期机制 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Etcd v2 是个纯内存的实现，并未实时将数据写入到磁盘，持久化机制很简单，就是将store整合序列化成json写入文件。数据在内存中是一个简单的树结构。比如以下数据存储到 Etcd 中的结构就如图所示。 12/nodes/1/name node1 /nodes/1/ip 192.168.1.1 store中有一个全局的currentIndex，每次变更，index会加1.然后每个event都会关联到currentIndex. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当客户端调用watch接口（参数中增加 wait参数）时，如果请求参数中有waitIndex，并且waitIndex 小于 currentIndex，则从 EventHistroy 表中查询index小于等于waitIndex，并且和watch key 匹配的 event，如果有数据，则直接返回。如果历史表中没有或者请求没有带 waitIndex，则放入WatchHub中，每个key会关联一个watcher列表。 当有变更操作时，变更生成的event会放入EventHistroy表中，同时通知和该key相关的watcher。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里有几个影响使用的细节问题： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;EventHistroy 是有长度限制的，最长1000。也就是说，如果你的客户端停了许久，然后重新watch的时候，可能和该waitIndex相关的event已经被淘汰了，这种情况下会丢失变更。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果通知watch的时候，出现了阻塞（每个watch的channel有100个缓冲空间），Etcd 会直接把watcher删除，也就是会导致wait请求的连接中断，客户端需要重新连接。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Etcd store的每个node中都保存了过期时间，通过定时机制进行清理。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从而可以看出，Etcd v2 的一些限制： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;过期时间只能设置到每个key上，如果多个key要保证生命周期一致则比较困难。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;watch只能watch某一个key以及其子节点（通过参数 recursive),不能进行多个watch。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;很难通过watch机制来实现完整的数据同步（有丢失变更的风险），所以当前的大多数使用方式是通过watch得知变更，然后通过get重新获取数据，并不完全依赖于watch的变更event。 Etcd v3 存储，Watch以及过期机制 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Etcd v3 将watch和store拆开实现，我们先分析下store的实现。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Etcd v3 store 分为两部分，一部分是内存中的索引，kvindex，是基于google开源的一个golang的btree实现的，另外一部分是后端存储。按照它的设计，backend可以对接多种存储，当前使用的boltdb。boltdb是一个单机的支持事务的kv存储，Etcd 的事务是基于boltdb的事务实现的。Etcd 在boltdb中存储的key是reversion，value是 Etcd 自己的key-value组合，也就是说 Etcd 会在boltdb中把每个版本都保存下，从而实现了多版本机制。 举个例子： 用etcdctl通过批量接口写入两条记录： 12345etcdctl txn &lt;&lt;&lt;&apos; put key1 &quot;v1&quot; put key2 &quot;v2&quot; &apos; 再通过批量接口更新这两条记录： 12345etcdctl txn &lt;&lt;&lt;&apos; put key1 &quot;v12&quot; put key2 &quot;v22&quot; &apos; boltdb中其实有了4条数据： 1234rev=&#123;3 0&#125;, key=key1, value=&quot;v1&quot; rev=&#123;3 1&#125;, key=key2, value=&quot;v2&quot; rev=&#123;4 0&#125;, key=key1, value=&quot;v12&quot; rev=&#123;4 1&#125;, key=key2, value=&quot;v22&quot; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;reversion主要由两部分组成，第一部分main rev，每次事务进行加一，第二部分sub rev，同一个事务中的每次操作加一。如上示例，第一次操作的main rev是3，第二次是4。当然这种机制大家想到的第一个问题就是空间问题，所以 Etcd 提供了命令和设置选项来控制compact，同时支持put操作的参数来精确控制某个key的历史版本数。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;了解了 Etcd 的磁盘存储，可以看出如果要从boltdb中查询数据，必须通过reversion，但客户端都是通过key来查询value，所以 Etcd 的内存kvindex保存的就是key和reversion之前的映射关系，用来加速查询。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;然后我们再分析下watch机制的实现。Etcd v3 的watch机制支持watch某个固定的key，也支持watch一个范围（可以用于模拟目录的结构的watch），所以 watchGroup 包含两种watcher，一种是 key watchers，数据结构是每个key对应一组watcher，另外一种是 range watchers, 数据结构是一个 IntervalTree（不熟悉的参看文文末链接），方便通过区间查找到对应的watcher。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;同时，每个 WatchableStore 包含两种 watcherGroup，一种是synced，一种是unsynced，前者表示该group的watcher数据都已经同步完毕，在等待新的变更，后者表示该group的watcher数据同步落后于当前最新变更，还在追赶。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当 Etcd 收到客户端的watch请求，如果请求携带了revision参数，则比较请求的revision和store当前的revision，如果大于当前revision，则放入synced组中，否则放入unsynced组。同时 Etcd 会启动一个后台的goroutine持续同步unsynced的watcher，然后将其迁移到synced组。也就是这种机制下，Etcd v3 支持从任意版本开始watch，没有v2的1000条历史event表限制的问题（当然这是指没有compact的情况下）。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;另外我们前面提到的，Etcd v2在通知客户端时，如果网络不好或者客户端读取比较慢，发生了阻塞，则会直接关闭当前连接，客户端需要重新发起请求。Etcd v3为了解决这个问题，专门维护了一个推送时阻塞的watcher队列，在另外的goroutine里进行重试。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Etcd v3 对过期机制也做了改进，过期时间设置在lease上，然后key和lease关联。这样可以实现多个key关联同一个lease id，方便设置统一的过期时间，以及实现批量续约。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;相比Etcd v2, Etcd v3的一些主要变化： 接口通过grpc提供rpc接口，放弃了v2的http接口。优势是长连接效率提升明显，缺点是使用不如以前方便，尤其对不方便维护长连接的场景。 废弃了原来的目录结构，变成了纯粹的kv，用户可以通过前缀匹配模式模拟目录。 内存中不再保存value，同样的内存可以支持存储更多的key。 watch机制更稳定，基本上可以通过watch机制实现数据的完全同步。 提供了批量操作以及事务机制，用户可以通过批量事务请求来实现Etcd v2的CAS机制（批量事务支持if条件判断）。 Etcd，Zookeeper，Consul 比较&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这三个产品是经常被人拿来做选型比较的。 Etcd 和 Zookeeper 提供的能力非常相似，都是通用的一致性元信息存储，都提供watch机制用于变更通知和分发，也都被分布式系统用来作为共享信息存储，在软件生态中所处的位置也几乎是一样的，可以互相替代的。二者除了实现细节，语言，一致性协议上的区别，最大的区别在周边生态圈。Zookeeper 是apache下的，用java写的，提供rpc接口，最早从hadoop项目中孵化出来，在分布式系统中得到广泛使用（hadoop, solr, kafka, mesos 等）。Etcd 是coreos公司旗下的开源产品，比较新，以其简单好用的rest接口以及活跃的社区俘获了一批用户，在新的一些集群中得到使用（比如kubernetes）。虽然v3为了性能也改成二进制rpc接口了，但其易用性上比 Zookeeper 还是好一些。 而 Consul 的目标则更为具体一些，Etcd 和 Zookeeper 提供的是分布式一致性存储能力，具体的业务场景需要用户自己实现，比如服务发现，比如配置变更。而Consul 则以服务发现和配置变更为主要目标，同时附带了kv存储。 在软件生态中，越抽象的组件适用范围越广，但同时对具体业务场景需求的满足上肯定有不足之处。 Etcd 的周边工具Confd&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在分布式系统中，理想情况下是应用程序直接和 Etcd 这样的服务发现/配置中心交互，通过监听 Etcd 进行服务发现以及配置变更。但我们还有许多历史遗留的程序，服务发现以及配置大多都是通过变更配置文件进行的。Etcd 自己的定位是通用的kv存储，所以并没有像 Consul 那样提供实现配置变更的机制和工具，而 Confd 就是用来实现这个目标的工具。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Confd 通过watch机制监听 Etcd 的变更，然后将数据同步到自己的一个本地存储。用户可以通过配置定义自己关注那些key的变更，同时提供一个配置文件模板。Confd 一旦发现数据变更就使用最新数据渲染模板生成配置文件，如果新旧配置文件有变化，则进行替换，同时触发用户提供的reload脚本，让应用程序重新加载配置。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Confd 相当于实现了部分 Consul 的agent以及consul-template的功能，作者是kubernetes的Kelsey Hightower，但大神貌似很忙，没太多时间关注这个项目了，很久没有发布版本，我们着急用，所以fork了一份自己更新维护，主要增加了一些新的模板函数以及对metad后端的支持。confd Metad&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;服务注册的实现模式一般分为两种，一种是调度系统代为注册，一种是应用程序自己注册。调度系统代为注册的情况下，应用程序启动后需要有一种机制让应用程序知道『我是谁』，然后发现自己所在的集群以及自己的配置。Metad 提供这样一种机制，客户端请求 Metad 的一个固定的接口 /self，由 Metad 告知应用程序其所属的元信息，简化了客户端的服务发现和配置变更逻辑。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Metad 通过保存一个ip到元信息路径的映射关系来做到这一点，当前后端支持Etcd v3，提供简单好用的 http rest 接口。 它会把 Etcd 的数据通过watch机制同步到本地内存中，相当于 Etcd 的一个代理。所以也可以把它当做Etcd 的代理来使用，适用于不方便使用 Etcd v3的rpc接口或者想降低 Etcd 压力的场景。 metad Etcd 集群一键搭建脚本&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Etcd 官方那个一键搭建脚本有bug，我自己整理了一个脚本，通过docker的network功能，一键搭建一个本地的 Etcd 集群便于测试和试验。一键搭建脚本 Etcd 使用注意事项Etcd cluster 初始化的问题&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果集群第一次初始化启动的时候，有一台节点未启动，通过v3的接口访问的时候，会报告Error: Etcdserver: not capable 错误。这是为兼容性考虑，集群启动时默认的API版本是2.3，只有当集群中的所有节点都加入了，确认所有节点都支持v3接口时，才提升集群版本到v3。这个只有第一次初始化集群的时候会遇到，如果集群已经初始化完毕，再挂掉节点，或者集群关闭重启（关闭重启的时候会从持久化数据中加载集群API版本），都不会有影响。 Etcd 读请求的机制&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;v2 quorum=true 的时候，读取是通过raft进行的，通过cli请求，该参数默认为true。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;v3 –consistency=“l” 的时候（默认）通过raft读取，否则读取本地数据。sdk 代码里则是通过是否打开：WithSerializable option 来控制。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;一致性读取的情况下，每次读取也需要走一次raft协议，能保证一致性，但性能有损失，如果出现网络分区，集群的少数节点是不能提供一致性读取的。但如果不设置该参数，则是直接从本地的store里读取，这样就损失了一致性。使用的时候需要注意根据应用场景设置这个参数，在一致性和可用性之间进行取舍。 Etcd 的 compact 机制&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Etcd 默认不会自动 compact，需要设置启动参数，或者通过命令进行compact，如果变更频繁建议设置，否则会导致空间和内存的浪费以及错误。Etcd v3 的默认的 backend quota 2GB，如果不 compact，boltdb 文件大小超过这个限制后，就会报错：”Error: etcdserver: mvcc: database space exceeded”，导致数据无法写入。 脑洞时间&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;自动上次 Elasticsearch 的文章之后，给自己安排了一个作业，每次分析源码后需要提出几个发散思维的想法，开个脑洞。 并发代码调用分析追踪工具&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当前IDE的代码调用分析追踪都是通过静态的代码分析来追踪方法调用链实现的，对阅读分析代码非常有用。但程序如果充分使用CSP或者Actor模型后，都通过消息进行调用，没有了明确的方法调用链，给阅读和理解代码带来了困难。如果语言或者IDE能支持这样的消息投递追踪分析，那应该非常有用。当然我这个只是脑洞，不考虑实现的可能性和复杂度。 实现一个通用的 multiple group raft库&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当前 Etcd 的raft实现保证了多个节点数据之间的同步，但明显的一个问题就是扩充节点不能解决容量问题。要想解决容量问题，只能进行分片，但分片后如何使用raft同步数据？只能实现一个 multiple group raft，每个分片的多个副本组成一个虚拟的raft group，通过raft实现数据同步。当前实现了multiple group raft的有 TiKV 和 Cockroachdb，但尚未一个独立通用的。理论上来说，如果有了这套 multiple group raft，后面挂个持久化的kv就是一个分布式kv存储，挂个内存kv就是分布式缓存，挂个lucene就是分布式搜索引擎。当然这只是理论上，要真实现复杂度还是不小。 Etcd 的开源产品启示&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Etcd在Zookeeper已经奠定江湖地位的情况下，硬是重新造了一个轮子，并且在生态圈中取得了一席之地。一方面可以看出是社区的形态在变化，沟通机制和对用户反馈的响应越来越重要，另外一方面也可以看出一个项目的易用的重要性有时候甚至高于稳定性和功能。新的算法，新的语言都会给重新制造轮子带来了机会。 gitchat交流群的问答问：业务使用的etcd v2 升级到 v3 会有什么问题呢，如何平滑过渡？ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;答：v2的大多数功能，用v3都能实现，比如用prefix模拟原来的目录结构，用txn模拟CAS，一般不会有什么问题。但因为v2和v3的数据是互相隔离的，所以迁移起来略麻烦。建议先在业务中封装一层，将etcd v2,v3的差异封装起来，然后通过开关切换。 问：metad的watch是怎么实现的？ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;答：metad的watch实现的比较简单，因为metad的watch返回的不是变更事件，而是最新的结果。所以metad只维护了一个全局版本号，只要发现客户端watch的版本小于等于全局版本号，就直接返回最新结果。 问：etcd和zk都是作为分布式配置管理的组件。均提供了watch功能，选主。作为初使用者，这两个之间的选取该如何？ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;答：etcd和zk二者大多数情况下可以互相替代，都是通用的分布式一致性kv存储。二者之间选择建议选择自己的开发栈比较接近并且团队成员比较熟悉的，比如一种是按语言选择，go语言的项目用etcd，java的用zk，出问题要看源码也容易些。如果是新项目，纠结于二者，那可以分装一层lib，类似于docker/libkv，同时支持两种，有需要可以切换。 问：etcd和eureka、consul 的异同，以及各自的适用场景，以及选型原则。这个问题其实可以把zk也包括进来，这些都有相同之处。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;答：etcd和zk的选型前面讲到了，二者的定位都是通用的一致性kv存储，而eureka和consul的定位则是专做服务注册和发现。前二者的优势当然是通用性，应用广泛，部署运维的时候容易和已有的服务一起共用，而同时缺点也是太通用了，每个应用的服务注册都有自己的一套元数据格式，互相整合起来就比较麻烦了，比如想做个通用的api gateway就会遇到元数据格式兼容问题。这也成为后二者的优势。同时因为后二者的目标比较具体，所以可以做一些更高级的功能，比如consul的DNS支持，consul-template工具，eureka的事件订阅过滤机制。Eureka本身的实现是一个AP系统，也就是说牺牲了一致性，它认为在服务发现和配置中心这个场景下，可用性和分区容错比一致性更重要。 我个人其实更期待后二者的这种专门的解决方案，要是能形成服务注册标准，那以后应用之间互相交互就容易了。但也有个可能是这种标准由集群调度系统来形成事实标准。后二者我了解的也不深入，感觉可以另起一篇文章了。 问：接上面，etcd和zk各自都有哪些坑可能会被踩到，都有多坑。掉进去了如何爬起来？ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这个坑的概念比较太广泛了，更详细的可以翻bug列表。但使用中的大多数坑一般有几种： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;误用导致的坑。要先认识清楚etcd，zk的定位，它需要保存的是整个集群共享的信息，不能当存储用。比如有人在某个zk的某个数据节点下创建了大量的子节点，然后获取，导致zk报错，zk的buffer有个4mb的限制，超过就会报错。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;运维方面的坑。etcd，zk这种服务，一般都比较稳定，搭建好后都不用管，但万一某些节点出问题了，要增加节点恢复系统的时候，可能没有预案或者操作经验，导致弄坏集群。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;网络分区以及可用性设计的坑。设计系统的时候，要想清楚如果etcd或zk整个挂了，或者出现网络分区，应用的一部分节点只能连接到少数派的etcd/zk(少数派不可用)的时候，应用会有什么表现。这种情况下，应用的正确表现应该是服务正常运作，但不支持变更，等etcd/zk集群恢复后就自动恢复了。但如果设计不当，有自动化的一些行为，可能带来的故障就大了。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;想要少踩坑，一个办法就是我文中提到的，研究原理知其然同时知其所以然，另外一个问题就是多试验，出了问题有预案。 问：一个实验性质的硬件集群项目的几个问题 我们实现了基于Arm的分布式互联的硬件集群（方法参考的是https://edcashin.wordpress.com/2013/12/29/trying-etcd-on-android-mac-and-raspberry-pi/comment-page-1/ 将etcd跑在Arm开发板上），将Etcd当作一个分布式的数据库使用（但是Etcd本身运行在这些硬件之上），然后参考go-rpiohttps://github.com/stianeikeland/go-rpio 实现基于etcd的key-value同步硬件的信息，控制某些GPIO。 问题1：目前已知Etcd可以为别的服务提供服务发现，在这个场景下假设已经存在5个运行Etcd节点的硬件，当一个新的Etcd硬件节点被安装时，Etcd能否为自己提供服务发现服务，实现Etcd节点的自动发现与加入？ 问题2：随着硬件安装规模的增加，Etcd的极限是多少，raft是否会因为节点的变多，心跳包的往返而导致同步一次的等待时间变长？ 问题3：当规模足够大，发生网络分区时，是否分区较小的一批硬件之间的数据是无法完成同步的？ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;答：这个案例挺有意思，我一个一个回答。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;etcd本来是做服务发现的，如果etcd集群也需要服务发现，那就再来一个etcd集群 ：）。你可以自己搭建一个etcd cluster或者用etcd官方提供的 discovery.etcd.io。详细参看：etcd 官方的 op-guide/clustering&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;etcd的机制是多节点一致的，所以它的极限有两部分，一是单机的容量限制，内存和磁盘。二是网络开销，每次raft操作需要所有节点参与，节点越多性能越低。所以扩展很多etcd节点是没有意义的，一般是 3，5，7，9。再多感觉就没意义了。如果你们不太在意一致性，建议读请求可以不通过一致性协议，直接读取节点本地数据。具体方式文中有说明。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;etcd网络分区时，少数派是不可用状态，不支持raft请求，但支持非一致性读请求。 问：如果跨机房部署服务，是部署两套ETCD吗？如果跨机房部署，如何部署及配置？ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;答：这个要看跨机房的场景。如果是完全无关联需要公网连接的两个机房，服务之间一般也不需要共享数据吧？部署两套互不相干的etcd，各用各的比较合适。但如果是类似于aws的可用区的概念，两个机房内网互通，搭建两套集群为了避免机房故障，可以随时切换。这个etcd当前没有太好的解决办法，建议的办法是跨可用区部署一个etcd cluster，调整心跳以及选举超时时间，这个办法如果有3个可用区机房，每个机房3个节点，挂任何一个机房都不影响整个集群，但两个机房就比较尴尬。还有个办法是两个集群之间同步，这个etcdv3提供了一个mirror的工具，但还是不太完善，不过感觉用etcd的watch机制做一个同步工具也不难。这个机制consul倒是提供了，多数据中心的集群数据同步，互相不影响可用性。 问：在使用 etcd watch 过程中，有没有一些措施能帮助降低出现惊群（Herd Effect）？ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;答：这个问题我也遇到了，但没发现太好的办法，除了在客户端做随机延迟。（注：这个问题后来和coreos的李响交流，他说etcd3.1会对有解决方案） 相关链接 raft官网 有论文地址以及相关资料。raft动画演示 看了这个动画就懂raft了。Interval_treeetcd：从应用场景到实现原理的全方位解读 这篇文章对使用场景描述的比较全面。confd 我们修改版的confd仓库地址。metad 仓库地址。etcd集群一键搭建脚本并发之痛 Thread，Goroutine，Actor 本人关于并发模型的一篇文章，有利于理解文章内提到的CSP模型。 Never forget to say “Thank you.”.]]></content>
      <categories>
        <category>共识算法</category>
      </categories>
      <tags>
        <tag>共识算法</tag>
        <tag>分布式</tag>
        <tag>ETCD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper一致性共识算法ZAB详解]]></title>
    <url>%2F2019%2F10%2F01%2FZookeeper%E4%B8%80%E8%87%B4%E6%80%A7%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95ZAB%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Zookeeper中一致性共识算法ZAB（Zookeeper Atomic Broadcast protocol）改进了Raft算法，提供一致性的元数据存储，多用在分布式系统中共享元数据信息。下面来看看具体细节。 1.ZAB介绍&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ZAB协议全称就是ZooKeeper Atomic Broadcast protocol，是ZooKeeper用来实现一致性的算法，分成如下4个阶段。 先来解释下部分名词: electionEpoch：每执行一次leader选举，electionEpoch就会自增，用来标记leader选举的轮次 peerEpoch：每次leader选举完成之后，都会选举出一个新的peerEpoch，用来标记事务请求所属的轮次 zxid：事务请求的唯一标记，由leader服务器负责进行分配。由2部分构成，高32位是上述的peerEpoch，低32位是请求的计数，从0开始。所以由zxid我们就可以知道该请求是哪个轮次的，并且是该轮次的第几个请求。 lastProcessedZxid：最后一次commit的事务请求的zxid Leader election: leader选举过程，electionEpoch自增，在选举的时候lastProcessedZxid越大，越有可能成为leader Discovery： 第一：leader收集follower的lastProcessedZxid，这个主要用来通过和leader的lastProcessedZxid对比来确认follower需要同步的数据范围第二：选举出一个新的peerEpoch，主要用于防止旧的leader来进行提交操作（旧leader向follower发送命令的时候，follower发现zxid所在的peerEpoch比现在的小，则直接拒绝，防止出现不一致性） Synchronization： follower中的事务日志和leader保持一致的过程，就是依据follower和leader之间的lastProcessedZxid进行，follower多的话则删除掉多余部分，follower少的话则补充，一旦对应不上则follower删除掉对不上的zxid及其之后的部分然后再从leader同步该部分之后的数据 Broadcast: 正常处理客户端请求的过程。leader针对客户端的事务请求，然后提出一个议案，发给所有的follower，一旦过半的follower回复OK的话，leader就可以将该议案进行提交了，向所有follower发送提交该议案的请求，leader同时返回OK响应给客户端 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上面简单的描述了上述4个过程，这4个过程的详细描述在zab的paper中可以找到，但是我看了之后基本和zab的源码实现上相差有点大，这里就不再提zab paper对上述4个过程的描述了，下面会详细的说明ZooKeeper源码中是具体怎么来实现的 2.ZAB协议源码实现先看下ZooKeeper整体的实现情况，如下图所示 上述实现中Recovery Phase包含了ZAB协议中的Discovery和Synchronization。 2.1 重要的数据介绍加上前面已经介绍的几个名词 long lastProcessedZxid：最后一次commit的事务请求的zxid LinkedList committedLog、long maxCommittedLog、long minCommittedLog：ZooKeeper会保存最近一段时间内执行的事务请求议案，个数限制默认为500个议案。上述committedLog就是用来保存议案的列表，上述maxCommittedLog表示最大议案的zxid,minCommittedLog表示committedLog中最小议案的zxid。 ConcurrentMap&lt;Long, Proposal&gt; outstandingProposalsLeader拥有的属性，每当提出一个议案，都会将该议案存放至outstandingProposals，一旦议案被过半认同了，就要提交该议案，则从outstandingProposals中删除该议案 ConcurrentLinkedQueue toBeAppliedLeader拥有的属性，每当准备提交一个议案，就会将该议案存放至该列表中，一旦议案应用到ZooKeeper的内存树中了，然后就可以将该议案从toBeApplied中删除 对于上述几个参数，整个Broadcast的处理过程可以描述为： leader针对客户端的事务请求（leader为该请求分配了zxid），创建出一个议案，并将zxid和该议案存放至leader的outstandingProposals中 leader开始向所有的follower发送该议案，如果过半的follower回复OK的话，则leader认为可以提交该议案，则将该议案从outstandingProposals中删除，然后存放到toBeApplied中 leader对该议案进行提交，会向所有的follower发送提交该议案的命令，leader自己也开始执行提交过程，会将该请求的内容应用到ZooKeeper的内存树中，然后更新lastProcessedZxid为该请求的zxid，同时将该请求的议案存放到上述committedLog，同时更新maxCommittedLog和minCommittedLog leader就开始向客户端进行回复，然后就会将该议案从toBeApplied中删除 2.2 Fast Leader Electionleader选举过程要关注的要点： 所有机器刚启动时进行leader选举过程 如果leader选举完成，刚启动起来的server怎么识别到leader选举已完成 投票过程有3个重要的数据: ServerState目前ZooKeeper机器所处的状态有4种，分别是 LOOKING：进入leader选举状态 FOLLOWING：leader选举结束，进入follower状态 LEADING：leader选举结束，进入leader状态 OBSERVING：处于观察者状态 HashMap&lt;Long, Vote&gt; recvset用于收集LOOKING、FOLLOWING、LEADING状态下的server的投票 HashMap&lt;Long, Vote&gt; outofelection用于收集FOLLOWING、LEADING状态下的server的投票（能够收集到这种状态下的投票，说明leader选举已经完成） 下面就来详细说明这个过程： 1. serverA首先将electionEpoch自增，然后为自己投票&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;serverA会首先从快照日志和事务日志中加载数据，就可以得到本机器的内存树数据，以及lastProcessedZxid（这一部分后面再详细说明） &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;初始投票Vote的内容： proposedLeader：ZooKeeper Server中的myid值，初始为本机器的id proposedZxid：最大事务zxid，初始为本机器的lastProcessedZxid proposedEpoch:peerEpoch值，由上述的lastProcessedZxid的高32得到 然后该serverA向其他所有server发送通知，通知内容就是上述投票信息和electionEpoch信息 2. serverB接收到上述通知，然后进行投票PK&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果serverB收到的通知中的electionEpoch比自己的大，则serverB更新自己的electionEpoch为serverA的electionEpoch &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果该serverB收到的通知中的electionEpoch比自己的小，则serverB向serverA发送一个通知，将serverB自己的投票以及electionEpoch发送给serverA，serverA收到后就会更新自己的electionEpoch &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在electionEpoch达成一致后，就开始进行投票之间的pk，规则如下： 1234567891011/* * We return true if one of the following three cases hold: * 1- New epoch is higher * 2- New epoch is the same as current epoch, but new zxid is higher * 3- New epoch is the same as current epoch, new zxid is the same * as current zxid, but server id is higher. */return ((newEpoch &gt; curEpoch) || ((newEpoch == curEpoch) &amp;&amp; ((newZxid &gt; curZxid) || ((newZxid == curZxid) &amp;&amp; (newId &gt; curId))))); &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;就是优先比较proposedEpoch，然后优先比较proposedZxid，最后优先比较proposedLeader &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;pk完毕后，如果本机器投票被pk掉，则更新投票信息为对方投票信息，同时重新发送该投票信息给所有的server。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果本机器投票没有被pk掉，则看下面的过半判断过程 3. 根据server的状态来判定leader&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果当前发来的投票的server的状态是LOOKING状态，则只需要判断本机器的投票是否在recvset中过半了，如果过半了则说明leader选举就算成功了，如果当前server的id等于上述过半投票的proposedLeader,则说明自己将成为了leader，否则自己将成为了follower &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果当前发来的投票的server的状态是FOLLOWING、LEADING状态，则说明leader选举过程已经完成了，则发过来的投票就是leader的信息，这里就需要判断发过来的投票是否在recvset或者outofelection中过半了 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;同时还要检查leader是否给自己发送过投票信息，从投票信息中确认该leader是不是LEADING状态。这个解释如下： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;因为目前leader和follower都是各自检测是否进入leader选举过程。leader检测到未过半的server的ping回复，则leader会进入LOOKING状态，但是follower有自己的检测，感知这一事件，还需要一定时间，在此期间，如果其他server加入到该集群，可能会收到其他follower的过半的对之前leader的投票，但是此时该leader已经不处于LEADING状态了，所以需要这么一个检查来排除这种情况。 2.3 Recovery Phase&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;一旦leader选举完成，就开始进入恢复阶段，就是follower要同步leader上的数据信息 1 通信初始化&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;leader会创建一个ServerSocket，接收follower的连接，leader会为每一个连接会用一个LearnerHandler线程来进行服务 2 重新为peerEpoch选举出一个新的peerEpochf&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ollower会向leader发送一个Leader.FOLLOWERINFO信息，包含自己的peerEpoch信息 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;leader的LearnerHandler会获取到上述peerEpoch信息，leader从中选出一个最大的peerEpoch，然后加1作为新的peerEpoch。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;然后leader的所有LearnerHandler会向各自的follower发送一个Leader.LEADERINFO信息，包含上述新的peerEpoch &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;follower会使用上述peerEpoch来更新自己的peerEpoch，同时将自己的lastProcessedZxid发给leader &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;leader的所有LearnerHandler会记录上述各自follower的lastProcessedZxid，然后根据这个lastProcessedZxid和leader的lastProcessedZxid之间的差异进行同步 3 已经处理的事务议案的同步&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;判断LearnerHandler中的lastProcessedZxid是否在minCommittedLog和maxCommittedLog之间 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LearnerHandler中的lastProcessedZxid和leader的lastProcessedZxid一致，则说明已经保持同步了 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果lastProcessedZxid在minCommittedLog和maxCommittedLog之间 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从lastProcessedZxid开始到maxCommittedLog结束的这部分议案，重新发送给该LearnerHandler对应的follower，同时发送对应议案的commit命令 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上述可能存在一个问题：即lastProcessedZxid虽然在他们之间，但是并没有找到lastProcessedZxid对应的议案，即这个zxid是leader所没有的，此时的策略就是完全按照leader来同步，删除该follower这一部分的事务日志，然后重新发送这一部分的议案，并提交这些议案 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果lastProcessedZxid大于maxCommittedLog &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;则删除该follower大于部分的事务日志 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果lastProcessedZxid小于minCommittedLog &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;则直接采用快照的方式来恢复 4 未处理的事务议案的同步&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LearnerHandler还会从leader的toBeApplied数据中将大于该LearnerHandler中的lastProcessedZxid的议案进行发送和提交（toBeApplied是已经被确认为提交的） &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LearnerHandler还会从leader的outstandingProposals中大于该LearnerHandler中的lastProcessedZxid的议案进行发送，但是不提交（outstandingProposals是还没被被确认为提交的） 5 将LearnerHandler加入到正式follower列表中&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;意味着该LearnerHandler正式接受请求。即此时leader可能正在处理客户端请求，leader针对该请求发出一个议案，然后对该正式follower列表才会进行执行发送工作。这里有一个地方就是： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上述我们在比较lastProcessedZxid和minCommittedLog和maxCommittedLog差异的时候，必须要获取leader内存数据的读锁，即在此期间不能执行修改操作，当欠缺的数据包已经补上之后（先放置在一个队列中，异步发送），才能加入到正式的follower列表，否则就会出现顺序错乱的问题 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;同时也说明了，一旦一个follower在和leader进行同步的过程（这个同步过程仅仅是确认要发送的议案，先放置到队列中即可等待异步发送，并不是说必须要发送过去），该leader是暂时阻塞一切写操作的。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于快照方式的同步，则是直接同步写入的，写入期间对数据的改动会放在上述队列中的，然后当同步写入完成之后，再启动对该队列的异步写入。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上述的要理解的关键点就是：既要不能漏掉，又要保证顺序 6 LearnerHandler发送Leader.NEWLEADER以及Leader.UPTODATE命令&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;该命令是在同步结束之后发的，follower收到该命令之后会执行一次版本快照等初始化操作，如果收到该命令的ACK则说明follower都已经完成同步了并完成了初始化 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;leader开始进入心跳检测过程，不断向follower发送心跳命令，不断检是否有过半机器进行了心跳回复，如果没有过半，则执行关闭操作，开始进入leader选举状态 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LearnerHandler向对应的follower发送Leader.UPTODATE，follower接收到之后，开始和leader进入Broadcast处理过程 2.4 Broadcast Phase前面其实已经说过了，参见2.1中的内容 3 特殊情况的注意点3.1 事务日志和快照日志的持久化和恢复先来看看持久化过程： Broadcast过程的持久化leader针对每次事务请求都会生成一个议案，然后向所有的follower发送该议案follower接收到该议案后，所做的操作就是将该议案记录到事务日志中，每当记满100000个（默认），则事务日志执行flush操作，同时开启一个新的文件来记录事务日志同时会执行内存树的快照，snapshot.[lastProcessedZxid]作为文件名创建一个新文件，快照内容保存到该文件中 leader shutdown过程的持久化一旦leader过半的心跳检测失败，则执行shutdown方法，在该shutdown中会对事务日志进行flush操作 再来说说恢复： 事务快照的恢复第一：会在事务快照文件目录下找到最近的100个快照文件，并排序，最新的在前第二：对上述快照文件依次进行恢复和验证，一旦验证成功则退出，否则利用下一个快照文件进行恢复。恢复完成更新最新的lastProcessedZxid 事务日志的恢复第一：从事务日志文件目录下找到zxid大于等于上述lastProcessedZxid的事务日志第二：然后对上述事务日志进行遍历，应用到ZooKeeper的内存树中，同时更新lastProcessedZxid第三：同时将上述事务日志存储到committedLog中，并更新maxCommittedLog、minCommittedLog 由此我们可以看到，在初始化恢复的时候，是会将所有最新的事务日志作为已经commit的事务来处理的 也就是说这里面可能会有部分事务日志还没真实提交，而这里全部当做已提交来处理。这个处理简单粗暴了一些，而raft对老数据的恢复则控制的更加严谨一些。 3.2 follower挂了之后又重启的恢复过程&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;一旦leader挂了，上述leader的2个集合 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ConcurrentMap&lt;Long, Proposal&gt; outstandingProposals&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ConcurrentLinkedQueue toBeApplied&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;就无效了。他们并不在leader恢复的时候起作用，而是在系统正常执行，而某个follower挂了又恢复的时候起作用。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们可以看到在上述2.3的恢复过程中，会首先进行快照日志和事务日志的恢复，然后再补充leader的上述2个数据中的内容。 3.3 同步follower失败的情况&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;目前leader和follower之间的同步是通过BIO方式来进行的，一旦该链路出现异常则会关闭该链路，重新与leader建立连接，重新同步最新的数据 3.4 对client端是否一致&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;客户端收到OK回复，会不会丢失数据？&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;客户端没有收到OK回复，会不会多存储数据？&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;客户端如果收到OK回复，说明已经过半复制了，则在leader选举中肯定会包含该请求对应的事务日志，则不会丢失该数据 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;客户端连接的leader或者follower挂了，客户端没有收到OK回复，目前是可能丢失也可能没丢失，因为服务器端的处理也很简单粗暴，对于未来leader上的事务日志都会当做提交来处理的，即都会被应用到内存树中。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;同时目前ZooKeeper的原生客户端也没有进行重试，服务器端也没有对重试进行检查。这一部分到下一篇再详细探讨与raft的区别 4 未完待续本文有很多细节，难免可能疏漏，还请指正。 4.1 问题&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里留个问题供大家思考下： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;raft每次执行AppendEntries RPC的时候，都会带上当前leader的新term，来防止旧的leader的旧term来执行相关操作，而ZooKeeper的peerEpoch呢？达到防止旧leader的效果了吗？它的作用是干什么呢？ Suffering is the best teacher of life.]]></content>
      <categories>
        <category>共识算法</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
        <tag>共识算法</tag>
        <tag>分布式</tag>
        <tag>ZAB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据仓库基本结构]]></title>
    <url>%2F2019%2F09%2F29%2F%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据仓库是一个面向主题的、集成的、稳定的、反映历史变化的数据集合，用于支持决策管理，下面来看看它的组成部分。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里有讲解数据仓库各个特点的具体详情，数据仓库特点详解 根据仓库特点我们可以大致猜到，他的结构大概是什么样子的： 面向主题的、集成的&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;集成的数据仓库，数据是从不同的业务主题数据源处经过清洗、整合、统一规范入库，那么仓库中就会有数据准备区(data staging层)缓存业务数据，然后清洗，得到的一致性数据ODS层，另外需要提供数据查询工具、数据可视化工具、数据建模工具、OLAP分析工具进行数据展示和分析，来更好地提供决策支持。 稳定的、反映历史变化的&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;仓库中的数据是定期按时从源数据端拉取过来，不会经常做更新操作，拉取更新操作需要用到抽取工具、任务调度工具，而且可以查询到历史数据状态，那么就需要管理数据的元数据管理工具，了解仓库中到底有哪些数据，这些数据目前的状况是怎样的。 一、概念结构 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从数据仓库的概念结构看，一般来说，数据仓库系统要包含数据源、数据准备区、数据仓库数据库、数据集市/知识挖掘库及各种管理工具和应用工具，如图 3-10 所示。数据仓库建立之后，首先要从数据源中抽取相关的数据到数据准备区，在数据准备区中经过净化处理后再加载到数据仓库数据库，最后根据用户的需求将数据导入数据集市和知识挖掘库中。当用户使用数据仓库时，可以利用包括 OLAP（On-Line Analysis Processing，联机分析处理）在内的多种数据仓库应用工具向数据集市/知识挖掘库或数据仓库进行决策查询分析或知识挖掘。数据仓库的创建、应用可以利用各种数据仓库管理工具辅助完成。 二、层次框架&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据仓库框架由数据仓库基本功能层、数据仓库管理层和数据仓库环境支持层组成。 数据仓库基本功能层: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据仓库的基本功能层部分包含数据源、数据准备区、数据仓库结构、数据集市或知识挖掘库，以及存取和使用部分。 数据仓库管理层: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据仓库管理层由数据仓库的数据管理和数据仓库的元数据管理组成。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据仓库的数据管理层包含数据抽取、新数据需求与查询管理，数据加载、存储、刷新和更新系统，安全性与用户授权管理系统及数据归档、恢复及净化系统等四部分。 数据仓库的环境支持层: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据仓库的环境支持层由数据仓库数据传输层和数据仓库基础层组成。 三、架构图 you must do what you like.]]></content>
      <categories>
        <category>数据仓库</category>
      </categories>
      <tags>
        <tag>OLAP</tag>
        <tag>数据仓库</tag>
        <tag>ODS</tag>
        <tag>Data Market</tag>
        <tag>Data Source</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一致性共识算法Raft图文详解]]></title>
    <url>%2F2019%2F09%2F29%2F%E4%B8%80%E8%87%B4%E6%80%A7%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95Raft%E5%9B%BE%E6%96%87%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;熟悉或了解分布式系统的开发者都知道一致性算法的重要性，Paxos一致性算法提出至今已经有二十几年了，而Paxos流程太过于繁杂，实现起来比较复杂，因此Raft应运而生，它是比Paxos更简单而又能实现Paxos所解决的问题的一致性算法。 Raft动图展示详解 Death is like the wind, always by my side.]]></content>
      <categories>
        <category>共识算法</category>
      </categories>
      <tags>
        <tag>一致性协议</tag>
        <tag>raft</tag>
        <tag>共识算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[事实表、维度、度量、指标之间的关系]]></title>
    <url>%2F2019%2F09%2F29%2F%E4%BA%8B%E5%AE%9E%E8%A1%A8%E3%80%81%E7%BB%B4%E5%BA%A6%E3%80%81%E5%BA%A6%E9%87%8F%E3%80%81%E6%8C%87%E6%A0%87%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB%2F</url>
    <content type="text"><![CDATA[数据仓库中有许多概念，比如事实表、维度、度量、指标，下面来看看他们是什么意思，这几个概念之间有什么关系？ 事实表：每个数据仓库都包含一个或多个事实数据表。事实数据表可能包含业务销售数据，如销售商品所产生的数据，与软件中实际表概念一样。 维度：说明数据，维度是指可指定不同值的对象的描述性属性或特征。例如，地理位置的维度可以描述为 “经度”、“纬度”或“城市名称”，“城市名称”维度的值可以为“旧金山”、“柏林”或“新加坡”。 指标：衡量数据，指标是指可以按照总数或者比值进行衡量的具体维度元素，例如，“城市”维度可以关联“人口”指标，其值为具体城市的总人口数。 维度和指标的关系：虽然维度和指标可以独立使用，但是一般会结合使用。维度和指标的值以及这些值之间的关系，使你的数据具有的意义。为了挖掘尽可能多的深层次信息，维度通常和一个或多个指标关联在一起。例如，维度“城市”可以与指标“人口”、“面积”相关联，有了这些数据，系统还可以创建“人口密度”等比值指标，带来有关这些城市更详细的深入信息。 度量：事实表和维度交叉汇聚的点，度量和维度构成OLAP的主要概念。这里面对于在事实表或一个多维立方体里面存放的数值型的、连续的字段，就是度量。这符合上面的意思，有标准，一个度量字段肯定是统一单位，例如元、人数。如果一个度量字段，其中的度量值可能是美元，可能是欧元，那这个度量可没法汇总。在统一计量单位下，对不同维度的描述。 指标与度量的关系：这就得说到指标，我愿意表述为“它是表示某种相对程度的值”。区别于上面的度量的概念，那是一种绝对值，尺子量出来的结果，汇总出来的数量等。而指标至少需要2个度量之间的计算才能得到，例如收入增长率，用本月收入比上上月收入。当然指标的计算可能需要两个以上的度量。 Learn to make friends with yourself: do something funny together, learn something together, and catch up with each other.]]></content>
      <categories>
        <category>数据仓库</category>
      </categories>
      <tags>
        <tag>事实表</tag>
        <tag>维度</tag>
        <tag>度量</tag>
        <tag>指标</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式一致性算法Raft]]></title>
    <url>%2F2019%2F09%2F28%2F%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E7%AE%97%E6%B3%95Raft%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;熟悉或了解分布式系统的开发者都知道一致性算法的重要性，Paxos一致性算法提出至今已经有二十几年了，而Paxos流程太过于繁杂，实现起来比较复杂，因此Raft应运而生，它是比Paxos更简单而又能实现Paxos所解决的问题的一致性算法。 Raft动图展示详解 一、背景&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;熟悉或了解分布性系统的开发者都知道一致性算法的重要性，Paxos一致性算法从90年提出到现在已经有二十几年了，而Paxos流程太过于繁杂实现起来也比较复杂，可能也是以为过于复杂 现在我听说过比较出名使用到Paxos的也就只是Chubby、libpaxos，搜了下发现Keyspace、BerkeleyDB数据库中也使用了该算法作为数据的一致性同步，虽然现在很广泛使用的Zookeeper也是基于Paxos算法来实现，但是Zookeeper使用的ZAB（Zookeeper Atomic Broadcast）协议对Paxos进行了很多的改进与优化，算法复杂我想会是制约他发展的一个重要原因；说了这么多只是为了要引出本篇文章的主角Raft一致性算法，没错Raft就是在这个背景下诞生的，文章开头也说到了Paxos最大的问题就是复杂，Raft一致性算法就是比Paxos简单又能实现Paxos所解决的问题的一致性算法。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Raft是斯坦福的Diego Ongaro、John Ousterhout两个人以易懂（Understandability）为目标设计的一致性算法，在2013年发布了论文：《In Search of an Understandable Consensus Algorithm》从2013年发布到现在不过只有两年，到现在已经有了十多种语言的Raft算法实现框架，较为出名的有etcd，Google的Kubernetes也是用了etcd作为他的服务发现框架；由此可见易懂性是多么的重要。 二、Raft概述&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;与Paxos不同Raft强调的是易懂（Understandability），Raft和Paxos一样只要保证n/2+1节点正常就能够提供服务；众所周知但问题较为复杂时可以把问题分解为几个小问题来处理，Raft也使用了分而治之的思想把算法流程分为三个子问题：选举（Leader election）、日志复制（Log replication）、安全性（Safety）三个子问题；这里先简单介绍下Raft的流程;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Raft开始时在集群中选举出Leader负责日志复制的管理，Leader接受来自客户端的事务请求（日志），并将它们复制给集群的其他节点，然后负责通知集群中其他节点提交日志，Leader负责保证其他节点与他的日志同步，当Leader宕掉后集群其他节点会发起选举选出新的Leader； 三、Raft详解1、角色&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Raft把集群中的节点分为三种状态：Leader、 Follower 、Candidate，理所当然每种状态负责的任务也是不一样的，Raft运行时提供服务的时候只存在Leader与Follower两种状态； Leader（领导者）：负责日志的同步管理，处理来自客户端的请求，与Follower保持这HeartBeat的联系；Follower（追随者）：刚启动时所有节点为Follower状态，响应Leader的日志同步请求，响应Candidate的请求，把请求到Follower的事务转发给Leader；Candidate（候选者）：负责选举投票，Raft刚启动时由一个节点从Follower转为Candidate发起选举，选举出Leader后从Candidate转为Leader状态； 2、Term&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在Raft中使用了一个可以理解为周期（第几届、任期）的概念，用Term作为一个周期，每个Term都是一个连续递增的编号，每一轮选举都是一个Term周期，在一个Term中只能产生一个Leader；先简单描述下Term的变化流程： Raft开始时所有Follower的Term为1，其中一个Follower逻辑时钟到期后转换为Candidate，Term加1这是Term为2（任期），然后开始选举，这时候有几种情况会使Term发生改变： 如果当前Term为2的任期内没有选举出Leader或出现异常，则Term递增，开始新一任期选举 当这轮Term为2的周期选举出Leader后，过后Leader宕掉了，然后其他Follower转为Candidate，Term递增，开始新一任期选举 当Leader或Candidate发现自己的Term比别的Follower小时Leader或Candidate将转为Follower，Term递增 当Follower的Term比别的Term小时Follower也将更新Term保持与其他Follower一致； &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;可以说每次Term的递增都将发生新一轮的选举，Raft保证一个Term只有一个Leader，在Raft正常运转中所有的节点的Term都是一致的，如果节点不发生故障一个Term（任期）会一直保持下去，当某节点收到的请求中Term比当前Term小时则拒绝该请求； 3、选举（Election）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Raft的选举由定时器来触发，每个节点的选举定时器时间都是不一样的，开始时状态都为Follower某个节点定时器触发选举后Term递增，状态由Follower转为Candidate，向其他节点发起RequestVote RPC请求，这时候有三种可能的情况发生： 该RequestVote请求接收到n/2+1（过半数）个节点的投票，从Candidate转为Leader，向其他节点发送heartBeat以保持Leader的正常运转 在此期间如果收到其他节点发送过来的AppendEntries RPC请求，如该节点的Term大则当前节点转为Follower，否则保持Candidate拒绝该请求 Election timeout发生则Term递增，重新发起选举 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在一个Term期间每个节点只能投票一次，所以当有多个Candidate存在时就会出现每个Candidate发起的选举都存在接收到的投票数都不过半的问题，这时每个Candidate都将Term递增、重启定时器并重新发起选举，由于每个节点中定时器的时间都是随机的，所以就不会多次存在有多个Candidate同时发起投票的问题。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;有这么几种情况会发起选举，1：Raft初次启动，不存在Leader，发起选举；2：Leader宕机或Follower没有接收到Leader的heartBeat，发生election timeout从而发起选举; 4、日志复制（Log Replication）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;日志复制（Log Replication）主要作用是用于保证节点的一致性，这阶段所做的操作也是为了保证一致性与高可用性；当Leader选举出来后便开始负责客户端的请求，所有事务（更新操作）请求都必须先经过Leader处理，这些事务请求或说成命令也就是这里说的日志，我们都知道要保证节点的一致性就要保证每个节点都按顺序执行相同的操作序列，日志复制（Log Replication）就是为了保证执行相同的操作序列所做的工作；在Raft中当接收到客户端的日志（事务请求）后先把该日志追加到本地的Log中，然后通过heartbeat把该Entry同步给其他Follower，Follower接收到日志后记录日志然后向Leader发送ACK，当Leader收到大多数（n/2+1）Follower的ACK信息后将该日志设置为已提交并追加到本地磁盘中，通知客户端并在下个heartbeat中Leader将通知所有的Follower将该日志存储在自己的本地磁盘中。 5、安全性（Safety）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;安全性是用于保证每个节点都执行相同序列的安全机制，如当某个Follower在当前Leader commit Log时变得不可用了，稍后可能该Follower又会倍选举为Leader，这时新Leader可能会用新的Log覆盖先前已committed的Log，这就是导致节点执行不同序列；Safety就是用于保证选举出来的Leader一定包含先前 commited Log的机制： 选举安全性（Election Safety） 每个Term只能选举出一个Leader Leader完整性（Leader Completeness） &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里所说的完整性是指Leader日志的完整性，当Log在Term1被Commit后，那么以后Term2、Term3…等的Leader必须包含该Log；Raft在选举阶段就使用Term的判断用于保证完整性：当请求投票的该Candidate的Term较大或Term相同Index更大则投票，否则拒绝该请求。 keep exercising, keep learning english, keep learning blockchain.]]></content>
      <categories>
        <category>共识算法</category>
      </categories>
      <tags>
        <tag>一致性协议</tag>
        <tag>raft</tag>
        <tag>共识算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式一致性协议Paxos]]></title>
    <url>%2F2019%2F09%2F27%2F%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AEPaxos%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Basic-Paxos算法(可以先看后面的实际例子再看前面的具体介绍部分）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;多个节点并发操纵数据，如何保证在读写过程中数据的一致性，并且解决方案要能适应分布式环境下的不可靠性，由此，Paxos共识算法应运而生。 一、Paxos算法的目的&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Paxos算法的目的是为了解决分布式环境下一致性的问题。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;多个节点并发操纵数据，如何保证在读写过程中数据的一致性，并且解决方案要能适应分布式环境下的不可靠性（系统如何就一个值达到统一） 二、Paxos的两个组件1.Proposer &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;提议发起者，处理客户端请求，将客户端的请求发送到集群中，以便决定这个值是否可以被批准。 2.Acceptor &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;提议批准者，负责处理接收到的提议，他们的回复就是一次投票。会存储一些状态来决定是否接收一个值 三、Paxos有两个原则1).安全原则—保证不能做错的事 a） 针对某个实例的表决只能有一个值被批准，不能出现一个被批准的值被另一个值覆盖的情况；(假设有一个值被多数Acceptor批准了，那么这个值就只能被学习)b） 每个节点只能学习到已经被批准的值，不能学习没有被批准的值。 2).存活原则—只要有多数服务器存活并且彼此间可以通信，最终都要做到的下列事情： a）最终会批准某个被提议的值；b）一个值被批准了，其他服务器最终会学习到这个值。 四、Paxos具体流程图 1.第一阶段（prepare）1).获取一个proposal number, n； 2).提议者向所有节点广播prepare(n)请求； 3).接收者（Acceptors比较善变，如果还没最终认可一个值，它就会不断认同提案号最大的那个方案）比较n和minProposal，如果n&gt;minProposal,表示有更新的提议minProposal=n；如果此时该接受者并没有认可一个最终值，那么认可这个提案，返回OK。如果此时已经有一个accptedValue, 将返回(acceptedProposal,acceptedValue)； 4).提议者接收到过半数请求后，如果发现有acceptedValue返回，表示有认可的提议，保存最高acceptedProposal编号的acceptedValue到本地 2.第二阶段(Accept)5）广播accept(n,value)到所有节点； 6).接收者比较n和minProposal，如果n&gt;=minProposal,则acceptedProposal=minProposal=n，acceptedValue=value，本地持久化后，返回；否则，拒绝并且返回minProposal 7).提议者接收到过半数请求后，如果发现有返回值&gt;n，表示有更新的提议，跳转1（重新发起提议）；否则value达成一致。 三、Paxos议案ID生成算法&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在Google的Chubby论文中给出了这样一种方法：假设有n个proposer，每个编号为ir(0&lt;=ir&lt;n)，proposal编号的任何值s都应该大于它已知的最大值，并且满足： 1s %n = ir =&gt; s = m*n + ir &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;proposer已知的最大值来自两部分：proposer自己对编号自增后的值和接收到acceptor的拒绝后所得到的值。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;例：以3个proposer P1、P2、P3为例，开始m=0,编号分别为0，1，2。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1） P1提交的时候发现了P2已经提交，P2编号为1 &gt;P1的0，因此P1重新计算编号：new P1 = 1*3+1 = 4； &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2） P3以编号2提交，发现小于P1的4，因此P3重新编号：new P3 = 1*3+2 = 5。 四、Paxos原理&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;任意两个法定集合，必定存在一个公共的成员。该性质是Paxos有效的基本保障 五、活锁&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当某一proposer提交的proposal被拒绝时，可能是因为acceptor 承诺返回了更大编号的proposal，因此proposer提高编号继续提交。 如果2个proposer都发现自己的编号过低转而提出更高编号的proposal，会导致死循环，这种情况也称为活锁。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;比如说当此时的 proposer1提案是3, proposer2提案是4, 但acceptor承诺的编号是5，那么此时proposer1,proposer2 都将提高编号假设分别为6,7，并试图与accceptor连接，假设7被接受了，那么提案5和提案6就要重新编号提交，从而不断死循环。 六、异常情况——持久存储&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在算法执行的过程中会产生很多的异常情况：proposer宕机，acceptor在接收proposal后宕机，proposer接收消息后宕机，acceptor在accept后宕机，learn宕机，存储失败，等等。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为保证paxos算法的正确性，proposer、aceptor、learn都实现持久存储，以做到server恢复后仍能正确参与paxos处理。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;propose存储已提交的最大proposal编号、决议编号（instance id）。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;acceptor存储已承诺（promise）的最大编号、已接受（accept）的最大编号和value、决议编号。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;learn存储已学习过的决议和编号。 七、具体实例：1.假设的3军问题1） 1支红军在山谷里扎营，在周围的山坡上驻扎着3支蓝军； 2） 红军比任意1支蓝军都要强大；如果1支蓝军单独作战，红军胜；如果2支或以上蓝军同时进攻，蓝军胜； 3） 三支蓝军需要同步他们的进攻时间；但他们惟一的通信媒介是派通信兵步行进入山谷，在那里他们可能被俘虏，从而将信息丢失；或者为了避免被俘虏，可能在山谷停留很长时间； 4） 每支军队有1个参谋负责提议进攻时间；每支军队也有1个将军批准参谋提出的进攻时间；很明显，1个参谋提出的进攻时间需要获得至少2个将军的批准才有意义； 5） 问题：是否存在一个协议，能够使得蓝军同步他们的进攻时间？ 2.接下来以两个假设的场景来演绎BasicPaxos；参谋和将军需要遵循一些基本的规则1） 参谋以两阶段提交（prepare/commit）的方式来发起提议，在prepare阶段需要给出一个编号； 2） 在prepare阶段产生冲突，将军以编号大小来裁决，编号大的参谋胜出； 3） 参谋在prepare阶段如果收到了将军返回的已接受进攻时间，在commit阶段必须使用这个返回的进攻时间； 2.1 两个参谋先后提议的场景 1） 参谋1发起提议，派通信兵带信给3个将军，内容为（编号1）； 2） 3个将军收到参谋1的提议，由于之前还没有保存任何编号，因此把（编号1）保存下来，避免遗忘；同时让通信兵带信回去，内容为（ok）； 3） 参谋1收到至少2个将军的回复，再次派通信兵带信给3个将军，内容为（编号1，进攻时间1）； 4） 3个将军收到参谋1的时间，把（编号1，进攻时间1）保存下来，避免遗忘；同时让通信兵带信回去，内容为（Accepted）； 5） 参谋1收到至少2个将军的（Accepted）内容，确认进攻时间已经被大家接收； 6） 参谋2发起提议，派通信兵带信给3个将军，内容为（编号2）； 7） 3个将军收到参谋2的提议，由于（编号2）比（编号1）大，因此把（编号2）保存下来，避免遗忘；又由于之前已经接受参谋1的提议，因此让通信兵带信回去，内容为（编号1，进攻时间1）； 8） 参谋2收到至少2个将军的回复，由于回复中带来了已接受的参谋1的提议内容，参谋2因此不再提出新的进攻时间，接受参谋1提出的时间； 2.2 两个参谋交叉提议的场景 1） 参谋1发起提议，派通信兵带信给3个将军，内容为（编号1）； 2） 3个将军的情况如下 a) 将军1和将军2收到参谋1的提议，将军1和将军2把（编号1）记录下来，如果有其他参谋提出更小的编号，将被拒绝；同时让通信兵带信回去，内容为（ok）；b) 负责通知将军3的通信兵被抓，因此将军3没收到参谋1的提议； 3） 参谋2在同一时间也发起了提议，派通信兵带信给3个将军，内容为（编号2）； 4） 3个将军的情况如下 a) 将军2和将军3收到参谋2的提议，将军2和将军3把（编号2）记录下来，如果有其他参谋提出更小的编号，将被拒绝；同时让通信兵带信回去，内容为（ok）；b) 负责通知将军1的通信兵被抓，因此将军1没收到参谋2的提议； 5） 参谋1收到至少2个将军的回复，再次派通信兵带信给有答复的2个将军，内容为（编号1，进攻时间1）； 6） 2个将军的情况如下 a) 将军1收到了（编号1，进攻时间1），和自己保存的编号相同，因此把（编号1，进攻时间1）保存下来；同时让通信兵带信回去，内容为（Accepted）；b) 将军2收到了（编号1，进攻时间1），由于（编号1）小于已经保存的（编号2），因此让通信兵带信回去，内容为（Rejected，编号2）； 7） 参谋2收到至少2个将军的回复，再次派通信兵带信给有答复的2个将军，内容为（编号2，进攻时间2）； 8） 将军2和将军3收到了（编号2，进攻时间2），和自己保存的编号相同，因此把（编号2，进攻时间2）保存下来，同时让通信兵带信回去，内容为（Accepted）； 9） 参谋2收到至少2个将军的（Accepted）内容，确认进攻时间已经被多数派接受； 10） 参谋1只收到了1个将军的（Accepted）内容，同时收到一个（Rejected，编号2）；参谋1重新发起提议，派通信兵带信给3个将军，内容为（编号3）； 11） 3个将军的情况如下 a) 将军1收到参谋1的提议，由于（编号3）大于之前保存的（编号1），因此把（编号3）保存下来；由于将军1已经接受参谋1前一次的提议，因此让通信兵带信回去，内容为（编号1，进攻时间1）；b) 将军2收到参谋1的提议，由于（编号3）大于之前保存的（编号2），因此把（编号3）保存下来；由于将军2已经接受参谋2的提议，因此让通信兵带信回去，内容为（编号2，进攻时间2）；c) 负责通知将军3的通信兵被抓，因此将军3没收到参谋1的提议； 12） 参谋1收到了至少2个将军的回复，比较两个回复的编号大小，选择大编号对应的进攻时间作为最新的提议；参谋1再次派通信兵带信给有答复的2个将军，内容为（编号3，进攻时间2）； 13） 将军1和将军2收到了（编号3，进攻时间2），和自己保存的编号相同，因此保存（编号3，进攻时间2），同时让通信兵带信回去，内容为（Accepted）； 14） 参谋1收到了至少2个将军的（accepted）内容，确认进攻时间已经被多数派接受； Interest is the best guider.]]></content>
      <categories>
        <category>共识算法</category>
      </categories>
      <tags>
        <tag>一致性协议</tag>
        <tag>共识算法</tag>
        <tag>paxos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据仓库维度建模中星型模型与❄雪花❄️模型的选择]]></title>
    <url>%2F2019%2F09%2F26%2F%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E7%BB%B4%E5%BA%A6%E5%BB%BA%E6%A8%A1%E4%B8%AD%E6%98%9F%E5%9E%8B%E6%A8%A1%E5%9E%8B%E4%B8%8E%E2%9D%84%E9%9B%AA%E8%8A%B1%E2%9D%84%EF%B8%8F%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%80%89%E6%8B%A9%2F</url>
    <content type="text"><![CDATA[维度建模中包含2种设计方式： 星型模式； 雪花模式；下面从多个角度来比较一下这2中模式的利弊吧。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从查询性能角度来看，在OLTP-DW环节，由于雪花型要做多个表联接，性能会低于星型架构；但从DW-OLAP环节，由于雪花型架构更有利于度量值的聚合，因此性能要高于星型架构。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从模型复杂度来看，星型架构更简单。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从层次概念来看，雪花型架构更加贴近OLTP系统的结构，比较符合业务逻辑，层次比较清晰。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从存储空间角度来看，雪花型架构具有关系数据模型的所有优点，不会产生冗余数据，而相比之下星型架构会产生数据冗余。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;根据我们的项目经验，一般建议使用星型架构。因为我们在实际项目中，往往最关注的是查询性能问题，至于磁盘空间一般都不是问题。 当然，在维度表数据量极大，需要节省存储空间的情况下，或者是业务逻辑比较复杂、必须要体现清晰的层次概念情况下，可以使用雪花型维度。 一、概念：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们先了解下星型模式和雪花模式的概念： 星型模式：一种使用关系数据库实现多维分析空间的模式，称为星型模式。星型模式的基本形式必须实现多维空间（常常被称为方块），以使用关系数据库的基本功能。雪花模式：不管什么原因，当星型模式的维度需要进行规范化时，星型模式就演进为雪花模式。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;那么我们怎么样来理解 多维分析空间 呢 ？ 几何学中的方块是指一个三维空间，其中每个维度的尺寸都相同。想象一个立方体，每个维度都有三个单元，我们即得到相同结构的3^3＝27个单元。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;多维分析空间（或者数据仓库方块）与几何空间中的方块仅仅存在细节上的差异。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;维度不限于 3 维。不过，处理很多维度的立方体也不是件轻松的事情，这会导致大多数的实现被限制于 6 或者 7 维。不要期盼使用图形可以很好地表示超过 4 的维度。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;维度并不具有相同的规模和单元。规模从几个单元到几百万个单元，差别巨大。单元可以是一天、一位顾客、部门等。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据立方体需要很大的内存以存储所有事实。无论是否包含事实，都必须要预留单元。这就是为什么使用关系数据库和星型模式的原因。使用它们能够优化存储并且保持数据结构的灵活性。 二、星型模式&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;星型模式的基本思想就是保持立方体的多维功能，同时也增加了小规模数据存储的灵活性。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在图中，星型模式使用事实 Flight 表示了一个 4 维方块（Passenger、Menu、Flight Schedulet 和 Time）。基本上，事实必须指定一个维度，以将其放入立方体的单元中。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;每个维度根据一个对象进行描述，对象可以用类表示，这些类就是有关业务主题的名称。这一点对于成功建立数据仓库来说是很重要的，因为仓库的用户（经理、分析员、市场）对于信息技术的术语并不是很熟悉。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;事实本身就是商业智能的另一个对象，仍然通过类进行表示。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;事实指每个维度。事实与维度的关联常常是一对任意，这也就意味着每个事实都与单个维度的一个单元准确对应，而维度的每个单元（每个Passenger、Time等）可以与任意数量的事实发生关联（包括0个事实）。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在星型模式中切片和切块是对维度的限制（选择）。这是一个运行时问题，而不是建模问题，但是模型必须分辨其需要。 三、雪花模式&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;基本的星型模式并不能满足数据挖掘的所有需要。我们需要更复杂的维度，例如时间。分析员希望根据周、月、季度等识别模式。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;维度必须进行规范化。我们不需要冗余的维度表，这只会使数据切片变得更加复杂。这种过程中我们得到的模式被称为雪花模式。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们来看一个简单的雪花模式例子。我们将时间维度规范化为周、月和季度。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们希望能够使用附加的规范化维度将立方体切片：周、月和季度。在本例中，我们假定季度是月的平行层次，这也就意味着我们不能将季度假定为若干月的聚合。由于这个原因，我们将使用一张范化表（是对 OLAP 查询的一项简单附加）预先选择时间维度。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;最终雪花模式添加了规范化维度。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当然，所有的维度都可以像时间例子那样进行规范化，这就导致了比较复杂的数据集市模式的出现。 吾之初心，永世不忘。]]></content>
      <categories>
        <category>数据仓库</category>
      </categories>
      <tags>
        <tag>数据仓库</tag>
        <tag>维度建模</tag>
        <tag>星型模型</tag>
        <tag>雪花模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据仓库工程师一般面试题]]></title>
    <url>%2F2019%2F09%2F25%2F%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%B7%A5%E7%A8%8B%E5%B8%88%E4%B8%80%E8%88%AC%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于高级数据仓库工程师的问题更侧重于各种工具的细枝末节。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于数据仓库架构师的问题更侧重于数据仓库的架构和总体认识。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于数据仓库项目经理的问题除了以上的问题外，就是一般项目管理的技能了吧。 一、什么叫数据仓库？&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据仓库是一个面向主题的、集成的、相对稳定的、反映历史变化的数据集合，用于支持管理决策。 面向主题：数据仓库是为了提供决策服务的，会建立不同的主题，而主题是进行决策时重点关注的部分； 集成：数据仓库可以整合来自不同数据源的数据，将这个数据入库、清洗、整合成统一的标准化数据，同时上文中提到的一个主题往往与多个系统相关，集成的数据很好地满足了主题构建的数据需求。数据仓库对原有的分散的数据库、文件进行数据抽取、清理的基础上经过系统加工、整理得到，清除原数据中的不一致性（面向事务的数据库往往单独存放单个系统的数据，且不同数据库相互独立，且是异构的）； 相对稳定：数据仓库中的数据是面向决策的，这就表明了仓库中的数据进入之后一般会长期保留，主要面对的是查询，更新和删除操作很少，一般是定期地加载、更新； 反映历史变化：仓库中会保留各个时间节点的数据，以满足不断变化的业务需求。 二、数据仓库与数据库的区别？&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据库：按照数据结构来组织、存储、管理数据，建立在计算机存储设备上面的仓库，一般适用于操作系统，因为符合范式的设计模式，所以数据的一致性较好。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据仓库：面向主题的、集成的、稳定的（不是时时刻刻变化）、反映历史变化的数据集合，可以包含多个数据库。 三、什么是OLAP？用途是什么？&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;OLAP：联机分析处理（online analytical processing），是数据仓库的主要应用，支持复杂的分析操作，侧重决策支持，并提供直观易懂的查询结果。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;联机分析处理（OLAP）的概念最早由关系型数据库之父E.F.Codd于1993年提出，当时引起了很大的反响，同联机事务处理（OLTP）明显地区分开来。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当今的数据处理大致可以分为两大类：OLTP和OLAP，OLTP是传统的关系型数据库的主要应用，主要是基本的、日常的事务处理，比如银行交易。OLAP是数据仓库系统的主要应用，支持复杂的分析操作，侧重决策支持，并提供直观易懂的查询结果。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;另一种解释是： 联机分析处理（On-Line Analytical Processing, OLAP）是基于数据仓库的在线多维统计分析。它允许用户在线地从多个维度观察某个度量值，从而为决策提供支持。 四、什么叫维度和度量值？&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;一个是出发点，一个是观察值 五、数据仓库的基本架构是什么？&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据源，ETL，data stage，ODS，data warehouse,datamart,OLAP等等，可能为针对每一个结构进行发问啊。 六、什么叫缓慢维度变化？&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了表现和记录基础数据变化情况在数据仓库中的记录，包括三大类维度处理方式，缓慢变化维包括三小类。 七、什么叫查找表，为什么使用替代键？&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其实目的和上面一样，从基础表到缓慢维度表的过程中的一种实现途径。 八、如何实现增量抽取？&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;主要采用时间戳方式，提供数据抽取和处理的性能。 九、用过什么ETL工具？&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;informatica，ssis，owb，datastage，以及该工具简单讲述特点。 十、ETL都包括那些组成部分？&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;工作流和数据流,数据流包括若干组件处理ETL的各个环节。 十一、用过什么报表工具？&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bo,hyperion,congo,reporting servce，以及该工具基本特点。 十二、数据仓库项目最重要或需要注意的是什么，以及如何处理？&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;一般答数据质量，主要是数据源数据质量分析，数据清洗转换，当然也可以定量分析。 十三、关于数据库部分的面试题(不是要DBA的，但是还是要具备DBA的部分知识结构) 用过什么数据库(SQLServer,Oracle)，并能够讲述其物理和逻辑结构，以Oracle为主 能够写基本的SQL语句，分组函数和关联，通常会给几个例子的 如何进行性能优化，只要能答索引的基本原理以及各种索引的区别就行了 总之，事实上数据仓库和DBA或者其他技术不同，没有什么绝对的答案，只要能表达和描述清楚自己的观点就行了。 我的命运，由我做主。]]></content>
      <categories>
        <category>数据仓库</category>
      </categories>
      <tags>
        <tag>数据仓库</tag>
        <tag>面试题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库三大设计范式]]></title>
    <url>%2F2019%2F09%2F22%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%89%E5%A4%A7%E8%AE%BE%E8%AE%A1%E8%8C%83%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;什么是数据库设计范式：简言之，就是在关系型数据库表设计的过程中所遵循的一种规范，如何设计使数据库表结构，来对数据的存储、查询、使用的性能更优。 第一范式（1NF）无重复的列&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;所谓第一范式（1NF）是指数据库表的每一列都是不可分割的基本数据项，同一列中不能有多个值，即实体中的某个属性不能有多个值或者不能有重复的属性。如果出现重复的属性，就可能需要定义一个新的实体，新的实体由重复的属性构成，新实体与原实体之间为一对多关系。在第一范式（1NF）中表的每一行只包含一个实例的信息。简而言之，第一范式就是无重复的列。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1NF的定义为：符合1NF的关系中的每个属性都不可再分 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;下表所示情况，便不符合1NF的要求： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;说明：在任何一个关系数据库中，第一范式（1NF）是对关系模式的基本要求，不满足第一范式（1NF）的数据库就不是关系数据库。 第二范式（2NF）属性完全依赖于主键&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第二范式（2NF）是在第一范式（1NF）的基础上建立起来的，即满足第二范式（2NF）必须先满足第一范式（1NF）。第二范式（2NF）要求数据库表中的每个实例或行必须可以被惟一地区分。为实现区分通常需要为表加上一个列，以存储各个实例的惟一标识。例如员工信息表中加上了员工编号（emp_id）列，因为每个员工的员工编号是惟一的，因此每个员工可以被惟一区分。这个惟一属性列被称为主关键字或主键、主码。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第二范式（2NF）要求实体的属性完全依赖于主关键字。所谓完全依赖是指不能存在仅依赖主关键字一部分的属性，如果存在，那么这个属性和主关键字的这一部分应该分离出来形成一个新的实体，新实体与原实体之间是一对多的关系。为实现区分通常需要为表加上一个列，以存储各个实例的惟一标识。简而言之，第二范式就是属性完全依赖于主键。 第三范式（3NF）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;满足第三范式（3NF）必须先满足第二范式（2NF）。简而言之，第三范式（3NF）要求一个数据库表中不包含已在其它表中已包含的非主关键字信息。例如，存在一个部门信息表，其中每个部门有部门编号（dept_id）、部门名称、部门简介等信息。那么在的员工信息表中列出部门编号后就不能再将部门名称、部门简介等与部门有关的信息再加入员工信息表中。如果不存在部门信息表，则根据第三范式（3NF）也应该构建它，否则就会有大量的数据冗余。简而言之，第三范式就是属性不依赖于其它非主属性。 也就是说， 如果存在非主属性对于码的传递函数依赖，则不符合3NF的要求。 I only need one partner: myself.]]></content>
      <categories>
        <category>数据仓库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>三大范式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈数据仓库建设中的数据建模方法]]></title>
    <url>%2F2019%2F09%2F22%2F%E6%B5%85%E8%B0%88%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%BB%BA%E8%AE%BE%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E5%BB%BA%E6%A8%A1%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;所谓水无定势，兵无常法。不同的行业，有不同行业的特点，因此，从业务角度看，其相应的数据模型是千差万别的。目前业界较为主流的是数据仓库厂商主要是 IBM 和 NCR，这两家公司除了能够提供较为强大的数据仓库平台之外，也有各自的针对某个行业的数据模型。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;例如，在银行业，IBM 有自己的 BDWM(Banking data warehouse model)，而 NCR 有自己的 FS-LDM 模型。在电信业，IBM 有 TDWM（Telecom Data warehouse model），而 NCR 有自己的 TS-LDM 模型。因此，我们看到，不同的公司有自己针对某个行业的理解，因此会有不同的公司针对某个行业的模型。而对于不同的行业，同一个公司也会有不同的模型，这主要取决于不同行业的不同业务特点。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;举例来说，IBM 的 TDWM 的模型总共包含了以下 9 个概念，如下图： 图 1. IBM 的 TDWM 概念模型 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;可能很多人要问，为什么你们的模型是 9 个概念而不是 10 个，11 个呢？你们的数据仓库模型的依据又是什么？其实这是我们在给客户介绍我们的数据模型时，经常被问到的一个问题，我希望读者在读完本文时，能够找到自己的答案。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;虽然每个行业有自己的模型，但是，我们发现，不同行业的数据模型，在数据建模的方法上，却都有着共通的基本特点。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;本文的主要目的之一，就是希望读者能够通过对本文的阅读，同时，结合自己对数据仓库建设的经验，在建设数据仓库的时候能够总结出一套适合自己的建模方法，能够更好的帮助客户去发挥数据仓库的作用。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;本文主要的主线就是回答下面三个问题： 什么是数据模型? 为什么需要数据模型? 如何建设数据模型? &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;最后，我们在本文的结尾给大家介绍了一个具体的数据仓库建模的样例，帮助大家来了解整个数据建模的过程。 一、什么是数据模型？&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据模型是抽象描述现实世界的一种工具和方法，是通过抽象的实体及实体之间联系的形式，来表示现实世界中事务的相互关系的一种映射。在这里，数据模型表现的抽象的是实体和实体之间的关系，通过对实体和实体之间关系的定义和描述，来表达实际的业务中具体的业务关系。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据仓库模型是数据模型中针对特定的数据仓库应用系统的一种特定的数据模型，一般的来说，我们数据仓库模型分为几下几个层次，如图 1 所示。 图 2. 数据仓库模型 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过上面的图形，我们能够很容易的看出在整个数据仓库的建模过程中，我们需要经历一般四个过程： 业务建模，生成业务模型，主要解决业务层面的分解和程序化。 领域建模，生成领域模型，主要是对业务模型进行抽象处理，生成领域概念模型。 逻辑建模，生成逻辑模型，主要是将领域模型的概念实体以及实体之间的关系进行数据库层次的逻辑化。 物理建模，生成物理模型，主要解决，逻辑模型针对不同关系型数据库的物理化以及性能等一些具体的技术问题。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;因此，在整个数据仓库的模型的设计和架构中，既涉及到业务知识，也涉及到了具体的技术，我们既需要了解丰富的行业经验，同时，也需要一定的信息技术来帮助我们实现我们的数据模型，最重要的是，我们还需要一个非常适用的方法论，来指导我们自己针对我们的业务进行抽象，处理，生成各个阶段的模型。 二、为什么需要数据模型？&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在数据仓库的建设中，我们一再强调需要数据模型，那么数据模型究竟为什么这么重要呢？首先我们需要了解整个数据仓库的建设的发展史。 数据仓库的发展大致经历了这样的三个过程： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;简单报表阶段：这个阶段，系统的主要目标是解决一些日常的工作中业务人员需要的报表，以及生成一些简单的能够帮助领导进行决策所需要的汇总数据。这个阶段的大部分表现形式为数据库和前端报表工具。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据集市阶段：这个阶段，主要是根据某个业务部门的需要，进行一定的数据的采集，整理，按照业务人员的需要，进行多维报表的展现，能够提供对特定业务指导的数据，并且能够提供特定的领导决策数据。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据仓库阶段：这个阶段，主要是按照一定的数据模型，对整个企业的数据进行采集，整理，并且能够按照各个业务部门的需要，提供跨部门的，完全一致的业务报表数据，能够通过数据仓库生成对业务具有指导性的数据，同时，为领导决策提供全面的数据支持。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过数据仓库建设的发展阶段，我们能够看出，数据仓库的建设和数据集市的建设的重要区别就在于数据模型的支持。因此，数据模型的建设，对于我们数据仓库的建设，有着决定性的意义。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;一般来说，数据模型的建设主要能够帮助我们解决以下的一些问题： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;进行全面的业务梳理，改进业务流程。在业务模型建设的阶段，能够帮助我们的企业或者是管理机关对本单位的业务进行全面的梳理。通过业务模型的建设，我们应该能够全面了解该单位的业务架构图和整个业务的运行情况，能够将业务按照特定的规律进行分门别类和程序化，同时，帮助我们进一步的改进业务的流程，提高业务效率，指导我们的业务部门的生产。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;建立全方位的数据视角，消灭信息孤岛和数据差异。通过数据仓库的模型建设，能够为企业提供一个整体的数据视角，不再是各个部门只是关注自己的数据，而且通过模型的建设，勾勒出了部门之间内在的联系，帮助消灭各个部门之间的信息孤岛的问题，更为重要的是，通过数据模型的建设，能够保证整个企业的数据的一致性，各个部门之间数据的差异将会得到有效解决。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;解决业务的变动和数据仓库的灵活性。通过数据模型的建设，能够很好的分离出底层技术的实现和上层业务的展现。当上层业务发生变化时，通过数据模型，底层的技术实现可以非常轻松的完成业务的变动，从而达到整个数据仓库系统的灵活性。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;帮助数据仓库系统本身的建设。通过数据仓库的模型建设，开发人员和业务人员能够很容易的达成系统建设范围的界定，以及长期目标的规划，从而能够使整个项目组明确当前的任务，加快整个系统建设的速度。 三、如何建设数据模型？&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;建设数据模型既然是整个数据仓库建设中一个非常重要的关键部分，那么，怎么建设我们的数据仓库模型就是我们需要解决的一个问题。这里我们将要详细介绍如何创建适合自己的数据模型。 数据仓库数据模型架构&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据仓库的数据模型的架构和数据仓库的整体架构是紧密关联在一起的，我们首先来了解一下整个数据仓库的数据模型应该包含的几个部分。从下图我们可以很清楚地看到，整个数据模型的架构分成 5 大部分，每个部分其实都有其独特的功能。 图 3. 数据仓库数据模型架构 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从上图我们可以看出，整个数据仓库的数据模型可以分为大概 5 大部分： 系统记录域（System of Record）：这部分是主要的数据仓库业务数据存储区，数据模型在这里保证了数据的一致性。 内部管理域（Housekeeping）：这部分主要存储数据仓库用于内部管理的元数据，数据模型在这里能够帮助进行统一的元数据的管理。 汇总域（Summary of Area）：这部分数据来自于系统记录域的汇总，数据模型在这里保证了分析域的主题分析的性能，满足了部分的报表查询。 分析域（Analysis Area）：这部分数据模型主要用于各个业务部分的具体的主题业务分析。这部分数据模型可以单独存储在相应的数据集市中。 反馈域（Feedback Area）：可选项，这部分数据模型主要用于相应前端的反馈数据，数据仓库可以视业务的需要设置这一区域。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过对整个数据仓库模型的数据区域的划分，我们可以了解到，一个好的数据模型，不仅仅是对业务进行抽象划分，而且对实现技术也进行具体的指导，它应该涵盖了从业务到实现技术的各个部分。 数据仓库建模阶段划分&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们前面介绍了数据仓库模型的几个层次，下面我们讲一下，针对这几个层次的不同阶段的数据建模的工作的主要内容： 图 4. 数据仓库建模阶段划分 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从上图我们可以清楚地看出，数据仓库的数据建模大致分为四个阶段： 业务建模，这部分建模工作，主要包含以下几个部分： 划分整个单位的业务，一般按照业务部门的划分，进行各个部分之间业务工作的界定，理清各业务部门之间的关系。 深入了解各个业务部门的内具体业务流程并将其程序化。 提出修改和改进业务部门工作流程的方法并程序化。 数据建模的范围界定，整个数据仓库项目的目标和阶段划分。 领域概念建模，这部分的建模工作，主要包含以下几个部分： 抽取关键业务概念，并将之抽象化。 将业务概念分组，按照业务主线聚合类似的分组概念。 细化分组概念，理清分组概念内的业务流程并抽象化。 理清分组概念之间的关联，形成完整的领域概念模型。 逻辑建模，这部分的建模工作，主要包含以下几个部分： 业务概念实体化，并考虑其具体的属性 事件实体化，并考虑其属性内容 说明实体化，并考虑其属性内容 物理建模，这部分的建模工作，主要包含以下几个部分： 针对特定物理化平台，做出相应的技术调整 针对模型的性能考虑，对特定平台作出相应的调整 针对管理的需要，结合特定的平台，做出相应的调整 生成最后的执行脚本，并完善之。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从我们上面对数据仓库的数据建模阶段的各个阶段的划分，我们能够了解到整个数据仓库建模的主要工作和工作量，希望能够对我们在实际的项目建设能够有所帮助。 数据仓库建模方法&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;大千世界，表面看五彩缤纷，实质上，万物都遵循其自有的法则。数据仓库的建模方法同样也有很多种，每一种建模方法其实代表了哲学上的一个观点，代表了一种归纳，概括世界的一种方法。目前业界较为流行的数据仓库的建模方法非常多，这里主要介绍范式建模法、维度建模法、实体建模法等几种方法，每种方法其实从本质上讲就是从不同的角度看我们业务中的问题，不管从技术层面还是业务层面，其实代表的是哲学上的一种世界观。我们下面给大家详细介绍一下这些建模方法。 范式建模法（Third Normal Form，3NF） &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;范式建模法其实是我们在构建数据模型常用的一个方法，该方法的主要由 Inmon 所提倡，主要解决关系型数据库的数据存储，利用的一种技术层面上的方法。目前，我们在关系型数据库中的建模方法，大部分采用的是三范式建模法。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;范式是数据库逻辑模型设计的基本理论，一个关系模型可以从第一范式到第五范式进行无损分解，这个过程也可称为规范化。在数据仓库的模型设计中目前一般采用第三范式，它有着严格的数学定义。从其表达的含义来看，一个符合第三范式的关系必须具有以下三个条件 : 每个属性值唯一，不具有多义性 ; 每个非主属性必须完全依赖于整个主键，而非主键的一部分 ; 每个非主属性不能依赖于其他关系中的属性，因为这样的话，这种属性应该归到其他关系中去。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由于范式是基于整个关系型数据库的理论基础之上发展而来的，因此，本人在这里不多做介绍，有兴趣的读者可以通过阅读相应的材料来获得这方面的知识。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;根据 Inmon 的观点，数据仓库模型的建设方法和业务系统的企业数据模型类似。在业务系统中，企业数据模型决定了数据的来源，而企业数据模型也分为两个层次，即主题域模型和逻辑模型。同样，主题域模型可以看成是业务模型的概念模型，而逻辑模型则是域模型在关系型数据库上的实例化。 图 5. 范式建模法 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从业务数据模型转向数据仓库模型时，同样也需要有数据仓库的域模型，即概念模型，同时也存在域模型的逻辑模型。这里，业务模型中的数据模型和数据仓库的模型稍微有一些不同。主要区别在于： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据仓库的域模型应该包含企业数据模型和域模型之间的关系，以及各主题域定义。数据仓库的域模型的概念应该比业务系统的主题域模型范围更加广。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在数据仓库的逻辑模型需要从业务系统的数据模型中的逻辑模型中抽象实体，实体的属性，实体的子类，以及实体的关系等。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;以笔者的观点来看，Inmon 的范式建模法的最大优点就是从关系型数据库的角度出发，结合了业务系统的数据模型，能够比较方便的实现数据仓库的建模。但其缺点也是明显的，由于建模方法限定在关系型数据库之上，在某些时候反而限制了整个数据仓库模型的灵活性，性能等，特别是考虑到数据仓库的底层数据向数据集市的数据进行汇总时，需要进行一定的变通才能满足相应的需求。因此，笔者建议读者们在实际的使用中，参考使用这一建模方式。 维度建模法 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;维度建模法，Kimball 最先提出这一概念。其最简单的描述就是，按照事实表，维表来构建数据仓库，数据集市。这种方法的最被人广泛知晓的名字就是星型模式（Star-schema）。 图 6. 维度建模法 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上图的这个架构中是典型的星型架构。星型模式之所以广泛被使用，在于针对各个维作了大量的预处理，如按照维进行预先的统计、分类、排序等。通过这些预处理，能够极大的提升数据仓库的处理能力。特别是针对 3NF 的建模方法，星型模式在性能上占据明显的优势。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;同时，维度建模法的另外一个优点是，维度建模非常直观，紧紧围绕着业务模型，可以直观的反映出业务模型中的业务问题。不需要经过特别的抽象处理，即可以完成维度建模。这一点也是维度建模的优势。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;但是，维度建模法的缺点也是非常明显的，由于在构建星型模式之前需要进行大量的数据预处理，因此会导致大量的数据处理工作。而且，当业务发生变化，需要重新进行维度的定义时，往往需要重新进行维度数据的预处理。而在这些预处理过程中，往往会导致大量的数据冗余。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;另外一个维度建模法的缺点就是，如果只是依靠单纯的维度建模，不能保证数据来源的一致性和准确性，而且在数据仓库的底层，不是特别适用于维度建模的方法。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;因此以笔者的观点看，维度建模的领域主要适用与数据集市层，它的最大的作用其实是为了解决数据仓库建模中的性能问题。维度建模很难能够提供一个完整地描述真实业务实体之间的复杂关系的抽象方法。 实体建模法 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;实体建模法并不是数据仓库建模中常见的一个方法，它来源于哲学的一个流派。从哲学的意义上说，客观世界应该是可以细分的，客观世界应该可以分成由一个个实体，以及实体与实体之间的关系组成。那么我们在数据仓库的建模过程中完全可以引入这个抽象的方法，将整个业务也可以划分成一个个的实体，而每个实体之间的关系，以及针对这些关系的说明就是我们数据建模需要做的工作。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;虽然实体法粗看起来好像有一些抽象，其实理解起来很容易。即我们可以将任何一个业务过程划分成 3 个部分，实体，事件和说明，如下图所示： 图 7. 实体建模法 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上图表述的是一个抽象的含义，如果我们描述一个简单的事实：“小明开车去学校上学”。以这个业务事实为例，我们可以把“小明”，“学校”看成是一个实体，“上学”描述的是一个业务过程，我们在这里可以抽象为一个具体“事件”，而“开车去”则可以看成是事件“上学”的一个说明。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从上面的举例我们可以了解，我们使用的抽象归纳方法其实很简单，任何业务可以看成 3 个部分： 实体，主要指领域模型中特定的概念主体，指发生业务关系的对象。 事件，主要指概念主体之间完成一次业务流程的过程，特指特定的业务过程。 说明，主要是针对实体和事件的特殊说明。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由于实体建模法，能够很轻松的实现业务模型的划分，因此，在业务建模阶段和领域概念建模阶段，实体建模法有着广泛的应用。从笔者的经验来看，在没有现成的行业模型的情况下，我们可以采用实体建模的方法，和客户一起理清整个业务的模型，进行领域概念模型的划分，抽象出具体的业务概念，结合客户的使用特点，完全可以创建出一个符合自己需要的数据仓库模型来。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;但是，实体建模法也有着自己先天的缺陷，由于实体说明法只是一种抽象客观世界的方法，因此，注定了该建模方法只能局限在业务建模和领域概念建模阶段。因此，到了逻辑建模阶段和物理建模阶段，则是范式建模和维度建模发挥长处的阶段。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;因此，笔者建议读者在创建自己的数据仓库模型的时候，可以参考使用上述的三种数据仓库的建模方法，在各个不同阶段采用不同的方法，从而能够保证整个数据仓库建模的质量。 四、数据仓库建模样例&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上面介绍的是一些抽象的建模方法和理论，可能理解起来相对有些难度，因此，笔者在这里举一个例子，读者可以跟着我们的这个样例，来初步了解整个数据仓库建模的大概过程。 背景介绍&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;熟悉社保行业的读者可以知道，目前我们国家的社保主要分为养老，失业，工伤，生育，医疗保险和劳动力市场这 6 大块主要业务领域。在这 6 大业务领域中，目前的状况养老和事业的系统已经基本完善，已经有一部分数据开始联网检测。而对于工伤，生育，医疗和劳动力市场这一块业务，有些地方发展的比较成熟，而有些地方还不够成熟。 业务建模阶段&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;基于以上的背景介绍，我们在业务建模阶段，就很容易来划分相应的业务。因此，在业务建模阶段，我们基本上确定我们本次数据仓库建设的目标，建设的方法，以及长远规划等。如下图：图 8. 业务建模阶段 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在这里，我们将整个业务很清楚地划分成了几个大的业务主线，例如：养老，失业，工伤，生育，医疗，劳动力等着几个大的部分，然后我们可以根据这些大的模块，在每个业务主线内，考虑具体的业务主线内需要分析的业务主题。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;因此，业务建模阶段其实是一次和业务人员梳理业务的过程，在这个过程中，不仅能帮助我们技术人员更好的理解业务，另一方面，也能够发现业务流程中的一些不合理的环节，加以改善和改进。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;同时，业务建模阶段的另一个重要工作就是确定我们数据建模的范围，例如：在某些数据准备不够充分的业务模块内，我们可以考虑先不建设相应的数据模型。等到条件充分成熟的情况下，我们可以再来考虑数据建模的问题。 领域概念建模阶段&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;领域概念建模阶段是数据仓库数据建模的一个重要阶段，由于我们在业务建模阶段已经完全理清相应的业务范围和流程，因此，我们在这个领域概念建模阶段的最主要的工作就是进行概念的抽象，整个领域概念建模的工作层次如下图所示：图 9. 领域概念建模阶段 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从上图我们可以清楚地看到，领域概念建模就是运用了实体建模法，从纷繁的业务表象背后通过实体建模法，抽象出实体，事件，说明等抽象的实体，从而找出业务表象后抽象实体间的相互的关联性，保证了我们数据仓库数据按照数据模型所能达到的一致性和关联性。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从图上看，我们可以把整个抽象过程分为四个层次，分别为： 抽象方法层，整个数据模型的核心方法，领域概念建模的实体的划分通过这种抽象方法来实现。 领域概念层，这是我们整个数据模型的核心部分，因为不同程度的抽象方法，决定了我们领域概念的不同。例如：在这里，我们可以使用“参与方”这个概念，同时，你也可以把他分成三个概念：“个人”，“公司”，和“经办机构”这三个概念。而我们在构建自己的模型的时候，可以参考业务的状况以及我们自己模型的需要，选择抽象程度高的概念或者是抽象程度低的概念。相对来说，抽象程度高的概念，理解起来较为复杂，需要专业的建模专家才能理解，而抽象程度低的概念，较适合于一般业务人员的理解，使用起来比较方便。笔者在这里建议读者可以选用抽象概念较低的实体，以方便业务人员和技术人员之间的交流和沟通。 具体业务层，主要是解决具体的业务问题，从这张图我们可以看出，具体的业务层，其实只是领域概念模型中实体之间的一些不同组合而已。因此，完整的数据仓库的数据模型应该能够相应灵活多变的前端业务的需求，而其本身的模型架构具有很强的灵活性。这也是数据仓库模型所具备的功能之一。 业务主线层，这个层次主要划分大的业务领域，一般在业务建模阶段即已经完成这方面的划分。我们一般通过这种大的业务主线来划分整个业务模型大的框架。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过领域概念建模，数据仓库的模型已经被抽象成一个个的实体，模型的框架已经搭建完毕，下面的工作就是给这些框架注入有效的肌体。 逻辑建模阶段&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过领域概念建模之后，虽然模型的框架已经完成，但是还有很多细致的工作需要完成。一般在这个阶段，我们还需要做非常多的工作，主要包括：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;实例化每一个抽象的实体，例如：在上面的概念模型之后，我们需要对“人”和“公司”等这些抽象实体进行实例化。主要是，我们需要考虑“人”的属性包括那些，在业务模块中，用到的所有跟“人”相关的属性是哪些，我们都需要将这些属性附着在我们数据模型的“人”这个实体上，例如“人”的年龄，性别，受教育程度等等。同理，我们对其他属性同样需要做这个工作。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;找出抽象实体间的联系，并将其实例化。这里，我们主要考虑是“事件”这个抽象概念的实例化，例如：对于养老金征缴这个“事件”的属性的考虑，对于失业劳动者培训这个“事件”的属性的考虑等等。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;找出抽象事件的关系，并对其进行说明。在这里我们主要是要针对“事件”进行完善的“说明”。例如：对于“事件”中的地域，事件等因素的考量等等。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;总而言之，在逻辑建模阶段，我们主要考虑的是抽象实体的一些细致的属性。通过逻辑建模阶段，我们才能够将整个概念模型完整串联成一个有机的实体，才能够完整的表达出业务之间的关联性。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在这个阶段，笔者建议大家可以参考 3NF 的建模方法，表达出实体的属性，以及实体与实体之间的联系。例如：在这个阶段，我们可以通过采用 ERWIN 等建模工具等作出符合 3NF 的关系型数据模型来。 物理建模阶段&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;物理建模阶段是整个数据建模的最后一个过程，这个过程其实是将前面的逻辑数据模型落地的一个过程。考虑到数据仓库平台的不同，因此，数据模型的物理建模过程可能会稍微有一些不同，在这个阶段我们主要的工作是： 生成创建表的脚本。不同的数据仓库平台可能生成不同的脚本。 针对不同的数据仓库平台，进行一些相应的优化工作，例如对于 DB2 数据仓库来说，创建一些 MQT 表（数据库中的视图view的概念，不过MQT会将数据存储起来，而不是直接查询源表），来加速报表的生成等等。 针对数据集市的需要，按照维度建模的方法，生成一些事实表，维表等工作。 针对数据仓库的 ETL 车和元数据管理的需要，生成一些数据仓库维护的表，例如：日志表等。 经过物理建模阶段，整个数据仓库的模型已经全部完成，我们可以按照自己的设计来针对当前的行业创建满足自己需要的数据模型来。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里，笔者通过一个数据建模的样例，希望能够给读者一个关于数据仓库建模的感性的认识。希望读者在利用这些数据仓库的建模方法创建自己的数据模型的时候，可以根据业务实际的需要和自己对抽象能力的把握来创建适合自己的数据模型。 参考资料： 用 Rational Data Architect 简化数据建模和集成设计。 通过访问 IM 和 Rational 集成应用开发专栏 获得更多建模方面的文章、教程和多媒体课件等的技术资源。 通过访问 developerWorks IM 专区 获得更多文章、教程和多媒体课件等的技术资源。 通过参与 developerWorks blog 加入 developerWorks 社区。 原文链接：数据仓库模型建设 Work, do your one hundred percent,eat, eat,laugh, laugh.]]></content>
      <categories>
        <category>数据仓库</category>
      </categories>
      <tags>
        <tag>数据仓库</tag>
        <tag>模型设计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据仓库元数据管理系统]]></title>
    <url>%2F2019%2F09%2F21%2F%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%85%83%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;相信很多朋友都是第一次听说元数据管理系统这个名词，当然，从事非数据仓库工作的人，很少会接触到这个系统，即使是正在从事这方面工作的朋友，可能仍然对它不是很了解，那么今天我来聊一聊元数据管理系统。本文大部分观点与图片汇总自网络，如有不同观点，欢迎留言交流～～ . 一、元数据的定义&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;按照传统的定义，元数据（Metadata）是关于数据的数据。在数据仓库系统中，元数据可以帮助数据仓库管理员和数据仓库的开发人员非常方便地找到他们所关心的数据；元数据是描述数据仓库内数据的结构和建立方法的数据，可将其按用途的不同分为两类：技术元数据（Technical Metadata）和业务元数据（Business Metadata）。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;技术元数据是存储关于数据仓库系统技术细节的数据，是用于开发和管理数据仓库使用的数据，它主要包括以下信息： 数据仓库结构的描述，包括仓库模式、视图、维、层次结构和导出数据的定义，以及数据集市的位置和内容； 业务系统、数据仓库和数据集市的体系结构和模式； 汇总用的算法，包括度量和维定义算法，数据粒度、主题领域、聚集、汇总、预定义的查询与报告； 由操作环境到数据仓库环境的映射，包括源数据和它们的内容、数据分割、数据提取、清理、转换规则和数据刷新规则、安全（用户授权和存取控制）。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;业务元数据从业务角度描述了数据仓库中的数据，它提供了介于使用者和实际系统之间的语义层，使得不懂计算机技术的业务人员也能够“读懂”数据仓库中的数据。业务元数据主要包括以下信息：使用者的业务术语所表达的数据模型、对象名和属性名；访问数据的原则和数据的来源；系统所提供的分析方法以及公式和报表的信息；具体包括以下信息： 企业概念模型：这是业务元数据所应提供的重要的信息，它表示企业数据模型的高层信息、整个企业的业务概念和相互关系。以这个企业模型为基础，不懂数据库技术和SQL语句的业务人员对数据仓库中的数据也能做到心中有数。 多维数据模型：这是企业概念模型的重要组成部分，它告诉业务分析人员在数据集市当中有哪些维、维的类别、数据立方体以及数据集市中的聚合规则。这里的数据立方体表示某主题领域业务事实表和维表的多维组织形式。 业务概念模型和物理数据之间的依赖：以上提到的业务元数据只是表示出了数据的业务视图，这些业务视图与实际的数据仓库或数据库、多维数据库中的表、字段、维、层次等之间的对应关系也应该在元数据知识库中有所体现。 二、元数据的作用&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;与其说数据仓库是软件开发项目，还不如说是系统集成项目，因为它的主要工作是把所需的数据仓库工具集成在一起，完成数据的抽取、转换和加载，OLAP分析和数据挖掘等。如下图所示，它的典型结构由操作环境层、数据仓库层和业务层等组成。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其中，第一层（操作环境层）是指整个企业内有关业务的OLTP系统和一些外部数据源；第二层是通过把第一层的相关数据抽取到一个中心区而组成的数据仓库层；第三层是为了完成对业务数据的分析而由各种工具组成的业务层。图中左边的部分是元数据管理，它起到了承上启下的作用，具体体现在以下几个方面： 1.元数据是进行数据集成所必需的&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据仓库最大的特点就是它的集成性。这一特点不仅体现在它所包含的数据上，还体现在实施数据仓库项目的过程当中。一方面，从各个数据源中抽取的数据要按照一定的模式存入数据仓库中，这些数据源与数据仓库中数据的对应关系及转换规则都要存储在元数据知识库中；另一方面，在数据仓库项目实施过程中，直接建立数据仓库往往费时、费力，因此在实践当中，人们可能会按照统一的数据模型，首先建设数据集市，然后在各个数据集市的基础上再建设数据仓库。不过，当数据集市数量增多时很容易形成“蜘蛛网”现象，而元数据管理是解决“蜘蛛网”的关键。如果在建立数据集市的过程中，注意了元数据管理，在集成到数据仓库中时就会比较顺利；相反，如果在建设数据集市的过程中忽视了元数据管理，那么最后的集成过程就会很困难，甚至不可能实现。 2.元数据定义的语义层可以帮助用户理解数据仓库中的数据&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;最终用户不可能象数据仓库系统管理员或开发人员那样熟悉数据库技术，因此迫切需要有一个“翻译”，能够使他们清晰地理解数据仓库中数据的含意。元数据可以实现业务模型与数据模型之间的映射，因而可以把数据以用户需要的方式“翻译”出来，从而帮助最终用户理解和使用数据。 3.元数据是保证数据质量的关键&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据仓库或数据集市建立好以后，使用者在使用的时候，常常会产生对数据的怀疑。这些怀疑往往是由于底层的数据对于用户来说是不“透明”的，使用者很自然地对结果产生怀疑。而借助元数据管理系统，最终的使用者对各个数据的来龙去脉以及数据抽取和转换的规则都会很方便地得到，这样他们自然会对数据具有信心；当然也可便捷地发现数据所存在的质量问题。甚至国外有学者还在元数据模型的基础上引入质量维，从更高的角度上来解决这一问题。 4.元数据可以支持需求变化&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;随着信息技术的发展和企业职能的变化，企业的需求也在不断地改变。如何构造一个随着需求改变而平滑变化的软件系统，是软件工程领域中的一个重要问题。传统的信息系统往往是通过文档来适应需求变化，但是仅仅依靠文档还是远远不够的。成功的元数据管理系统可以把整个业务的工作流、数据流和信息流有效地管理起来，使得系统不依赖特定的开发人员，从而提高系统的可扩展性。 三、元数据管理功能 1.数据地图&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据地图展现是以拓扑图的形式对数据系统的各类数据实体、数据处理过程元数据进行分层次的图形化展现，并通过不同层次的图形展现粒度控制，满足开发、运维或者业务上不同应用场景的图形查询和辅助分析需要。 2.元数据分析2.1 血缘分析&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;血缘分析（也称血统分析）是指从某一实体出发，往回追溯其处理过程，直到数据系统的数据源接口。对于不同类型的实体，其涉及的转换过程可能有不同类型，如：对于底层仓库实体，涉及的是ETL处理过程；而对于仓库汇总表，可能既涉及ETL处理过程，又涉及仓库汇总处理过程；而对于指标，则除了上面的处理过程，还涉及指标生成的处理过程。数据源接口实体由源系统提供，作为数据系统的数据输入，其它的数据实体都经过了一个或多个不同类型的处理过程。血缘分析正是提供了这样一种功能，可以让使用者根据需要了解不同的处理过程，每个处理过程具体做什么，需要什么样的输入，又产生什么样的输出。 2.2 影响分析&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;影响分析是指从某一实体出发，寻找依赖该实体的处理过程实体或其他实体。如果需要可以采用递归方式寻找所有的依赖过程实体或其他实体。该功能支持当某些实体发生变化或者需要修改时，评估实体影响范围。 2.3 实体关联分析&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;实体关联分析是从某一实体关联的其它实体和其参与的处理过程两个角度来查看具体数据的使用情况，形成一张实体和所参与处理过程的网络，从而进一步了解该实体的重要程度。本功能可以用来支撑需求变更影响评估的应用。 2.4 实体差异分析&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;实体差异分析是对元数据的不同实体进行检查，用图形和表格的形式展现它们之间的差异，包括名字、属性及数据血缘和对系统其他部分影响的差异等,在数据系统中存在许多类似的实体。这些实体（如数据表）可能只有名字上或者是在属性中存在微小的差异，甚至有部分属性名字都相同，但处于不同的应用中。由于各种原因，这些微小的差异直接影响了数据统计结果，数据系统需要清楚了解这些差异。本功能有助于进一步统一统计口径，评估近似实体的差异 2.5 指标一致性分析&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;指标一致性分析是指用图形化的方式来分析比较两个指标的数据流图是否一致，从而了解指标计算过程是否一致。该功能是指标血缘分析的一种具体应用。指标一致性分析可以帮助用户清楚地了解到将要比较的两个指标在经营分析数据流图中各阶段所涉及的数据对象和转换关系是否一致，帮助用户更好地了解指标的来龙去脉，清楚理解分布在不同部门且名称相同的指标之间的差异，从而提高用户对指标值的信任。 3.辅助应用优化&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;元数据对数据系统的数据、数据加工过程以及数据间的关系提供了准确的描述，利用血缘分析、影响分析和实体关联分析等元数据分析功能，可以识别与系统应用相关的技术资源，结合应用生命周期管理过程，辅助进行数据系统的应用优化. 4.辅助安全管理&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;企业数据平台所存储的数据和提供的各类分析应用，涉及到公司经营方面的各类敏感信息。因此在数据系统建设过程中，须采用全面的安全管理机制和措施来保障系统的数据安全。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据系统安全管理模块负责数据系统的数据敏感度、客户隐私信息和各环节审计日志记录管理，对数据系统的数据访问和功能使用进行有效监控。为实现数据系统对敏感数据和客户隐私信息的访问控制，进一步实现权限细化，安全管理模块应以元数据为依据，由元数据管理模块提供敏感数据定义和客户隐私信息定义，辅助安全管理模块完成相关安全管控操作。 5.基于元数据的开发管理&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据系统项目开发的主要环节包括：需求分析、设计、开发、测试和上线。开发管理应用可以提供相应的功能，对以上各环节的工作流程、相关资源、规则约束、输入输出信息等提供管理和支持。 End～ I will be your side till the day i die.]]></content>
      <categories>
        <category>数据仓库</category>
      </categories>
      <tags>
        <tag>数据仓库</tag>
        <tag>元数据管理系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[P2P（PeerToPeer）网络原理]]></title>
    <url>%2F2019%2F09%2F18%2FP2P%EF%BC%88PeerToPeer%EF%BC%89%E7%BD%91%E7%BB%9C%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;最近在研究P2P技术，奈何相关资料不多，自己琢磨了一下，分享一下学习P2P的一些原理, 以及如何打造一个P2P聊天应用。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里指的P2P是指peer to peer， 点对点的技术， 每个客户端都是服务端，没有中心服务器，不是websocket针对某个connection推送消息。 一、技术要点 udp协议 节点之间的建立连接和广播 内网穿透，如何能让两个处在内网的节点，相互发现自己的存在，并且建立通信 二、原理&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;首先解决的是内网穿透的问题，常见的底层协议tcp，udp，他们各自有优缺点，简单说明一下。 tcp：需要处理粘包问题，双工流通道，是可靠的链接。 udp： 每次发送的都是数据包，没有粘包问题，但是连接不可靠，只能传输少量数据。更加详细的请Google。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里选择udp协议，简单一些。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;再下来是内网穿透，先说结论： 两个处于不同内部网络的节点，永远无法发现他们之间的相互存在，你就算是想顺着网线过去打他都不行。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;所有的内网穿透原理无外乎需要一个有公网ip的中介服务器，包括虚拟货币像比特币之类的，所以首先要有一个创世节点 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在NodeJS中，创建udp服务也很简单 123const dgram = require(&quot;dgram&quot;);const udp = dgram.createSocket(&quot;udp4&quot;);udp.bind(1090, callback) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;把服务部署要公网，那么其他所有的节点都能访问，通过中转服务器，能够使得两个节点可以建立连接 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们是要建立这样的P2P网络 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;假如现在只有3个节点： 创世节点, B节点, C节点， 创世节点有公网IP &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我用对话的形式，阐述他们建立连接的过程: B节点: hey，创世节点，我要加入到P2P网络里面，告诉其他兄弟，我来了创世节点: 兄弟们，刚刚有个叫做B的节点加入网络了，你们也去告诉其他节点其他节点: 刚刚收到来自 “创世节点”的通知，有个fresh meet加入网络了，叫做 “B” … 至此，所有人都知道了B节点加入了网络，里面记载着B节点的相关信息，包括IP地址，包括udp端口号 此时C节点也要加入网络，并且想要和B节点对话: C节点: hey，创世节点，我要加入到P2P网络里面，并且我要和B对话创世节点: 兄弟们，刚刚有个叫做C的节点加入网络了，你们也去告诉其他节点，顺便看看有没有B这个节点其他节点: 刚刚收到来自 “创世节点”的通知，有个fresh meet加入网络了，叫做 “C”，你们也看看有没有B这个节点其他节点2: 收到通知，听说一个叫做C的节点在找一个B节点，我这里有它的信息，ip是xxxx.xxxx.xxx.xxxx, 端口10086B节点: 有个C的家伙(ip: xxxx.xxxx.xxxx.xxxx, 端口1000)要找我 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;到这里，B获取到了C的信息，包括IP和端口，C也拿到了B的信息. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;于是，他们两个就可以建立通信。消息流: B &lt;—-&gt; C. 中间不经过任何服务器 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;用一张图来形容: 三、总结&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在设计中，每个节点的功能都是一样的。如果需要加入到网络中，不一定跟创世节点链接 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;假设已存在的节点: 创世节点，A、B、C节点，此时有个D节点想要加入到网络。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;那么D节点不一定非得链接到创世节点，可以链接到A、B、C中的任意一个节点，然后该节点再广播给其他节点说”Hey, 有个新人叫做D的加入了网络”。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这样所有人都知道，有个叫做D的节点存在，你可以和它通信，同时D节点和会同步已存在的节点。这样D节点也知道了其他节点的存在了。 找到自己感兴趣的事情，然后100%投入。]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>P2P网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据仓库模型设计]]></title>
    <url>%2F2019%2F09%2F17%2F%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%A8%A1%E5%9E%8B%E8%AE%BE%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;大千世界，万物都有其遵循的自有法则，数据仓库也不例外，根据业务场景，选择不同的设计模式，解决不同的业务问题。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;下面来看看数据仓库的三种设计模式。 一、范式建模法（Third Normal Form，3NF）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;范式建模法其实是我们在构建数据模型常用的一个方法，该方法的主要由 Inmon 所提倡，主要解决关系型数据库得数据存储，利用的一种技术层面上的方法。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;目前，我们在关系型数据库中的建模方法，大部分采用的是三范式建模法。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;范式是数据库逻辑模型设计的基本理论，一个关系模型可以从第一范式到第五范式进行无损分解，这个过程也可称为规范化。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在数据仓库的模型设计中目前一般采用第三范式，它有着严格的数学定义。从其表达的含义来看，一个符合第三范式的关系必须具有以下三个条件 : 1.每个属性值唯一，不具有多义性 ;2.每个非主属性必须完全依赖于整个主键，而非主键的一部分 ;3.每个非主属性不能依赖于其他关系中的属性，因为这样的话，这种属性应该归到其他关系中去。 优点： 从关系型数据库的角度出发，结合了业务系统的数据模型，能够比较方便的实现数据仓库的建模。 缺点： 由于建模方法限定在关系型数据库之上，在某些时候反而限制了整个数据仓库模型的灵活性，性能等，特别是考虑到数据仓库的底层数据向数据集市的数据进行汇总时，需要进行一定的变通才能满足相应的需求。 二、维度建模法&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;维度建模法，Kimball 最先提出这一概念。其最简单的描述就是，按照事实表，维表来构建数据仓库，数据集市。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;事实表是用来记录具体事件的，包含了每个事件的具体要素，以及具体发生的事情；维表则是对事实表中事件的要素的描述信息。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;比如一个事件会包含时间、地点、人物、事件，事实表记录了整个事件的信息，但对时间、地点和人物等要素只记录了一些关键标记，比如事件的主角叫“Michael”，那么Michael到底“长什么样”，就需要到相应的维表里面去查询“Michael”的具体描述信息了。 优点: 1.维度建模非常直观，紧紧围绕着业务模型，可以直观的反映出业务模型中的业务问题。2.不需要经过特别的抽象处理，即可以完成维度建模。这一点也是维度建模的优势。 缺点: 1.由于在构建星型模式之前需要进行大量的数据预处理，因此会导致大量的数据处理工作。2.而且，当业务发生变化，需要重新进行维度的定义时，往往需要重新进行维度数据的预处理。而在这些与处理过程中，往往会导致大量的数据冗余。3.如果只是依靠单纯的维度建模，不能保证数据来源的一致性和准确性，而且在数据仓库的底层，不是特别适用于维度建模的方法。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;因此以笔者的观点看，维度建模的领域主要适用与数据集市层，它的最大的作用其实是为了解决数据仓库建模中的性能问题。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;维度建模很难能够提供一个完整地描述真实业务实体之间的复杂关系的抽象方法。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;基于事实表和维表就可以构建出多种多维模型，包括星形模型、雪花模型和星座模型。 1.星型模式（Star-schema）星型模式的核心是一个大的中心表（事实表），一组小的附属表（维表）。星型模式示例如下所示： 可以看出，星形模式的维度建模由一个事实表和一组维表成，且具有以下特点： a. 维表只和事实表关联，维表之间没有关联；b. 每个维表的主码为单列，且该主码放置在事实表中，作为两边连接的外码；c. 以事实表为核心，维表围绕核心呈星形分布； 2.雪花模型&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;雪花模式是星型模式的扩展，其中某些维表被规范化，进一步分解到附加表（维表）中。雪花模式示例如下图所示： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从图中我们可以看到地址表被进一步细分出了城市（city）维。supplier_type表被进一步细分出来supplier维。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;星形模式中的维表相对雪花模式来说要大，而且不满足规范化设计。雪花模型相当于将星形模式的大维表拆分成小维表，满足了规范化设计。然而这种模式在实际应用中很少见，因为这样做会导致开发难度增大，而数据冗余问题在数据仓库里并不严重。 3.星座模型&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据仓库由多个主题构成，包含多个事实表，而维表是公共的，可以共享，这种模式可以看做星型模式的汇集，因而称作星系模式或者事实星座模式。本模式示例如下图所示： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如上图所示，事实星座模式包含两个事实表：sales和shipping，二者共享维表。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;事实星座模式是数据仓库最常使用的数据模式，尤其是企业级数据仓库（EDW）。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;前面介绍的两种维度建模方法都是多维表对应单事实表，但在很多时候维度空间内的事实表不止一个，而一个维表也可能被多个事实表用到。在业务发展后期，绝大部分维度建模都采用的是星座模式。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这也是数据仓库区别于数据集市的一个典型的特征，从根本上而言，数据仓库数据模型的模式更多是为了避免冗余和数据复用，套用现成的模式，是设计数据仓库最合理的选择。 4.三种模式对比 三、实体建模法&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;实体建模法并不是数据仓库建模中常见的一个方法，它来源于哲学的一个流派。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;那么我们在数据仓库的建模过程中完全可以引入这个抽象的方法，将整个业务也可以划分成一个个的实体，而每个实体之间的关系，以及针对这些关系的说明就是我们数据建模需要做的工作。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;即我们可以将任何一个业务过程划分成 3 个部分，实体，事件和说明。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;例如我们描述一个简单的事实：“小明开车去学校上学”。以这个业务事实为例，我们可以把“小明”，“学校”看成是一个实体，“上学”描述的是一个业务过程，我们在这里可以抽象为一个具体“事件”，而“开车去”则可以看成是事件“上学”的一个说明。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从上面的举例我们可以了解，我们使用的抽象归纳方法其实很简单，任何业务可以看成 3 个部分： 1.实体，主要指领域模型中特定的概念主体，指发生业务关系的对象。2.事件，主要指概念主体之间完成一次业务流程的过程，特指特定的业务过程。3.说明，主要是针对实体和事件的特殊说明。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由于实体建模法，能够很轻松的实现业务模型的划分，因此，在业务建模阶段和领域概念建模阶段，实体建模法有着广泛的应用。从笔者的经验来看，再没有现成的行业模型的情况下，我们可以采用实体建模的方法，和客户一起理清整个业务的模型，进行领域概念模型的划分，抽象出具体的业务概念，结合客户的使用特点，完全可以创建出一个符合自己需要的数据仓库模型来。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;但是，实体建模法也有着自己先天的缺陷，由于实体说明法只是一种抽象客观世界的方法，因此，注定了该建模方法只能局限在业务建模和领域概念建模阶段。因此，到了逻辑建模阶段和物理建模阶段，则是范式建模和维度建模发挥长处的阶段。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;因此，笔者建议读者在创建自己的数据仓库模型的时候，可以参考使用上述的三种数据仓库得建模方法，在各个不同阶段采用不同的方法，从而能够保证整个数据仓库建模的质量。 实践才是检验真理的唯一标准]]></content>
      <categories>
        <category>数据仓库</category>
      </categories>
      <tags>
        <tag>数据仓库模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库、数据仓库、数据集市的区别与联系]]></title>
    <url>%2F2019%2F09%2F17%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E3%80%81%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E3%80%81%E6%95%B0%E6%8D%AE%E9%9B%86%E5%B8%82%E7%9A%84%E5%8C%BA%E5%88%AB%E4%B8%8E%E8%81%94%E7%B3%BB%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据库（Database）：按照数据结构来组织、存储、管理数据，建立在计算机存储设备上面的仓库。数据库一般适用于操作型系统。因为符合三范式的设置，数据的一致性较好。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据仓库(Data Warehouse) 是一个面向主题的(SubjectOri2ented) 、集成的(Integrate) 、相对稳定的(Non -Volatile) 、反映历史变化(TimeVariant) 的数据集合用于支持管理决策。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据集市（Data Market）：数据集市不同于数据仓库，一般是服务于某几个部门。数据仓库向各个数据集市提供数据，且一般来讲，数据仓库的表设计符合规范化设计，而数据集市一般使用维度建模。 一、数据库&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;按照数据结构来组织、存储、管理数据的建立在计算机存储设备上面的仓库。数据库一般适用于操作型系统。因为符合三范式的设置，数据的一致性较好。 二、数据仓库&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据仓库(Data Warehouse) 是一个面向主题的(SubjectOri2ented) 、集成的( Integrate ) 、相对稳定的(Non -Volatile ) 、反映历史变化( TimeVariant) 的数据集合用于支持管理决策。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这个定义比较系统地阐述了数据仓库的特点，下面我们一一解读。 1. 面向主题的&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据仓库是为了提供决策服务的，会建立不同的主题，而主题是进行决策时需要重点关注的部分。 2. 集成的&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据仓库可以整合来自不同数据源的数据，将这个数据入库、清洗整合成统一的标准化数据。同时上文中提到的一个主题往往与多个系统相关，集成的数据很好的满足了主题构建的数据需求。数据仓库对原有的分散的数据库进行数据抽取、清理的基础上经过系统加工、汇总整理得到，清除原数据中的不一致性（面向事务的数据库往往单独存放单个系统的数据，且不同数据库相互独立，且是异构的）。 3. 相对稳定的&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据仓库的数据面向决策，这就表明了仓库中的数据进入之后就会长期保留，主要面对的是查询， 修改与删除操作比较少，一般是定期的加载、更新。 4. 反映历史变化的&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据仓库中会保存各个日期节点的数据，以满足不断变化的业务的需求。 三、数据集市&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据集市不同于数据仓库，一般是服务于某几个部门。数据仓库向各个数据集市提供数据，且一般来讲，数据仓库的表设计符合规范化设计，而数据集市一般使用维度建模。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;请看下图： 四、总结&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于操作型系统，我们希望数据便于修改、满足一致性，因此产生了三范式数据库；在面对企业级决策需求的数据支撑时，我们希望系统可以集成不同的数据源的数据、数据稳定、结构统一、保存历史数据，可以满足不同部门的不断变化的数据系统，因此产生了数据仓库；对于不同的部门来讲，进行决策时如果直接访问数据仓库，得到信息需要多张表进行关联，访问压力大，且主题繁多不易于管理。因此需要建立数据集市，从数据仓库中直接取数，对数据进行汇总整理以满足特定部门的需求。 仰望星空，脚踏实地]]></content>
      <categories>
        <category>数据仓库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>数据仓库</tag>
        <tag>数据集市</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[零知识证明]]></title>
    <url>%2F2019%2F09%2F16%2F%E9%9B%B6%E7%9F%A5%E8%AF%86%E8%AF%81%E6%98%8E%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;零知识证明（Zero-Knowledge Proof）或零知识协议是一种基于概率的验证方法，包括两部分：宣称某一命题为真的证明者（prover）和确认该命题确实为真的验证者（verifier）。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;零知识证明指的是证明者能够在不向验证者提供任何有用的信息的情况下，使验证者相信某个论断是正确的，在密码学中非常有用。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;顾名思义，零知识证明就是既能充分证明自己是某种权益的合法拥有者，又不把有关的信息泄漏出去，即给外界的 “知识” 为“零”。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;“能够在不知道用户是谁，或者他们有多少钱的情况下判断‘一个用户是否有足够的钱发送给另一个用户’的问题，是零知识证明在区块链中的主要应用之一。”&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;——Demiro Massessi 1.为何零知识证明如此重要？&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据隐私是当今社会最重要的课题之一。保护与个人身份有关的个人资料 (出生日期、银行月结单、交易记录、学历) 极为重要，并会不断增加其重要性。在科技时代，我们正在生成前所未有的海量数据，而我们不断创造的关于我们自己的数据也在不断被获取。像谷歌和 Facebook 这样的大公司已经利用我们的数据成为了今天主宰世界的科技巨头。然而，最近密码学的突破和区块链的兴起使一种新的方法能够帮助保护我们的数据和身份，甚至保护我们与之交互的组织。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;零知识证明可能就是如何保护数据隐私的答案。 2.零知识证明的原则&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;零知识证明是麻省理工学院研究人员在 20 世纪 80 年代提出的一种加密方案。零知识证明协议是一方 (证明者) 向另一方 (验证者) 证明某件事情是真实的一种方法。除了该特定声明是真实的以外，没有披露任何其他信息。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;例如，当前网站将用户密码的哈希值存储在其 web 服务器中。为了验证客户端是否真的知道密码，大多数网站目前使用的方法是对客户端输入的密码进行哈希值计算，并将其与存储的结果进行比较。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;零知识证明可以保护用户的帐号信息不被泄露。如果零知识证明可以实现，那么在客户的密码是未知的情况下，仍然可以在客户端登录进行身份验证。当服务器受到攻击时，用户的帐户仍然是安全的，因为客户的密码没有存储在 web 服务器中。 3.交互式零知识证明&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;零知识证明协议的基础是交互式的。它要求验证者不断地提出一系列关于证明者所知道的 “知识” 的问题。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;例如，如果有人声称知道九宫格谜题的答案，零知识证明就是验证者随机指定按列、行或九个正方形进行验证。每个测试不需要知道具体的答案，只需要检测数字 “1” 到“9”是否包含在其内。只要验证的次数足够多，就有可能判断证明者是否知道九宫格谜题的答案。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;然而，这种简单的验证方式并不能使人们相信证明者和验证者都未做伪证。在九宫格游戏中，两者可能会事先串通，以便证明者在不知道答案的情况下通过验证。如果他们想说服第三方相信这个结果，验证者还必须证明验证过程是随机的，并且它不会将答案泄露给证明者。因此，第三方很难验证交互零知识证明的结果，需要第三方的参与，等额外的努力和成本才能向多人证明某件事是真实的。 4.非交互式零知识证明&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;非交互式零知识证明，顾名思义，不需要交互式过程，避免了验证者和证明者串通的可能性，但可能需要第三方机器和程序来确定验证的顺序。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;例如，在九宫格游戏中，由第三方程序决定要验证哪一列或哪一行。验证序列必须保密，否则验证者可能在不知道真实 “知识” 的情况下通过验证序列。 5.零知识证明在区块链中的应用&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;比特币和以太坊网络都使用公共地址来代替验证者和证明者的真实身份，使得交易部分匿名; 只有发送和接收地址，以及交易数量是公众知道的。但是，通过区块链上提供的各种信息，如交互记录等，可以发现地址的真实身份，存在隐私暴露的隐患。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;用了零知识证明之后，发送方、接收方和第三方的细节信息可以保持匿名，同时保证交易有效。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;最早使用零知识证明技巧的区块链叫做 Zcash，实际的作法叫做 Zk-Snarks，这是许多零知识证明的做法之一，也是最有名的一个。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Zk-Snarks 是 “零知识简洁无交互知识认证” 的简称，是一种在无需泄露数据本身情况下证明某些数据运算的一种零知识证明。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Zk-Snarks 技术缩减了证明所需的时间和验证它们所需的计算量。它能够证明有效交易的条件已经满足，而不需要透露交易所涉及的地址或交易量的任何关键信息。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Zcash 可以将交易纪录上的汇款者、收款者和金额都经过加密隐藏起来，因此矿工无从得知这些交易上的细节，但仍然可以验证交易。不过，目前多数使用者在 Zcash 上的交易，还是选择未经加密的作法，因为花费的成本比较高。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;另外，以太坊（Ethereum）上的智能合约目前也已经可以运用 Zk-Snarks 这套零知识证明的作法。但以太坊不完全是从隐私的角度切入，而是从节省运算成本的角度应用零知识证明。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;透过 Zk-Snarks，以太坊矿工可以不用再重新执行交易的运算，而是只要对方提得出证明即可。大概就像我不需要真的知道你会高一到高三的数学，而只要看到高中毕业证就能确定你懂高中数学。不过，这只有在制作证明的成本，远低于实际运算成本的情况下才划算。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Zk-Snarks 将需要验证的交易内容转化为两个多项式乘积相等的证明，并结合同态加密等高级技术，在执行事务验证的同时保护隐藏的事务量。其过程可简单描述为: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;将代码分解为可验证的逻辑验证步骤，然后将这些步骤分解为由加减乘除组成的计算流程。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;进行一系列变换，将待验证代码转换为多项式方程，如 t(x)h(x)= w(x)v(x)。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了使证明更加简洁，验证者事先随机选择几个检查点 s，检查这些点上的方程是否为真。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过同态编码或加密，验证者在计算方程时不知道实际输入值，但仍然可以验证。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在等式的左右两边，乘以一个不等于 0 的密值 k。当验证 (t(s)h(s)k) = (w(s)v(s)k) 时，具体的 t(s)、h(s)、w(s)、v(s)是不可知的，可以对信息进行保护。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当前履行 Zk-Snarks 算法的一个缺陷是需要在 advanced 中内置参数。如果这些参数泄露，整个网络将面临毁灭性的破坏。因此，用户必须信任在使用这些网络时不会泄露的信息。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;可能的解决方案包括使用现代的“可信执行环境”，如 Intel SGX 和 ARM TrustZone。对于 Intel 的 SGX 技术，即使应用程序、操作系统、BIOS 或 VMM 受到威胁，私钥也是安全的。此外，最近的一份白皮书揭示了它在零知识密码学中的创新：Zk-Snarkss(零知识可伸缩透明知识参数)。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;根据 Zk-Snarks 白皮书，Zk-Snarks 是第一个不依赖任何信任设置实现区块链验证的系统，而随着计算数据数量的增加，计算速度呈指数增长。它不依赖于公钥加密系统，而且更简单的假设使它在理论上更安全，因为它唯一的加密假设是哈希函数 (如 SHA2) 是不可预测的。零知识证明和 Zk-S(T|N)ARK 等技术的测试和采用需要时间。 知道一点东西，并不能说明你会写应用，完整的应用是需要经验积累的。]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>零知识证明</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MVC设计模式]]></title>
    <url>%2F2019%2F09%2F15%2FMVC%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[总结一下MVC(Model, View, Controller)设计模式：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MVC (Model View Controler)本来是存在于Desktop程序中的，M是指数据模型，V是指用户界面，C则是控制器。使用MVC的目的是将M和V的实现代码分离，从而使同一个程序可以使用不同的表现形式。比如一批统计数据你可以分别用柱状图、饼图来表示。C存在的目的则是确保M和V的同步，一旦M改变，V应该同步更新。 1.视图&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;视图是用户看到并与之交互的界面（它可以包括一些可以显示数据信息的页面，或者展示形式。例如jsp，html，asp，php）。对老式的Web应用程序来说，视图就是由HTML元素组成的界面，在新式的Web应用程序中，HTML依旧在视图中扮演着重要的角色，但一些新的技术已层出不穷，它们包括Macromedia Flash和象XHTML，XML/XSL，WML等一些标识语言和Web services. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如何处理应用程序的界面变得越来越有挑战性。MVC一个大的好处是它能为你的应用程序处理很多不同的视图。在视图中其实没有真正的处理发生，不管这些数据是联机存储的还是一个雇员列表，作为视图来讲，它只是作为一种输出数据并允许用户操纵的方式。 2.模型&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;模型表示企业数据和业务规则（可以说就是后端接口，用于业务处理）。在MVC的三个部件中，模型拥有最多的处理任务。例如它可能用象EJBs和ColdFusion Components这样的构件对象来处理数据库。被模型返回的数据是中立的，就是说模型与数据格式无关，这样一个模型能为多个视图提供数据。由于应用于模型的代码只需写一次就可以被多个视图重用，所以减少了代码的重复性。 3.控制器&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;控制器接受用户的输入并调用模型和视图去完成用户的需求（接受客户发送的请求，根据请求调用所对应的接口，然后模型业务处理后返回的数据，由控制器决定调用那个View展示）。所以当单击Web页面中的超链接和发送HTML表单时，控制器本身不输出任何东西和做任何处理。它只是接收请求并决定调用哪个模型构件去处理请求，然后用确定用哪个视图来显示模型处理返回的数据。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;现在我们总结MVC的处理过程，首先控制器接收用户的请求，并决定应该调用哪个模型来进行处理，然后模型用业务逻辑来处理用户的请求并返回数据，最后控制器用相应的视图格式化模型返回的数据，并通过表示层呈现给用户。 Making English as your working language.]]></content>
      <categories>
        <category>程序设计</category>
      </categories>
      <tags>
        <tag>MVC</tag>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简单排序]]></title>
    <url>%2F2019%2F09%2F15%2F%E7%AE%80%E5%8D%95%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[简单排序，包括冒泡、选择、插入，他们的时间复杂度都很高，为 O(N^2) 但是相对来说 插入比选择快，选择比冒泡快，下面来看看如何操作。 1.冒泡排序&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;源代码在这里 2.选择排序&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;源代码在这里 3.插入排序&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;源代码在这里 All experience comes from mistakes.]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>冒泡排序</tag>
        <tag>选择排序</tag>
        <tag>插入排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大O表示法（时间复杂度）]]></title>
    <url>%2F2019%2F09%2F15%2F%E5%A4%A7O%E8%A1%A8%E7%A4%BA%E6%B3%95%EF%BC%88%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6%EF%BC%89%2F</url>
    <content type="text"><![CDATA[大O表示法，又名 时间复杂度，平时在对不同的数据结构进行操作时，都会有不同的时间复杂度，下面看下时间复杂度怎么计算的？ 具体来看看如何定义的： Don’t be one of the leeches.]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>时间复杂度</tag>
        <tag>大O表示法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Solidity学习]]></title>
    <url>%2F2019%2F09%2F14%2FSolidity%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[solidity 语言，作为以太坊平台智能合约语言，那必须得学一下啊。 一、入门智能合约&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;让我们先看一下最基本的例子。现在就算你都不理解也不要紧，后面我们会有更深入的讲解。 1.1 存储合约（把一个数据保存到链上）12345678910111213pragma solidity &gt;=0.4.0 &lt;0.7.0;contract SimpleStorage &#123; uint storedData; function set(uint x) public &#123; storedData = x; &#125; function get() public view returns (uint) &#123; return storedData; &#125;&#125; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第一行就是告诉编译器源代码所适用的 Solidity 版本为&gt;=0.4.0 及 &lt;0.7.0 。这是为了确保合约不会在新的编译器版本中突然行为异常。关键字 pragma 的含义是，一般来说，pragmas（编译指令）是告知编译器如何处理源代码的指令的（例如， pragma once ）。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Solidity 中合约的含义就是一组代码（它的 函数 )和数据（它的 状态 ），它们位于以太坊区块链的一个特定地址上。 代码行 uint storedData; 声明一个类型为 uint (256 位无符号整数）的状态变量，叫做 storedData 。 你可以认为它是数据库里的一个位置，可以通过调用管理数据库代码的函数进行查询和变更。对于以太坊来说，上述的合约就是拥有合约（owning contract）。在这种情况下，函数 set 和 get 可以用来变更或取出变量的值。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;要访问一个状态变量，并不需要像 this. 这样的前缀，虽然这是其他语言常见的做法。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;该合约能完成的事情并不多（由于以太坊构建的基础架构的原因）：它能允许任何人在合约中存储一个单独的数字，并且这个数字可以被世界上任何人访问，且没有可行的办法阻止你发布这个数字。当然，任何人都可以再次调用 set ，传入不同的值，覆盖你的数字，但是这个数字仍会被存储在区块链的历史记录中。随后，我们会看到怎样施加访问限制，以确保只有你才能改变这个数字。 注解 所有的标识符（合约名称，函数名称和变量名称）都只能使用 ASCII 字符集。UTF-8 编码的数据可以用字符串变量的形式存储。 警告 小心使用 Unicode 文本，因为有些字符虽然长得相像（甚至一样），但其字符码是不同的，其编码后的字符数组也会不一样。 1.2 子货币合约（Subcurrency）示例&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;下面的合约实现了一个最简单的加密货币。这里，币确实可以无中生有地产生，但是只有创建合约的人才能做到（实现一个不同的发行计划也不难）。而且，任何人都可以给其他人转币，不需要注册用户名和密码 —— 所需要的只是以太坊密钥对。 1234567891011121314151617181920212223242526272829pragma solidity &gt;=0.5.0 &lt;0.7.0;contract Coin &#123; // 关键字“public”让这些变量可以从外部读取 address public minter; mapping (address =&gt; uint) public balances; // 轻客户端可以通过事件针对变化作出高效的反应 event Sent(address from, address to, uint amount); // 这是构造函数，只有当合约创建时运行 constructor() public &#123; minter = msg.sender; &#125; function mint(address receiver, uint amount) public &#123; require(msg.sender == minter); equire(amount &lt; 1e60); balances[receiver] += amount; &#125; function send(address receiver, uint amount) public &#123; require(amount &lt;= balances[msg.sender], &quot;Insufficient balance.&quot;); balances[msg.sender] -= amount; balances[receiver] += amount; emit Sent(msg.sender, receiver, amount); &#125;&#125; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这个合约引入了一些新的概念，让我们逐一解读。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;address public minter; 这一行声明了一个可以被公开访问的 address 类型的状态变量。 address 类型是一个 160 位的值，且不允许任何算数操作。这种类型适合存储合约地址或外部人员的密钥对。关键字 public 自动生成一个函数，允许你在这个合约之外访问这个状态变量的当前值。如果没有这个关键字，其他的合约没有办法访问这个变量。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由编译器生成的函数的代码大致如下所示（暂时忽略 external 和 view）： 123function minter() external view returns (address) &#123; return minter;&#125; 当然，加一个和上面完全一样的函数是行不通的，因为我们会有同名的一个函数和一个变量，这里，主要是希望你能明白——编译器已经帮你实现了。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;下一行， mapping (address =&gt; uint) public balances; 也创建一个公共状态变量，但它是一个更复杂的数据类型。 该类型将 address 映射为无符号整数。 Mappings 可以看作是一个 哈希表 它会执行虚拟初始化，以使所有可能存在的键都映射到一个字节表示为全零的值。 但是，这种类比并不太恰当，因为它既不能获得映射的所有键的列表，也不能获得所有值的列表。 因此，要么记住你添加到 mapping 中的数据（使用列表或更高级的数据类型会更好），要么在不需要键列表或值列表的上下文中使用它，就如本例。 而由 public 关键字创建的 getter 函数 getter function 则是更复杂一些的情况， 它大致如下所示： 123function balances(address _account) external view returns (uint) &#123; return balances[_account];&#125; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;正如你所看到的，你可以通过该函数轻松地查询到账户的余额。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;event Sent(address from, address to, uint amount); 这行声明了一个所谓的 “ 事件（event）”，它会在 send 函数的最后一行被发出。用户界面（当然也包括服务器应用程序）可以监听区块链上正在发送的事件，而不会花费太多成本。一旦它被发出，监听该事件的 listener 都将收到通知。而所有的事件都包含了 from ， to 和 amount 三个参数，可方便追踪交易。 为了监听这个事件，你可以使用如下 JavaScript 代码（假设 Coin 是已经通过 web3.js 创建好的合约对象 ）： 12345678910Coin.Sent().watch(&#123;&#125;, &apos;&apos;, function(error, result) &#123; if (!error) &#123; console.log(&quot;Coin transfer: &quot; + result.args.amount + &quot; coins were sent from &quot; + result.args.from + &quot; to &quot; + result.args.to + &quot;.&quot;); console.log(&quot;Balances now:\n&quot; + &quot;Sender: &quot; + Coin.balances.call(result.args.from) + &quot;Receiver: &quot; + Coin.balances.call(result.args.to)); &#125;&#125;) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里请注意自动生成的 balances 函数是如何从用户界面调用的。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;特殊函数 constructor 是在创建合约期间运行的构造函数，不能在事后调用。 它永久存储创建合约的人的地址: msg (以及 tx 和 block ) 是一个特殊的全局变量，其中包含一些允许访问区块链的属性。 msg.sender 始终是当前（外部）函数调用的来源地址。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;最后，真正被用户或其他合约所调用的，以完成本合约功能的方法是 mint 和 send。 如果 mint 被合约创建者外的其他人调用则什么也不会发生。 另一方面， send 函数可被任何人用于向他人发送币 (当然，前提是发送者拥有这些币)。记住，如果你使用合约发送币给一个地址，当你在区块链浏览器上查看该地址时是看不到任何相关信息的。因为，实际上你发送币和更改余额的信息仅仅存储在特定合约的数据存储器中。通过使用事件，你可以非常简单地为你的新币创建一个“区块链浏览器”来追踪交易和余额。 二、投票合约&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;以下的合约有一些复杂，但展示了很多 Solidity 的语言特性。它实现了一个投票合约。 当然，电子投票的主要问题是如何将投票权分配给正确的人员以及如何防止被操纵。 我们不会在这里解决所有的问题，但至少我们会展示如何进行委托投票，同时，计票又是 自动和完全透明的 。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们的想法是为每个（投票）表决创建一份合约，为每个选项提供简称。 然后作为合约的创造者——即主席，将给予每个独立的地址以投票权。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;地址后面的人可以选择自己投票，或者委托给他们信任的人来投票。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在投票时间结束时，winningProposal() 将返回获得最多投票的提案。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;以下是代码，从代码中学习 solidity 的其它一些语法：Ballot Smart Contract &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;源代码在这里，我在学习的时候照着模板编写发现有下面这个错误，就将 address to 引用到 tempTo 就 ok 了，我觉得是变量 to 在循环中被修改了导致的。]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>智能合约</tag>
        <tag>solidity</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[程序设计]]></title>
    <url>%2F2019%2F09%2F13%2F%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;程序如何设计，达到用户使用简单，是我们需要不停探索的问题。 1.抽象&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从what（什么）中将how（如何）分离出来的过程，即类中的操作如何进行，相对什么是类用户可见的，被称为抽象。抽象是软件工程中重要的方面，把类的功能抽象出来，会使程序设计变得更简单，因为在设计的初期就考虑操作的细节。 Be an especially simple person,do not expect good luck coming suddenly,manage yourself well and cherish time at the moment.]]></content>
      <categories>
        <category>程序设计</category>
      </categories>
      <tags>
        <tag>程序设计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法]]></title>
    <url>%2F2019%2F09%2F11%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[数据结构，是指数据在计算机存储空间中（磁盘中）的安排方式，算法，是指软件程序用来操作这些数据结构中的数据的过程。 1.数组Array1.1查找&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从数据的第一位开始查找，直到找到为止，需要n/2步操作；&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果数组中数据项允许重复，则需要全部遍历一遍，需要n步操作。 1.2插入&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;插入过程是很快的，一步完成，新的数据项只需插入到数组中的第一个空位上；&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果数组不允许重复项出现，则需要进行n步查询对比操作。 1.3删除&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;删除操作有3个过程：查找、删除、移动。删除算法中暗含着一个假设，即数组中不允许有洞，洞指的是一个或几个空的数据单元，他们后面还有非空数据单元（在更高的下标下还有数据项），如果删除算法中允许有洞，那么所有其他算法都将变得复杂，因为在查看某一单元数据项时，都需要判断一下是否为空。同样算法需要找到非空数据项而变得效率低下；&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;因此，删除操作后，需要将后面非空数据项前移，来填补这个洞。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;删除需要（假设不允许重复）查找平均n/2个数据项，并平均移动剩下的n/2个数据项来填洞，总共是n步。查看删除的代码 2019-09-13更新———————————————————————— 1.4有序数组&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;有序数组的有点&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;使用有序数组会给我们带来什么好处？最主要的好处就是查找速度比无序数组快多了。不好的地方就是在插入数据时，由于所有靠后的数据都需要向后移动一位以腾开空间，导致速度比无序数组慢一些。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;有序数组和无序数组的删除操作都比较慢，因为数据项必须向前移动来填补删除数据带来的洞。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;有序数组在查找频繁的情况下非常有用，但若是插入和删除比较多的情况下，则不太适用，无法高效工作。例如，有序数组适用于公司雇员的数据库；另一方面，零售商店的货物清单不适用有序数组来实现，这是由于频繁的进货出货导致的插入删除操作都会执行地很慢。 记住一个结论:二分查找法的查询次数最大为log2n，即2对n的对数，n为数组的长度；而线性查找法的平均查询次数为n/2。当n很大时，就可以发现二分查找法的优势了，具体看下图示例： 查看二分查找法的代码 1.5对象存储使用对数组的增删改查功能，自定义对象封装数组，具体代码如下查看二分查找法的代码 Pretty looks are in a rut, while interesting souls are cream of crop,In love, looks and talents,Which do you think is pretty important?]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[比特币双花攻击]]></title>
    <url>%2F2019%2F09%2F09%2F%E6%AF%94%E7%89%B9%E5%B8%81%E5%8F%8C%E8%8A%B1%E6%94%BB%E5%87%BB%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在学习区块链的过程中，大家一定对会听到“双花”这个词，意思就是双重支付，或者更直白点就是一笔资金被花费了两次。这篇文章我们来简单的分析一下为什么会有双花，比特币是如何避免双花的。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在传统的交易中，因为有银行这样的中心化机构，所以是不会存在双花问题的：每一笔支付都将从你的银行账户中扣除相应的资金，所有的明细在银行都有记录。但是在比特币中，因为没有账户的概念，而是引入了UTXO即未花费交易输出。因为没有银行这样的中心化机构的保证，当发生一笔交易时就可能存在着双花的危险：比方说A有一个比特币，然后他同时构造两笔交易T1和T2来花费这1个比特币，其中一个给了B，从B那里买件衣服，一个给了C，从C那里买双鞋。如果不引入某种机制来避免这种情况，那作为数字货币的比特币将没有任何存在的意义。接下来就来分析一下比特币是如何做到防止这种“双花”攻击的。 (1) 正常情况&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;首先我们来看看正常情况，说白了就是绝大多数时候，区块链的共识机制就能将双花消灭在萌芽状态。我们还是以上面提到的例子来做说明： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;假设A构造了两笔交易T1和T2，将自己价值1btc的UTXO分别转给了B和C，妄图同时从B和C那里获得好处。然后A几乎在同一时间将构造好的这两笔交易广播至网络。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;假设网络中的矿工节点先收到了交易T1，发现这笔交易的资金来源确实没有被花费过，于是将T1加入到自己的内存交易池中等待打包进区块。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;大部分情况下，这个矿工节点会在不久后又收到交易T2，此时矿工会先检查A的UTXO是否足够，因为T2所指向的交易输入与已经加入交易池的T1相同，于是发现UTXO不足，矿工节点会拒绝处理该交易。网络中其他的矿工节点都类似，因此A试图双花的尝试胎死腹中。 (2) 分叉情况&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上面说的是正常的情况，但是也有非正常的情况要考虑：假设矿工节点M1和M2几乎在同一时间挖出了区块，并且很不幸M1挖到区块时只收到了交易T1，而M2挖到的区块时只收到了交易T2，这样交易T1和T2被分别打包进两个区块。因为这两个区块是差不多同一时间被挖出，于是造成了区块链的分叉： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;网络中某些节点（可能是离M1近的）先收到了M1打包的区块BLK1，于是用该区块延长自己的区块链，而另外一些节点（邻近M2的）则先收到M2打包的区块BLK2，用该区块延长自己的区块链，于是整个区块链网络中呈现出了不一致的问题： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;像这种不一致问题，一般只需要一个确认就能得到解决：假设随后又收到新区块，而新区块是以BLK1作为父区块，那么之前用BLK1延长自己区块链的节点，只需要将新区块链接到自己的区块链上，而之前以BLK2延长自己区块链的节点，则需要切换到新的最长链上，如下图：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;因此在出现分叉的情况下，通常也只需要等一个区块的确认时间网络节点中的区块链就可以重新一致，在这个例子中，经过一个区块的确认期以后，B最终确认自己收到A的1btc，而因为包含有转账给C的交易T2的区块BLK2位于备用链上，因此无法通过支付验证。A的双花尝试也以失败告终。 (3) 为什么说比特币需要6个确认才安全&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上面提到一般情况下，只要经过1个区块的确认时间基本上就能确保“相对的安全”。而在比特币中，对于很小额的支付，为了提高交易速度，一般也就是等1个区块的确认即可。但是注意这里说的是“相对安全”，对于数额特别大交易，1个区块的确认远远不够。我们考虑上面提到的分叉情况：假设经过1个区块的确认后，B知道了A给他的1btc确实已经位于链上，于是发货给A。此时A及其同伙掌握着很大的一部分算力，A通知其同伙开始使劲挖矿延长备用链（攻击链），当A最终成功的使攻击链的长度（累计工作量）超过当前主链时，会再一次导致网络中的节点切换主链的情况，如下面的示意图： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;于是包含了A转给C的交易T2的区块BLK2位于了主链之上，此时A通知C钱已到账，C做支付验证也没问题，于是C给A发货，A的双花攻击成功。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;因此对于额度稍大的交易，必须要等待6个区块的确认才能保证安全，因为攻击者要想构造攻击链追上已经经过6个区块确认的主链需要花费的算力成本是非常大的，很有可能得不偿失。 总结 (1) 大部分情况下比特币的UTXO机制和区块链的共识机制都能有效应对双花攻击； (2) 对于小额支付，等待一个确认通常就可以认为安全了，但是对于大额支付，需要等6个确认才能大概率的认为安全，否则如果攻击者掌握很强算力，有可能构造累计工作量超过当前主链的攻击链导致双花成功。 Never give up on something if you think you can fight for it. It’s difficult to wait but it’s more difficult when you regret.]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>BTC</tag>
        <tag>双花攻击</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[比特币本质其实是UTXO]]></title>
    <url>%2F2019%2F09%2F09%2F%E6%AF%94%E7%89%B9%E5%B8%81%E6%9C%AC%E8%B4%A8%E5%85%B6%E5%AE%9E%E6%98%AFUTXO%2F</url>
    <content type="text"><![CDATA[UTXO的全称为Unspent Transaction Output，翻译过来就是未被花费的交易输出。其实并没有什么比特币，我们在交易所里或者钱包里显示的比特币余额其实是UTXO。那到底什么是UTXO呢? 好像觉得还是不太理解。。。。？ 示例&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在比特币区块链账本上记录了一笔一笔的交易，每一笔交易都有若干个交易输入（转账者），也就是资金来源，同时也有若干个交易输出（收款者），也就是资金去向。每一笔交易都要花费一笔输入，产生一笔输出，而产生的这笔输出，就是UTXO。 举个简单的例子：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A地址下有1个btc，A要把1个btc转给B，则账本上交易的输入就是A，输出为B的地址，这时脚本会校验A地址是否有1个btc（余额都不够怎么会给你转），即在某一笔输出（UTXO）中查询到了A确实有1个btc。所以A可以作为输入转给B一个btc，这时就有一笔价值1个btc的输出指向B地址，直到B进行下一次转账前这笔交易都是B未被花费的输出（UTXO）。后续B要转给C时又重复A转B的操作。 总结&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;比特币并不是基于账户的方案，而是基于UTXO方案。这个和传统银行账户的思维完全不一样。张三拥有10个btc，其实就是当前区块链账本中，有若干笔交易的输出（UTXO）收款人都是张三的地址，而这些UXTO的总额为10。这个地址一共收了多少UTXO，则是要通过比特币钱包代为跟踪计算，所以钱包里显示的余额其实是有多少价值btc的输出指向你的地址。 This is your life, and you’ve got to fight for it. Fight for what’s right. Fight for what’s important to you. Fight for the people you love.]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>BTC</tag>
        <tag>UTXO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch集群环境搭建]]></title>
    <url>%2F2019%2F09%2F06%2FElasticSearch%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[ElasticSearch作为一个基于搜索引擎lucene的文档数据库，搜索速度在目前的大数据存储系统中，算是佼佼者了。又有许多人把它当数据库使用，索引做库，类型做表，也是不错的选择。在数据仓库中，也可以做report层的存储，对接数据可视化工具，提供接口查询业务数据、结果数据。下面来看看集群环境怎么搭建。 1、软件需求jdk8elasticsearch包 2、es安装、配置123456789tar -zxvf elasticsearch-*.tar.gz -C /usr/localcd /usr/local/elasticsearchvim /config/elasticsearch.ymlcluster.name: es-cluster-1 配置集群名称 三台服务器保持一致node.name: node-1 配置单一节点名称，每个节点唯一标识network.host: 0.0.0.0 设置绑定的ip地址http.port: 9200 端口discovery.zen.ping.unicast.hosts: [&quot;192.168.21.12&quot;, &quot;192.168.21.13&quot;,&quot;192.168.21.14&quot;] 集群节点ip或者主机discovery.zen.minimum_master_nodes: 3 设置这个参数来保证集群中的节点可以知道其它N个有master资格的节点。默认为1，对于大的集群来说，可以设置大一点的值（2-4） 2.1新建用户（三台都需要）在 Linux 环境中，elasticsearch 不允许以 root 权限来运行！所以需要创建一个非root用户，以非root用户来起eselsearch 新增elsearch用户组 12useradd elsearch -g elsearch -p elasticsearch 创建elsearch用户chown -R elsearch:elsearch ./elasticsearch 用户目录权限 2.2运行操作三台服务12su elsearch./bin/elasticsearch -d //-d表示后台启动 3、问题汇总解决方案3.1问题一12ERROR: bootstrap checks failedmax file descriptors [4096] for elasticsearch process likely too low, increase to at least [65536] 原因：无法创建本地文件问题,用户最大可创建文件数太小解决方案：切换到root用户，编辑limits.conf配置文件， 添加类似如下内容： 1vi /etc/security/limits.conf 添加如下内容: 1234* soft nofile 65536* hard nofile 131072* soft nproc 2048* hard nproc 4096 备注：* 代表Linux所有用户名称（比如 hadoop）保存、退出、重新登录才可生效 3.2问题二1max virtual memory areas vm.max_map_count [65530] likely too low, increase to at least [262144] 原因：最大虚拟内存太小解决方案：切换到root用户下，修改配置文件sysctl.conf 1vi /etc/sysctl.conf 添加下面配置： 1vm.max_map_count=655360 并执行命令：sysctl -p然后重新启动elasticsearch，即可启动成功。 3.3问题三：1max virtual memory areas vm.max_map_count [65530] likely too low, increase to at least [262144] 原因：最大虚拟内存太小解决方案：切换到root用户下，修改配置文件sysctl.confvi /etc/sysctl.conf添加下面配置： 1vm.max_map_count=655360 并执行命令：sysctl -p然后重新启动elasticsearch，即可启动成功。 3.4问题四：ElasticSearch启动找不到主机或路由原因：ElasticSearch 单播配置有问题解决方案：检查ElasticSearch中的配置文件vi config/elasticsearch.yml找到如下配置：discovery.zen.ping.unicast.hosts: [“192.168.21.12”, “192.168.21.13”,”192.168.21.14”]一般情况下，是这里配置有问题，注意书写格式 3.5问题五：org.elasticsearch.transport.RemoteTransportException: Failed to deserialize exception response from stream原因:ElasticSearch节点之间的jdk版本不一致解决方案：ElasticSearch集群统一jdk环境 3.6问题六：Unsupported major.minor version 52.0原因：jdk版本问题太低解决方案：更换jdk版本，ElasticSearch5.0.0支持jdk1.8.0]]></content>
      <categories>
        <category>环境搭建</category>
      </categories>
      <tags>
        <tag>ElasticSearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[区块链三大共识机制]]></title>
    <url>%2F2019%2F09%2F03%2F%E5%8C%BA%E5%9D%97%E9%93%BE%E4%B8%89%E5%A4%A7%E5%85%B1%E8%AF%86%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[区块链中三大共识机制POW、POS、DPOS的区别，进来看看： 三大共识机制POW、POS、DPOS的区别 2019-09-08更新 1.PoW算法1.1 PoW历史工作量证明源于经济学，是一个经济学的概念，是指为了达成某种目标而设定一个度量的方法。可以和平时工作中的绩效考核做类比：为了考核达到5星，你必须要完成leader指派给你的KPI，这些KPI就是工作量证明，完成它可能需要加班加点干很久，但是考评的时候leader验证起来却很容易。 1993年，Cynthia Dwork和Moni Naor在学术论文中首次提出工作量证明的的概念。 1997年，Adam Back在学术论文Hashcash中提出了Hahscash的概念，用于抵抗Dos攻击和垃圾邮件网关的滥用。Hahscash和比特币区块链的PoW如出一辙，在比特币之前，Hashcash的最常见应用场景就是反垃圾邮件。 1999年，Markus Jakobsson和Ari Juels发表的论文中首次提出了Proof of Work这个名词。 PoW需要两个角色：工作者和验证者，并且具备以下特点： (1) 工作只能由验证者发布；(2) 工作者必须要完成一定量的工作，具体的工作量有验证者给出；(3) 工作者没有办法快速完成工作，要完成工作必须消耗一定代价，但是验证者验证起来很容易； 1.2 HashcashHashcash是Adam Back与1997年发明的一种抵抗邮件服务DoS攻击的算法。理解了Hashcash，就能够轻而易举的理解比特币中的PoW。 Hashcash是基于SHA1散列算法，它抵抗垃圾邮件的原理如下： 假设S给R发送邮件，要想发送成功，则： (1) S的邮件头中需要带一个称之为hashcash stamp的戳记；(2) 对hashcash stamp进行SHA1后的值必须满足接收方R设定的条件：生成的hash值的前20位必须为0；(3) hashcash stamp可能由多个域组成，比如生成邮件的时间，收件人地址等不变量，还包含可变量counter；(4) 由于Hash的特点，导致发送方S没有办法快速找到满足条件的hashcash stamp，只能通过不断递增counter的值来穷举；(5) 发送方S通过暴力破解的方式计算出满足接收方条件的值，这个过程发送方消耗了一定量的CPU。而验证方只需要对收到的hashcash stamp进行SHA1，检查结果是否满足； 由于发送方计算满足邮件接收方条件的值需要消耗一定时间，对于垃圾邮件系统来说，这样的成本基本上是不可接受的，从而有效避免了垃圾邮件。 1.3 PoW如何解决分布式系统的共识问题可以把比特币区块链看成一个分布式的账本，然后定义几个名词： (1) 分布式账本：比特币区块链是一个分布式账本，这个账本人人可以随时查阅，但是由谁来记账需要通过共识算法来决定；(2) 记账：将区块写入区块链；(3) 记账者：就是平常所说的矿工，比特币系统中的每个矿工节点都是潜在的记账者；(4) 选举：就是大家所熟知的挖矿，用某种算法从一批候选矿工（记账者）中选出一个来记账（写入账本）；记账者中大部分都是正常的，但是也有少数不怀好意的记账者，因此系统需要通过共识算法选出一个正常的记账者来记账，这其实就是一个拜占庭将军问题：在可能存在恶意节点时，如何保证账本的正确性。 既然是拜占庭问题，那么像PBFT，PoW等算法都可以解决，比特币使用PoW，其达成共识的过程如下： (1) 系统指定一个目标hash值，各个矿工节点竞争，构造出候选区块，并不断计算区块头hash，直到得到满足条件的hash为止；(2) 与hashcash一样，矿工节点除了暴力破解，没有快速找到答案的办法；(3) 率先计算出解的矿工节点将区块广播给全网，其他节点验证区块（工作量是否满足，交易是否合法），没有问题就将区块加入到区块链中；(4) 计算出解的矿工得到一笔奖励费；很明显，这是一个凭算力取胜的游戏，简单分析一下：假设系统有4个矿工节点A，B，C，D，则：(1) 如果4个矿工节点的算力都相同，其中存在一个恶意节点，那么最终能正确写入账本的概率是3/4，而被恶意矿工写入的概率是1/4；(2) 假如B是恶意节点，他买了一台奇快的专用矿机，使得他的算力大增，达到了全部矿工节点算力的51%，那么系统基本上就被这个恶意矿工控制；因此，系统最坏的情况下能容忍的问题节点的数量是：占据整个系统51%算力的问题节点的数量，当然要掌控这么高的算力还是很困难的。关于具体的源码，后续会单独用一篇文章来分析比特币挖矿的过程，本文先理解PoW算法就好。 2.PoS算法2.1 PoS的提出PoW算法存在两个问题： (1) 太浪费资源，因为需要巨量的计算，会浪费资源（电力）；(2) 存在51%攻击问题：一旦能掌握系统51%的算力基本上就能控制整个系统。(3) 系统的吞吐量降低，比如比特币，平均每10分钟才产生一个区块；为了克服PoW算法的问题，2012年Sunny King提出了PoS权益证明算法，并发布了点点币（PPCoin），点点币中采用了PoS作为共识算法。PoS与PoW原理上很相似，都是一种基于概率的解决共识问题的算法。只不过PoW是拼算力，算力越强的抢到记账权的概率越大，而PoS则是拼财力，谁的财力越高，抢到记账权的概率就越大。 2.2 PoS算法的原理PoS一个重要的概念是币龄，币龄 = 持有的币数 * 持有币的天数，例如钱包里有90个点点币，都持有了10天，则币龄=900；与PoW一样，为了抢到将区块写入区块链的权利，节点同样要进行hash计算，只不过最终的解和币龄有关，计算公式：proofHash &lt; coinAge * target；coinAge是币龄，target是一个目标值，用于调整难度。coinAge * target的值越大，难度就越小，抢到区块的概率就越高。假如你的钱包里是0个币，那么你的币龄就是0, 计算一个小于0的hash值的概率基本上也0，因此基本上抢不到区块；PoS可以解决PoW的问题： (1) 首先，不需要PoW那么大的算力，可以减少资源浪费；(2) 不容易遭受51%攻击，相比起掌握系统一半以上的算力，拥有整个系统51%的财力会更加困难。但是PoS也存在明显的缺陷：(1) 容易被垄断：因为持币越多，持有的越久，币龄就越高，越容易挖到区块并得到激励，持币少的人基本上没有机会，这样整个系统的安全性实际上会被持币数量较大的一部分人（大股东们）掌握；而比特币则不存在这个问题，因为理论上任何人都可以购买矿机获得提高自己的算力（甚至可以联合起来），提升自己挖矿成功的概率；(2) 很难应对分叉的情况：当出现分叉时，PoS可以在两条链上同时挖矿并获得收益。而PoW则不存在这个问题，因为当出现分叉以后，PoW总是选择工作量大的链做为主链。 3.DPoS算法PoS算法中记账权很容易被持币较多的人垄断，从而容易趋于中心化（永远是持币多的那些人获得写入区块并获得奖励），于是又有了DPoS（委托权益证明）算法。与PoS算法相比，DPoS中多了受托人的概念。DPoS算法是在2014年由Bitshares的首席开发者Dan Larimer提出，此人现为EOS的CTO。DPoS算法的原理如下： (1) 区块由受托人产生并写入区块链；(2) 受托人由持币人选举产生；(3) 根据受托人所得的票数排名，选取排名最靠前的若干（一般为101位）作为记账节点，来生成区块并写入区块链；(4) 被选中的受托人会隔一定的周期进行一次调整；首先，为了利益最大化，股东会选择将票投给那些信誉好，可靠性高的节点；其次，受托人每隔一定的周期就会重新投票进行调整，更加民主和公平。这就好比公司选举领导层干部，为了公司利益最大化，股东一般会选择能力强的人进入管理层。而一旦管理层出了问题领导，股东可以把他撸下去，重新投票选择更胜任的人。 4.总结本文介绍了PoW、PoS两类基于概率的共识算法，任何一个区块链都必须要解决区块何时产生，由谁将区块写入区块链的问题，PoW和PoS解决的思路总结起来就是： (1) PoW：比拼算力，算力越强越容易拿到写区块链的权利；(2) PoS：比拼财力，占的股份越大（币龄越高），越容易拿到记账权；(3) DPoS：引入了受托人，由投票选举出的若干信誉度更高的受托人记账，受托人每隔一定周期调整。这些算法在许多的区块链中被广泛使用，这些算法是区块链安全的基石，学习中需要结合项目源码加以理解，弄清楚区块链到底在共识什么，为什么需要共识，共识算法是如何解决这些问题的。以太坊将在比特币创世块挖出的那一天2020年1月3日升级ETH1.0为ETH2.0，由POW共识改为POS共识，期待他的表现。 你需要做的就是坚持。]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>共识机制</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka集群搭建]]></title>
    <url>%2F2019%2F08%2F31%2Fkafka%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[kafka，领英开源消息队列框架，在大数据实时、批处理过程中，充当缓冲、中间存储、解耦的组件，深得工程师的喜爱，下面看下如何搭建。 1、软件环境搭建好的zookeeper集群下载kafka安装包 2、安装、修改配置文件12tar -zxvf kafka_*.tgz -C /usr/local/cd /usr/local/kafka/config 2.1修改配置文件12345678910111213vim server.propertiesbroker.id=1/* 这是这台虚拟机上的值，在另外两台虚拟机上应该是2或者3，这个值是唯一的，每台虚拟机或者叫服务器不能相同。 // 设置本机IP和端口。 我这里设置的是listeners，也可以直接设置host.name=192.168.172.10,port=9092,这个IP地址也是与本机相关的，每台服务器上设置为自己的IP地址。 /listeners=PLAINTEXT://192.168.172.10:9092advertised.listeners=PLAINTEXT://x.x.x.x:9092 //外部访问// 该端口默认是9092// 在og.retention.hours=168下面新增下面三项 #默认消息的最大持久化时间，168小时，7天message.max.byte=5242880 #消息保存的最大值5M default.replication.factor=2 #kafka保存消息的副本数，如果一个副本失效了，另一个还可以继续提供服务replica.fetch.max.bytes=5242880 #取消息的最大直接数/ 设置zookeeper的连接端口，新版本的kafka不再使用zookeeper而是通过brokerlist的配置让producer直接连接broker，这个brokerlist可以配置多个，只要有一个能连接上，就可以让producer获取道集群中的其他broker的信息，绕过了zookeeper。因此这个zookeeper.connect可以设置多个值 */zookeeper.connect=192.168.172.12:2181,192.168.172.11:2181,192.168.172.10:2181 3、启动kafka首先要启动kafka集群，并且是三台都要手动去启动。// 进入kafka的bin目录cd /opt/kafka/kafka_2.11-1.0.0/bin/// 启动kafka./kafka-server-start.sh -daemon ../config/server.properties &amp; //-daemon代表着以后台模式运行kafka集群，这样kafka的启动就不会影响我们继续在控制台输入命令。//查看服务是否正常jps 4、创建topic，测试4.1创建topic创建Topic1./kafka-topics.sh --create --zookeeper 10.0.0.60:2181 --replication-factor 2 --partitions 1 --topic shuaige // 解释 123--replication-factor 2 #复制两份--partitions 1 #创建1个分区--topic #主题为shuaige ‘’’在一台服务器上创建一个发布者’’’ 创建一个broker，发布者123./kafka-console-producer.sh --broker-list 10.0.0.60:9092 --topic shuaige&apos;&apos;&apos;在一台服务器上创建一个订阅者&apos;&apos;&apos;./kafka-console-consumer.sh --zookeeper localhost:2181 --topic shuaige --from-beginning 4.2 查看topic1./kafka-topics.sh --list --zookeeper localhost:2181 4.3 查看topic状态1/kafka-topics.sh --describe --zookeeper localhost:12181 --topic shuaige 下面是显示信息Topic:ssports PartitionCount:1 ReplicationFactor:2 Configs: Topic: shuaige Partition: 0 Leader: 1 Replicas: 0,1 Isr: 1//分区为为1 复制因子为2 他的 shuaige的分区为0//Replicas: 0,1 复制的为0，1 4.4 创建kafka topic1bin/kafka-topics.sh --zookeeper node01:2181 --create --topic t_cdr --partitions 30 --replication-factor 2 注： partitions指定topic分区数，replication-factor指定topic每个分区的副本数 partitions分区数: partitions ：分区数，控制topic将分片成多少个log。可以显示指定，如果不指定则会使用broker(server.properties)中的num.partitions配置的数量 虽然增加分区数可以提供kafka集群的吞吐量、但是过多的分区数或者或是单台服务器上的分区数过多，会增加不可用及延迟的风险。因为多的分区数，意味着需要打开更多的文件句柄、增加点到点的延时、增加客户端的内存消耗。 分区数也限制了consumer的并行度，即限制了并行consumer消息的线程数不能大于分区数 分区数也限制了producer发送消息是指定的分区。如创建topic时分区设置为1，producer发送消息时通过自定义的分区方法指定分区为2或以上的数都会出错的；这种情况可以通过alter –partitions 来增加分区数。 replication-factor副本 replication factor 控制消息保存在几个broker(服务器)上，一般情况下等于broker的个数。 如果没有在创建时显示指定或通过API向一个不存在的topic生产消息时会使用broker(server.properties)中的default.replication.factor配置的数量查看所有topic列表1bin/kafka-topics.sh --zookeeper node01:2181 --list 查看指定topic信息 1bin/kafka-topics.sh --zookeeper node01:2181 --describe --topic t_cdr 控制台向topic生产数据 1bin/kafka-console-producer.sh --broker-list node86:9092 --topic t_cdr 控制台消费topic的数据 1bin/kafka-console-consumer.sh --zookeeper node01:2181 --topic t_cdr --from-beginning 查看topic某分区偏移量最大（小）值 1bin/kafka-run-class.sh kafka.tools.GetOffsetShell --topic hive-mdatabase-hostsltable --time -1 --broker-list node86:9092 --partitions 0 注： time为-1时表示最大值，time为-2时表示最小值增加topic分区数, 为topic t_cdr 增加10个分区 1bin/kafka-topics.sh --zookeeper node01:2181 --alter --topic t_cdr --partitions 10 删除topic，慎用，只会删除zookeeper中的元数据，消息文件须手动删除 1bin/kafka-run-class.sh kafka.admin.DeleteTopicCommand --zookeeper node01:2181 --topic t_cdr 查看topic消费进度这个会显示出consumer group的offset情况， 必须参数为–group， 不指定–topic，默认为所有topic 1234567891011121314151617Displays the: Consumer Group, Topic, Partitions, Offset, logSize, Lag, Owner for the specified set of Topics and Consumer Groupbin/kafka-run-class.sh kafka.tools.ConsumerOffsetCheckerrequired argument: [group]Option Description------ -------------broker-info Print broker info--group Consumer group.--help Print this message.--topic Comma-separated list of consumertopics (all topics if absent).--zkconnect ZooKeeper connect string. (default: localhost:2181)Example,bin/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker --group pvGroup Topic Pid Offset logSize Lag Ownerpv page_visits 0 21 21 0 nonepv page_visits 1 19 19 0 nonepv page_visits 2 20 20 0 none 以上图中参数含义解释如下： 123456topic：创建时topic名称pid：分区编号offset：表示该parition已经消费了多少条messagelogSize：表示该partition已经写了多少条messageLag：表示有多少条message没有被消费。Owner：表示消费者 细看kafka-run-class.sh脚本，它是调用 了ConsumerOffsetChecker的main方法，所以，我们也可以通过java代码来访问scala的ConsumerOffsetChecker类，代码如下： 12345678910111213import kafka.tools.ConsumerOffsetChecker;/** * kafka自带很多工具类，其中ConsumerOffsetChecker能查看到消费者消费的情况, * ConsumerOffsetChecker只是将信息打印到标准的输出流中 * */public class RunClass &#123; public static void main(String[] args) &#123; //group-1是消费者的group名称,可以在zk中 String[] arr = new String[]&#123;&quot;--zookeeper=192.168.199.129:2181,192.168.199.130:2181,192.168.199.131:2181/kafka&quot;,&quot;--group=group-1&quot;&#125;; ConsumerOffsetChecker.main(arr); &#125;&#125; 5、日志说明默认kafka的日志是保存在/opt/kafka/kafka_2.10-0.9.0.0/logs目录下的，这里说几个需要注意的日志server.log #kafka的运行日志state-change.log #kafka他是用zookeeper来保存状态，所以他可能会进行切换，切换的日志就保存在这里controller.log #kafka选择一个节点作为“controller”,当发现有节点down掉的时候它负责在游泳分区的所有节点中选择新的leader,这使得Kafka可以批量的高效的管理所有分区节点的主从关系。如果controller down掉了，活着的节点中的一个会备切换为新的controller. 6、 Kafka-manager（v2.0.0.2）（Kafka集群可视化管理）6.1 sbt编译123curl https://bintray.com/sbt/rpm/rpm &gt; bintray-sbt-rpm.repomv bintray-sbt-rpm.repo /etc/yum.repos.d/yum install sbt 检查sbt是否安装成功 1sbt version 6.2 安装部署kafka-manager12wget https://github.com/yahoo/kafka-manager/releases/kafka-manager-2.0.0.2.tar.gztar zxvf kafka-manager-2.0.0.2.tar.gz -C /usr/local 编译kafka-manager 12cd /usr/local/kafka-manager-2.0.0.2./sbt clean dist //需要一段时间 编译结果查看 1ls /usr/local/kafka-manager-2.0.0.2/target/universal/ //存在kafka-manager-2.0.0.2.zip 创建目录kafka-manager12mkdir -p /usr/local/kafka-managercp /usr/local/kafka-manager-2.0.0.2/target/universal/kafka-manager-2.0.0.2.zip /usr/local/kafka-manager 解压文件 1unzip kafka-manager-2.0.0.2.zip 修改配置文件 1vim /usr/local/kafka-manager/kafka-manager-2.0.0.2/conf/application.conf 修改信息 1234单机：kafka-manager.zkhosts=&quot;localhost:2181&quot;集群：kafka-manager.zkhosts=&quot;node3.cn:2181,node4.cn:2181,node5.cn:2181&quot; 6.3启动kafka-manager控制台启动 1bin/kafka-manager 后台守护启动 1nohup bin/kafka-manager &amp; 后台启动通过 -Dhttp.port，指定端口; -Dconfig.file=conf/application.conf指定配置文件 1nohup bin/kafka-manager -Dconfig.file=conf/application.conf -Dhttp.port=8080 &amp;]]></content>
      <categories>
        <category>环境搭建</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建zookeeper集群]]></title>
    <url>%2F2019%2F08%2F31%2F%E6%90%AD%E5%BB%BAzookeeper%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[zookeeper作为一个hadoop生态组件的连接器，在节点服务之间的通信及元数据管理上起着非常重要的作用，下面看看搭建步骤。 1、软件环境Linux服务器。使用数量为一台，三台，五台，（2*n+1）。zookeeper集群的工作是超过半数才能对外提供服务，三台中超过两台超过半数，允许一台挂掉。最好不要使用偶数台。例如：如果有4台，那么挂掉一台还剩下三台，如果再挂掉一台就不能行了，因为是要超过半数。 Java jdk1.8 zookeeper包 2、配置与安装zookeeper配置文件zoo.cfg12345678910111213141516tar -zxvf zookeeper-*.tar.gz -C /usr/localcd /usr/local/zookeeper/confcp zoo_sample.cfg zoo.cfgvim zoo.cfgtickTime=2000initLimit=10syncLimit=5dataDir=/usr/local/zookeeper/zkdatadataLogDir=/usr/local/zookeeper/zkdatalogclientPort=2181// 此处的IP就是你所操作的三台虚拟机的IP地址，每台虚拟机的zoo.cfg中都需要填入这三个地址。第一个端口是master和slave之间的通信端口，默认是2888，第二个端口是leader选举的端口，集群刚启动的时候选举或者leader挂掉之后进行新的选举的端口默认是3888server.1=192.168.172.10:2888:3888server.2=192.168.172.11:2888:3888server.3=192.168.172.12:2888:3888// server.1 这个1是服务器的标识也可以是其他的数字， 表示这个是第几号服务器，用来标识服务器，这个标识要写到快照目录下面myid文件里创建myid文件。以现在所在的第一台虚拟机192.168.172.10为例，对应server.1，通过上边的配置信息可以查到。创建myid文件的目的是为了让zookeeper知道自己在哪台服务器上，例如现在所在的虚拟机是192.168.172.10，它对应的id是1，那么就在myid文件中写入1. 节点id配置1234echo &quot;1&quot; &gt; /usr/local/zookeeper/zkdata/myid另外两台虚拟机上也需要创建myid文件并写入相应的id，id根据zoo.cfg文件中的IP地址查询。echo &quot;2&quot; &gt; /usr/local/zookeeper/zkdata/myidecho &quot;3&quot; &gt; /usr/local/zookeeper/zkdata/myid 3、启动zookeeper12345cd /usr/local/zookeeper/bin// 启动服务 (注意！三台虚拟机都要进行该操作)./zkServer.sh start// 检查服务器状态./zkServer.sh status // 显示如下JMX enabled by defaultUsing config: /opt/zookeeper/zookeeper-3.4.6/bin/../conf/zoo.cfgMode: follower #他是主节点leader还是从节点follower 4、补充说明1. myid文件和server.myid在快照目录下存放的标识本台服务器的文件，他是整个zk集群用来发现彼此的一个重要标识，myid必须与zoo.cfg配置中的 server.? 一致。 2. zoo.cfg配置文件zoo.cfg文件是zookeeper配置文件，在conf目录里。 123456789101112// tickTime：这个时间是作为 Zookeeper 服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个 tickTime 时间就会发送一个心跳。// initLimit：这个配置项是用来配置 Zookeeper 接受客户端（这里所说的客户端不是用户连接 Zookeeper 服务器的客户端，而是 Zookeeper 服务器集群中连接到 Leader 的 Follower 服务器）初始化连接时最长能忍受多少个心跳时间间隔数。当已经超过 5个心跳的时间（也就是 tickTime）长度后 Zookeeper 服务器还没有收到客户端的返回信息，那么表明这个客户端连接失败。总的时间长度就是 52000=10 秒// syncLimit：这个配置项标识 Leader 与Follower 之间发送消息，请求和应答时间长度，最长不能超过多少个 tickTime 的时间长度，总的时间长度就是52000=10秒// dataDir：快照日志的存储路径// dataLogDir：事物日志的存储路径，如果不配置这个那么事物日志会默认存储到dataDir制定的目录，这样会严重影响zk的性能，当zk吞吐量较大的时候，产生的事物日志、快照日志太多// clientPort：这个端口就是客户端连接 Zookeeper 服务器的端口，Zookeeper 会监听这个端口，接受客户端的访问请求。修改他的端口改大点]]></content>
      <categories>
        <category>环境搭建</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink学习]]></title>
    <url>%2F2019%2F08%2F26%2FFlink%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[flink，号称第二代实时大数据计算引擎，被他的名头吸引过来，我也来学习学习，下面是我在学习过程中遇到的一些问题和解决方案。 如何保证数据处理的有序性&nbsp;&nbsp;&nbsp;&nbsp;flink通过watermark来解决这个问题。当使用事件时间来进行对事件排序时，很有必要跟踪事件的处理时间，例如在一个窗口操作t到t+5中，只有当系统能够保证没有数据的事件时间小于t+5时，然后对这个窗口中的数据进行排序计算，才是保证数据处理的有序性，那么如何确定没有数据的事件时间小于t+5呢？flink是使用watermark来确定的，它会追踪穿过系统中的每一条数据，当它知道没有数据对应的时间戳小于t1后，它会将这个t1水印广播📢到下流operators，一旦watermark被提交，下流operators在获取watermark值时就会发现并作出相应的反应。&nbsp;&nbsp;&nbsp;&nbsp;在窗口操作中，窗口会等待t+5的watermark，然后触发计算，并向下游广播t+5的watermark。&nbsp;&nbsp;&nbsp;&nbsp;当所有的operater都在等待他的watermark和输入数据时，系统会被延时，从而影响时效性。&nbsp;&nbsp;&nbsp;&nbsp;下图是实际水印、事件时间与处理时间之间的关系： you can see the original document at this link Time and Order in Streams 学习使我快乐。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[重温《头号玩家》VR游戏中的区块链世界]]></title>
    <url>%2F2019%2F08%2F19%2F%E9%87%8D%E6%B8%A9%E3%80%8A%E5%A4%B4%E5%8F%B7%E7%8E%A9%E5%AE%B6%E3%80%8BVR%E6%B8%B8%E6%88%8F%E4%B8%AD%E7%9A%84%E5%8C%BA%E5%9D%97%E9%93%BE%E4%B8%96%E7%95%8C%2F</url>
    <content type="text"><![CDATA[2018年上映的电影《头号玩家》，得到许多年轻人的喜爱，故事情节紧凑而又丰富，其中包含许多区块链世界的元素，咱们一起探讨下吧。 2019-08-25更新———————————— 游戏中通用的金币、不同的游乐场景、统一的入口等等，貌似现在的区块链世界。比特币、以太坊、比特现金，谁会是区块链游戏中的通用货币呢？每个玩家都有自己的唯一私钥，可以访问不同区域不同游戏，音乐、电影、游戏、运动等等，应有尽有，其中私钥就是区块链世界的入口。而且金币可以买现实世界中的物品，所以金币属于虚拟资产，类似现在的Q币、游戏币等等。你们觉得这种游戏会在现实中出现吗？我觉得可能，中心化的游戏，为并发和性能提供支持，区块链实现资产的去中心化，不受别人控制。所有如何实现centralized app与decentralized app之间的交互呢？]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>头号玩家</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[以太坊]]></title>
    <url>%2F2019%2F08%2F18%2F%E4%BB%A5%E5%A4%AA%E5%9D%8A%2F</url>
    <content type="text"><![CDATA[以太坊 Do you like it? 一、layer2扩展解决方案 侧链loom network plasma 2019-08-25更新———————————— 二、Ethereum2.0研发计划阶段0:信标链&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;主要负责管理权益证明协议的运行，并协调所有独立的平行分片，他是整个开发中最复杂的部分，详情请看信标链 阶段1:分片链&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;主要实现将验证者分散在1024条分片链上，点对点网络以足够快的速度与验证者之间准确无误的进行通信。 阶段2:执行层&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;提供巨大的设计空间和无拘无束的开发氛围，提供一些不同的执行环境，例如代币转账执行环境（匿名），智能合约语言执行环境，为处理高容量Plasma侧链而优化的执行环境，以及为企业用户量身打造的执行环境，具备许可性和隐私性。 2019-09-08更新 三、账户&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;以太坊中有2类账户，他们共用同一个地址空间。 外部账户&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;该类账户被公钥-私钥对（人类）控制 合约账户&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;该类账户被存储在合约中的代码控制 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;另外，每个账户都有一个以太币余额，单位是Wei，该账户余额可以向它发送带有金额交易的方式来改变。 四、以太坊虚拟机&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;以太坊虚拟机EVM，是以太坊中智能合约的运行环境。它不仅被沙箱隔离起来，实际上完全隔离，也就是说运行在虚拟机中的代码，不能接触到网络、文件系统、或其他进程。甚至智能合约和其他智能合约只能有有限的接触。 五、Gas&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;以太坊上的每笔交易都需要消耗gas，目的是限制执行交易所需的工作量，同时为执行支付费用。当EVM执行交易时，gas将按照特定规则被逐渐消耗。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;gas price（gas价格，以太币计）是由交易创建者设定的，发送账户需要预付的交易费用=gas price * gas amount，如果执行结束gas还有剩余，那么这些gas将返回给发送账户。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;无论交易被执行到什么位置，一旦gas被耗尽（比如降为负值），将会触发一个out of gas异常，当前调用帧所做的所有状态修改都将被回滚。 2019-09-13更新———————————————————————— 六、存储、内存和栈6.1 存储&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;每个账户有一块持久化内存区叫做存储。存储是将256位字映射到256位字的键值存储区，在合约中枚举存储是不可能的，且读存储的开销很高，修改存储的开销甚至更高。合约只能读写存储区内属于自己的部分。 6.2 内存&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第二个内存区称为内存，合约会试图为每一次消息调用获取一块被重新擦拭干净的内存示例。内存是线性的，可以按照字节寻址，但读的长度被限制在256位，而写的长度可以是8位或256位。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当访问（无论是读还是写）之前从未访问过的内存字（word）时（无论是偏移到该字内的任何位置），内存将按字进行扩展（每个字是256位）。扩容也将消耗一定的gas。 随着内存使用量的增长，其费用也会增高（以平方级别）。 6.3 栈&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;EVM不是基于寄存器的，而是基于栈的，因此所有的计算都在一个称为 栈。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;栈最大有1024个元素，每个元素的长度为一个字（256位），对栈的访问仅限于顶端，限制方式为允许拷贝最顶端的16个元素中的一个到栈顶，或者是栈顶元素和下面的16个元素中的一个进行交换。所有其他操作都只能取最顶的2个（或1个，或更多，取决于具体的操作）元素，运算后，把结果压入栈顶。当然可以把栈上的元素放到存储或内存中。但是无法只访问栈上指定深度的那个元素，除非先从栈顶移除其他元素。 七、指令集&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;EVM的指令集量应尽量少，以最大限度地避免可能导致共识问题的错误实现。所有的指令都是针对”256位的字（word）”这个基本的数据类型来进行操作。具备常用的算术、位、逻辑和比较操作。也可以做到有条件和无条件跳转。此外，合约可以访问当前区块的相关属性，比如它的编号和时间戳。 八、消息调用&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;合约可以通过消息调用的方式来调用其它合约或者发送以太币到非合约账户。消息调用和交易非常类似，它们都有一个源、目标、数据、以太币、gas和返回数据。事实上每个交易都由一个顶层消息调用组成，这个消息调用又可创建更多的消息调用。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;合约可以决定在其内部的消息调用中，对于剩余的 gas ，应发送和保留多少。如果在内部消息调用时发生了out-of-gas异常（或其他任何异常），这将由一个被压入栈顶的错误值所指明。此时，只有与该内部消息调用一起发送的gas会被消耗掉。并且，Solidity中，发起调用的合约默认会触发一个手工的异常，以便异常可以从调用栈里“冒泡出来”。 如前文所述，被调用的合约（可以和调用者是同一个合约）会获得一块刚刚清空过的内存，并可以访问调用的payload——由被称为 calldata 的独立区域所提供的数据。调用执行结束后，返回数据将被存放在调用方预先分配好的一块内存中。 调用深度被 限制 为 1024 ，因此对于更加复杂的操作，我们应使用循环而不是递归。 九、委托调用/代码调用和库&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;有一种特殊类型的消息调用，被称为 委托调用(delegatecall) 。它和一般的消息调用的区别在于，目标地址的代码将在发起调用的合约的上下文中执行，并且 msg.sender 和 msg.value 不变。 这意味着一个合约可以在运行时从另外一个地址动态加载代码。存储、当前地址和余额都指向发起调用的合约，只有代码是从被调用地址获取的。 这使得 Solidity 可以实现”库“能力：可复用的代码库可以放在一个合约的存储上，如用来实现复杂的数据结构的库。 十、日志&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;有一种特殊的可索引的数据结构，其存储的数据可以一路映射直到区块层级。这个特性被称为 日志(logs) ，Solidity用它来实现 事件(events) 。合约创建之后就无法访问日志数据，但是这些数据可以从区块链外高效的访问。因为部分日志数据被存储在 布隆过滤器(Bloom filter) 中，我们可以高效并且加密安全地搜索日志，所以那些没有下载整个区块链的网络节点（轻客户端）也可以找到这些日志。 十一、合约创建&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;合约甚至可以通过一个特殊的指令来创建其他合约（不是简单的调用零地址）。创建合约的调用 create calls 和普通消息调用的唯一区别在于，负载会被执行，执行的结果被存储为合约代码，调用者/创建者在栈上得到新合约的地址。 十二、失效和自毁&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;合约代码从区块链上移除的唯一方式是合约在合约地址上执行自毁操作 selfdestruct 。合约账户上剩余的以太币会发送给指定的目标，然后其存储和代码从状态中被移除。移除一个合约听上去不错，但其实有潜在的危险，如果有人发送以太币到移除的合约，这些以太币将永远提丢失。 *注释 尽管一个合约的代码中没有显式地调用 selfdestruct ，它仍然有可能通过 delegatecall 或 callcode 执行自毁操作。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果要使合同失效，则应通过更改内部状态来禁用合约，这样可以在使用函数无法执行从而进行 revert，从而达到返还以太的目的。 *注释 旧合约的删减可能会，也可能不会被以太坊的各种客户端程序实现。另外，归档节点可选择无限期保留合约存储和代码。目前，外部账户 不能从状态中移除。 当你累了的时候，停下来做个梦吧。愿你坚持到底。]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>以太坊</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[拜占庭问题]]></title>
    <url>%2F2019%2F08%2F18%2F%E6%8B%9C%E5%8D%A0%E5%BA%AD%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[拜占庭问题，即去中心化网络的一致性问题。 一、问题&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;拜占庭帝国想要进攻一个强大的敌人，为此派出了 10 支军队去包围这个敌人。这个敌人虽不比拜占庭帝国，但也足以抵御 5 支常规拜占庭军队的同时袭击。这 10 支军队在分开的包围状态下同时攻击。他们任一支军队单独进攻都毫无胜算，除非有至少 6 支军队（一半以上）同时袭击才能攻下敌国。他们分散在敌国的四周，依靠通信兵骑马相互通信来协商进攻意向及进攻时间。困扰这些将军的问题是，他们不确定他们中是否有叛徒，叛徒可能擅自变更进攻意向或者进攻时间。在这种状态下，拜占庭将军们才能保证有多于 6 支军队在同一时间一起发起进攻，从而赢取战斗？&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;先看在没有叛徒情况下，假如一个将军 A 提一个进攻提议（如：明日下午 1 点进攻，你愿意加入吗？）由通信兵通信分别告诉其他的将军，如果幸运中的幸运，他收到了其他 6 位将军以上的同意，发起进攻。如果不幸，其他的将军也在此时发出不同的进攻提议（如：明日下午 2 点、3 点进攻，你愿意加入吗？），由于时间上的差异，不同的将军收到（并认可）的进攻提议可能是不一样的，这是可能出现 A 提议有 3 个支持者，B 提议有 4 个支持者，C 提议有 2 个支持者等等。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;再加一点复杂性，在有叛徒情况下，一个叛徒会向不同的将军发出不同的进攻提议（通知 A 明日下午 1 点进攻， 通知 B 明日下午 2 点进攻等等），一个叛徒也会可能同意多个进攻提议（即同意下午 1 点进攻又同意下午 2 点进攻）。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;叛徒发送前后不一致的进攻提议，被称为 “拜占庭错误”，而能够处理拜占庭错误的这种容错性称为「Byzantine fault tolerance」，简称为 BFT。 二、解决方案1. 中本聪的解决方案: 工作量证明机制（POW）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在出现比特币之前，解决分布式系统一致性问题主要是 Lamport 提出的 Paxos 算法或其衍生算法。Paxos 类算法仅适用于中心化的分布式系统，这样的系统的没有不诚实的节点（不会发送虚假错误消息，但允许出现网络不通或宕机出现的消息延迟）。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;中本聪在比特币中创造性的引入了 “工作量证明（POW : Proof of Work）” 来解决这个问题，有兴趣可进一步阅读工作量证明。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过工作量证明就增加了发送信息的成本，降低节点发送消息速率，这样就以保证在一个时间只有一个节点 (或是很少) 在进行广播，同时在广播时会附上自己的签名。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这个过程就像一位将军 A 在向其他的将军（B、C、D…）发起一个进攻提议一样，将军 B、C、D… 看到将军 A 签过名的进攻提议书，如果是诚实的将军就会立刻同意进攻提议，而不会发起自己新的进攻提议。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;以上就是比特币网络中是单个区块（账本）达成共识的方法（取得一致性）。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;理解了单个区块取得一致性的方法，那么整个区块链（总账本）如果达成一致也好理解。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们稍微把将军问题改一下：假设攻下一个城堡需要多次的进攻，每次进攻的提议必须基于之前最多次数的胜利进攻下提出的（只有这样敌方已有损失最大，我方进攻胜利的可能性就更大），这样约定之后，将军 A 在收到进攻提议时，就会检查一下这个提议是不是基于最多的胜利提出的，如果不是（基于最多的胜利）将军 A 就不会同意这样的提议，如果是的，将军 A 就会把这次提议记下来。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这就是比特币网络最长链选择。 2. 权益证明机制（POS）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;工作量证明其实相当于提高了做叛徒（发布虚假区块）的成本，在工作量证明下，只有第一个完成证明的节点才能广播区块，竞争难度非常大，需要很高的算力，如果不成功其算力就白白的耗费了（算力是需要成本的），如果有这样的算力作为诚实的节点，同样也可以获得很大的收益（这就是矿工所作的工作），这也实际就不会有做叛徒的动机，整个系统也因此而更稳定。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;很多人批评工作量证明造成巨大的电力浪费，促使人们去探索新的解决一致性（共识）问题的机制：”权益证明机制（POS: Proof of Stake）”是一个代表。在拜占庭将军问题的角度来看，它同样提高了做叛徒的成本，因为账户需要首先持有大量余额才能有更多的几率广播区块。 2019-08-25更新————————————当你累了的时候，停下来做个梦吧。愿你坚持到底。]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>拜占庭问题,拜占庭容错</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[并发与并行]]></title>
    <url>%2F2019%2F08%2F18%2F%E5%B9%B6%E5%8F%91%E4%B8%8E%E5%B9%B6%E8%A1%8C%2F</url>
    <content type="text"><![CDATA[下面一张图片可以让你更好地理解程序中的并发与并行之间的区别： 更详细的信息，请参考链接https://golangbot.com/concurrency/]]></content>
      <categories>
        <category>计算机语言</category>
      </categories>
      <tags>
        <tag>并发</tag>
        <tag>并行</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DAPP到底是什么？]]></title>
    <url>%2F2019%2F08%2F14%2FDAPP%E5%88%B0%E5%BA%95%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[什么是Dapp，相比较于app，有什么不同？ 一、什么是Dapp？DAPP是Decentralized Application的缩写，即去中心化应用，也有人称为分布式应用。它被认为开启了区块链3.0时代。DAPP就是在底层区块链平台衍生的各种分布式应用，是区块链世界中的服务提供形式。DAPP之于区块链，有些类似APP之于IOS和Android。 二、Dapp的特点 1.Dapp通过网络节点去中心化操作可以运行在用户的个人设备之上，比如：手机、个人电脑。永远属于用户，也可以自由转移给任何人。2.运行在对等网络上不依赖中心服务器，不需要专门的通信服务器传递消息，也不需要中心数据库来记数据。数据保存在用户个人空间，可能是手机，也可能是个人云盘。3.数据加密后存储在区块链上可以依托于区块链进行产权交易、销售，承载没有中介的交易方式。4.参与者信息被安全存储可以保护数字资产，保证产权不会泄露、被破坏。5.Dapp必须开源、自治可以由用户自由打包生成，签名标记所属权。它的发布不受任何机构限制。 各种创意与创新可以自由表达和实现。 三、Dapp与app的区别从客户体验角度APP相对于DAPP有四大问题，一是截留用户数据，二是垄断生态平台，三是保留用户权利，四是限制产品标准扼杀创新。但是由于Dapp得到的是去中心化，所以响应速度固然没有中心化服务器快。从技术角度DAPP与APP区别主要有两个方面，一是APP在安卓或苹果系统上安装并运行；DAPP在区块链公链上开发并结合智能合约；二是APP信息存储在数据服务平台，可以运营方直接修改；DAPP数据加密后存储在区块链，难以篡改。 四、Dapp的分类根据去中心化的对象，DAPP可以进行分类。对于一个中心化服务器而言，包括计算、存储能力，以及所产生的数据三个方面，而由数据之前的关联度又产生了某种特定的“关系”。因此一般而言，去中心化包括以下几类： 1.基于计算能力的去中心化（Pow机制）2.基于存储能力的去中心化（IPFS）3.基于数据的去中心化（Steemit）4.基于关系的去中心化（去中心化ID）]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>Dapp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[go语言学习]]></title>
    <url>%2F2019%2F08%2F10%2Fgo%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[go语言: 面向对象、强类型、类似c的语言。 1.同一个目录下面不能有多个package main，分到不同的文件夹中即可；2.go test *_test.go是golang特有的约定，为测试文件: go run: cannot run *_test.go files; go test 默认执行当前目录下以xxx_test.go的测试文件; go test -v 可以看到详细的输出信息; go test -v xxx_test.go 指定测试单个文件，但是该文件中如果调用了其它文件中的模块会报错; go test -v -test.run Testxxx, 该测试会测试包含该函数名的所有函数. 函数修饰符view：只能读取数据，不能更改数据；修饰符pure：不访问程序中的数据，他的返回值完全取决于传入的参数 测试代码见github 2019-08-25更新————————————当你累了的时候，停下来做个梦吧。愿你坚持到底。 2019-09-01————————————推荐给大家一个非常好的入门学习中文网站，里面很全，从基本数据类型、语法，到协程并发、高阶函数、类、多态等。go语言中文网 panic和recover参考文档：panic和recover代码在这里 头等函数参考文档：头等函数代码在这里 反射参考文档：反射代码在这里 读取文件参考文档：读取文件代码在这里 写入文件/并发写入参考文档：写入文件/并发写入代码在这里 2019-09-15更新 框架beego学习执行过程 当你累了的时候，停下来做个梦吧。愿你坚持到底。]]></content>
      <categories>
        <category>计算机语言</category>
      </categories>
      <tags>
        <tag>go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[比原链-共享经济平台简介]]></title>
    <url>%2F2019%2F08%2F04%2F%E6%AF%94%E5%8E%9F%E9%93%BE-%E5%85%B1%E4%BA%AB%E7%BB%8F%E6%B5%8E%E5%B9%B3%E5%8F%B0%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[Origin Protocol ：基于以太坊，开源的共享经济平台，是否能够成功呢？ Origin Protocol ：基于以太坊，开源的共享经济平台诸如Airbnb、Uber、Craigslist、WeWork等共享经济公司出现之后，共享经济改变了整个商业世界的规则：消费者更喜欢去拥有一个事物的使用权，而不是所有权；而服务提供者从自己提供服务变成了生产者和消费者之间连接的纽带。整个商业模式发生了变化。 到2016年为止，美国有大约22%成年人成为了共享经济的供应商，共享经济平台的收入在2017年是186亿美元，而据估计在2022年会达到400多亿美元，这是一个巨大的市场。 痛点现有市场还存在哪些问题呢？ 首先，价值的分配是不公平的。都说第一个吃螃蟹的人能够获取更多的利益，但是第一个开Uber、第一个给Airbnb提供房间的人并没有因为自己是早期参与者（共建者）而受益，利益全部都在公司本身手里。 其次，高昂的终结费用。Airbnb对房客收取5%-15%手续费，对房东也收取3%-5%，Uber也会对司机和乘客收取类似比例的费用。并且在平台做大之后，他们为了垄断会把收入用在阻碍创新上。 然后，还有数据的所有权、安全性等问题。 其实上述的问题都是中心化平台出现的问题。Origin Protocol就是为解决上述的痛点开发的。 ￼ 简介Origin Protocol是基于以太坊上的共享经济平台，并用IPFS解决文件的存储问题，在分布式网络环境中促进开放、免费的服务交换。平台主要由三个部分组成： 1、 Origin Dapp：分布式应用（Dapp）服务提供者能够锁定一定的代币作为抵押来创建列表，让用户搜索服务，在Dapp中能够利用法币、ERC20代币来进行结算。在不同垂直行业可以开发不同的应用来做到定制化。并且，在Origin Protocol中注册的用户能够方便地访问基于Origin Protocol的所有应用。 2、 Origin 共享数据层和标准共享数据层能够让所有人都能够访问数据库。这些数据存储在IPFS和以太坊中，任何人都可以从中获取到列表项目、交易记录和买卖双方的信誉评级，从而能够被信任。 3、 Origin 社区基金很大一部分的资金会交与基金管理，保证平台的长期发展。基金需要负责；项目管理、项目的孵化、雇佣开发者编写以及审核代码、财务和技术审核、提供仲裁服务等。 4、Origin Protocol的特点用智能合约保证价值的点对点传播（无中介，安全可靠） 支持数百种列表类型，提供多元化的服务 用共享数据鼓励创新 利用区块链技术保证数据和身份的安全 5、团队目前，Origin的核心团队有10名成员，延伸团队有8名成员，涵盖了技术团队、社群运营专家、商业产品团队，具备了项目研发、商业落地的人力资源。核心成员来自伯克利、斯坦福等高校，拥有丰富的创业经验，其创业项目被沃尔玛、雅虎等公司收购。首席区块链工程师曾任Sphero（知名智能玩具公司）的核心技术工程师，技术团队都拥有软硬件开发的从业经历。总的来说，这是一个组成完备，从业经验丰富的团队。￼ 6、Origin Protocol代币技术层面上Origin Protocol的代币是十分复杂的，具体可以参考白皮书。一句话来说，代币的作用是用正面和负面的激励来确保平台的安全、实现管理并且促进买卖双方的交易。 一个具体的场景是，为了避免垃圾信息，卖家在实施相关措施的时候需要抵押一定的代币，通过“押金-质疑-投票”机制，鼓励用户抵押等量代币，标记出质疑的内容，社区进行投票，胜利方可以获得这些代币，通过这样的机制来避免垃圾信息。 具体代币分配暂未公布。 目前在COINLIST上开放投资者注册通道，不允许中国投资者参与，请需要参加的准备好相关材料。 7、开发路线Origin Protocol项目从17年五月开始，12月份就推出了测试网络，预计在18年第三季度完成平台的开发，19年达到去中心化并且正式运行。 合作伙伴 官网上列出大量合作伙伴，并且有不少团队已经开始基于Origin Protocol的app开发。 ￼ 总结总体来看，Origin Protocol相比于各类提出4.0、5.0概念的公链，是一个十分落地的项目，并且已经与大量的企业建立合作关系。目前Origin Protocol的测试网络已经上线，我们期待未来Origin Protocol的发展。 官网：https://www.originprotocol.com 白皮书：https://www.originprotocol.com/en/product-brief DEMO视频：https://demo.originprotocol.com 个人观点比原链如何避免共享平台的垃圾信息、虚假信息呢？比原链的解决方案是，卖家在实施相关措施的时候需要抵押一定的代币，通过“押金-质疑-投票”机制，鼓励用户抵押等量代币，标记出质疑的内容，社区进行投票，胜利方可以获得这些代币，通过这样的机制来避免垃圾信息。那在所有人都可以参与的情况下，如何保证刷单的事情发生呢？卖家同时拥有许多账号，并且进行投票给自己的竞争对手，此时，就会形成恶性竞争。]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>比原链</tag>
        <tag>公链</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[公链如此多，而应用却寥寥无几]]></title>
    <url>%2F2019%2F08%2F04%2F%E5%85%AC%E9%93%BE%E5%A6%82%E6%AD%A4%E5%A4%9A%EF%BC%8C%E8%80%8C%E5%BA%94%E7%94%A8%E5%8D%B4%E5%AF%A5%E5%AF%A5%E6%97%A0%E5%87%A0%2F</url>
    <content type="text"><![CDATA[现在很多公司都在开发自己的公链，真正应用落地的很少，其中包括IBM开源，贡献给Linux基金会的HyperLedger，另一个就是以太坊了。 下面来看看为什么？ 现在很多公司都在开发自己的公链。创业团队在开发公链，一些加密数字货币交易所也在开发公链。我曾经见过一个由三个年轻人组成的创业团队，不仅开发了自己的公链，而且还提出了一个新的共识算法。但目前的公链市场却出现了一个非常尴尬的局面，一方面是公链产品无穷多，但另外一方面却是公链落地的项目寥寥无几。显然在区块链产品落地应用方面，出现了明显的脱节。当然任何一个新技术的落地应用都需要一段时间。对于区块链技术这个从根本上改变现有的以中心化技术为基础的计算模式的新技术来说，其应用落地的时间就更长。但在目前的公链产品落地应用的过程中，还是有些方面可以改进，以此来加速公链技术应用落地的。在当前的社会中，其实不缺区块链应用的落地场景。市场中的很多问题实际上都能应用区块链能得到非常好的解决。区块链技术的最强项是采用完全信息真实透明的方式杜绝欺诈，是用技术的方式保证多方的合作顺利完成。如果从这个角度看，现实中太多的问题都可以用区块链来解决了。譬如个人借贷的违约方面。违约者的一个主要动机就是因为违约成本非常低。他可以欺诈了一个出借方之后，再去欺诈另外一个欺诈方。但是如果每个人的信贷记录都真实无误地记录在公链上，任何个人和机构都能查到这个借贷记录，那么借贷者进行欺诈动机就小多了。对于一直没有被现有的金融机构服务到的客户来说，如果其在金融机构之外的各种借贷记录都真实无误的记录在区块链上，那么他个人的信用历史就是真实可信的。金融机构就可以基于这个信用向其进行贷款，因为在这个过程中，征信的成本几乎为零，而征信成本过高正是金融机构不愿意进行贷款的一个原因。在公司的层面，同样存在着大量的可以应用区块链技术的地方，譬如贸易金融和银团的联合贷款。在证券领域，区块链技术除了在交易后清算之外，另外一个最直接的应用就是投行项目的融资过程。由于上市的收获巨大，所以在这个过程中，各个参与者在各个融资阶段铤而走险进行欺诈的案例屡见不鲜。在这个过程中的一个主要欺诈方面就是信息作假。如果采用区块链技术来管理这个流程，每个参与者都需要为自己上传到链上的信息负责，那么每个参与者作假的动机就会大幅减小。即使有人铤而走险进行造假，那么此后的法律诉讼过程中的取证就非常容易，也就容易形成及时公正的判决。那么为什么现实中有这么多的需求，但于此同时基于区块的应用却为什么这么少？在这个方面既有产品通常规律中犯错的地方，也有区块链技术应用的具体问题。 1、产品同市场的需求不匹配首先，造成这种局面的原因主要还是产品与市场需求的不匹配，也就是缺少 Product Market Fit. 一些区块链技术的开发方专注于解决区块链技术本身的问题，而忽略了解决其技术应该解决的市场中的问题。一些公链项目总是在宣扬自己的产品的性能如何好，能达到多少的 TPS。但这个卖点本身就是错误的定位。首先，一个企业级技术产品的衡量指标不只是性能，而且还有稳定性、安全性、权限控制等其他方面。其次，市场中对区块链技术的评价，首先会把它同相应的中心化解决方案相比较。在性能方面，基于区块链的解决方案绝对无法同基于中心化技术的解决方案相比的。所以一味地强调性能根本无法说服市场来接受这个产品。区块链技术最擅长的解决是多方合作中的信任问题。而在现实的场景中，很多这种场景是不需要高性能的。譬如贸易金融的合作过程，又譬如企业融资过程中的各类机构合作的过程。这些过程更注重于性能以外的其他因素，如信息的一致性、权限控制和使用的便捷性等等。如果公链一味地追求性能，那么它就同市场中的真正需求南辕北辙了。 2、高度竞争的领域公链产品定位的另外一个主要错误是在开发一个同以太坊相竞争的普适的公链。但这样的产品定位，其成功的可能性极小。以太坊的问题很多，这是众所周知的。但它已经是经过几年的发展，已经基本上成为市场中默认的公链选择。现在希望取代以太坊的公链创业项目太多了。在这样的高度竞争的环境中，胜出的几率是非常小的。在这个方面，很多公链团队都做出了错误的选择。选择加入到了一个高度竞争的领域。这恰恰违背了一个产品开发的基本规律，就是避开竞争。记得彼得•蒂尔的建议避开竞争的观点吗？避开竞争的一个有效手段就是采用创新的方式来解决市场中的一个问题。因为创新一开始是并不为市场接受，所以并没有太多的竞争者做同样的事情。当创新的方式逐渐为市场接受时，这个创新产品在市场中已经占据稳定的地位了。别的团队就无法再做同样的产品进行竞争。在这个方面，中本聪发明的比特币就是此方面的最好的代表。中本聪的初衷是发行一个电子现金来取代市场中的货币。但是他并没有以直接同现有货币竞争的方式发行一种货币。比特币的金融属性直到多年以后才被市场发现，拥有了众多的用户，并开始对现有的金融市场形成了巨大的挑战。尽管后来也持续不断的有模仿者，但这些模仿者都已经无法对比特币的地位形成挑战了。 3、改变群体行为的困难区块链技术本身的特点也决定它比其它的产品更难被市场接受。这是因为它需要改变的是一个群体行为，而不是一个个体行为。譬如在多方合作的过程中，如果只有一方愿意采用基于区块链的解决方案，但其他它方没有动力的情况下，这个解决方案就没法推动。特别是当其中的一些参与方本来的想法就是利用信息的不透明来为自己谋利。那么如何才能实现区块链技术的快速落地应用呢？首先我认为从技术的角度来切入市场没问题，但更应该从需求的角度来切入市场，也就是说从市场中的一个具体的问题出发，来分析如何用区块链技术来解决这个问题。鉴于目前市场中互不信任和欺诈行为的普遍存在，所以找到这样的场景并不困难。其次才是自己开发或在市场中找到合适的区块链底层来进一步开发。而这样的区块链技术底层并不一定需要各项技术指标方面都十全十美。它只要能对针对需要解决的问题能够令人满意的解决就可以了。看一看目前的国内市场，获得市场欢迎和资本支持的一些成功的区块链项目并不是基于什么技术性能特别好的公链项目，实际上这些项目是利用的只是区块链技术最基本的分布式存储和不可更改的功能。但是这些技术底层为需要解决的问题提供了足够好的技术方案，这就足以创造很好的商业价值了。其次就是不要挑战以中心化技术为基础的现有实力最强的地方。这些领域如零售支付、稳定币、证券和银行领域中的清算。在这些领域中采用区块链技术，产生的收益未必足够大，但遇到的风险和阻力却会非常大，因此在这些领域中应用区块链技术需要非常慎重。第三，一定要用创新的方式解决现有的问题。在这方面，比特币是最优秀的典范。在现有的很多问题中，由于技术、监管、经营习惯和、成本、应用落地所需的时间和风险等方面的考虑，行业中现有的参与者们并没有很强的动力去采用区块链技术、区块链技术的应用一定要找到非常创新，容易被市场接受的地方。尽管这个初始应用的市场并不大，但只要是此方面应用的商业和技术模式具有很强的可扩展性，并且是针对潜在的巨大市场，只要项目方做好商业和技术方面的顶层设计（见我的相关文章区块链时代的顶层设计）；另外，再由于这种创新方式不一定被广泛认可，因此也不会迅速地吸引竞争者，这个方式因此就有足够的时间逐步发展起来。等到市场的各方终于发现这种创新模式的价值的时候，同它的竞争和对它的打压都已经来不及了。这就是在本质上重新复制了比特币的成功模式。 本文摘抄自链接：https://www.jianshu.com/p/58f4cce02e3f]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>区块链</tag>
        <tag>公链</tag>
        <tag>区块链应用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018年全球各国谋杀比例]]></title>
    <url>%2F2019%2F08%2F04%2F2018%E5%B9%B4%E5%85%A8%E7%90%83%E5%90%84%E5%9B%BD%E8%B0%8B%E6%9D%80%E6%AF%94%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[Murders rate per 100,000 people, last available year.2018年，每10万人中被谋杀的比例如下： 比例 国家 🇭🇳HON: 90 洪都拉斯 🇻🇪VEN: 54 委内瑞拉 🇧🇷BRA: 25 巴西 🇲🇽MEX 21.5 墨西哥 🇳🇬NIG: 20 尼日尔 🇷🇺RUS: 9.2 俄罗斯 🇵🇰PAK: 7.7 巴基斯坦 🇺🇸USA: 4.7 美国 🇮🇳IND: 3.5 印度 🇹🇷TUR: 2.6 土耳其 🇨🇦CAN: 1.6 加拿大 🇦🇺AUS 1.1 澳大利亚 🇨🇳CHN: 1 中国 🇬🇧GBR: 1 英国 🇫🇷FRA: 1 法国 🇰🇷KOR: 0.9 韩国 🇮🇹ITA: 0.9 意大利 🇩🇪GER: 0.8 德国 🇪🇸ESP: 0.8 西班牙 🇦🇪UAE: 0.7 阿联酋 🇯🇵JPN: 0.3 日本]]></content>
      <categories>
        <category>世界排名</category>
      </categories>
      <tags>
        <tag>谋杀比例</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阿甘正传剪辑]]></title>
    <url>%2F2019%2F08%2F03%2F%E9%98%BF%E7%94%98%E6%AD%A3%E4%BC%A0%E5%89%AA%E8%BE%91%2F</url>
    <content type="text"><![CDATA[var player = new YKU.Player( 'youkuplayer',{ styleid: '0', client_id: 'YOUR YOUKUOPENAPI CLIENT_ID', vid: 'XNDI5NzE3Mzk0MA', newPlayer: true } );]]></content>
      <categories>
        <category>视频剪辑</category>
      </categories>
      <tags>
        <tag>阿甘正传</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[airbnb开源调度系统airflow的一些命令及使用方法]]></title>
    <url>%2F2019%2F08%2F02%2Fairbnb%E5%BC%80%E6%BA%90%E8%B0%83%E5%BA%A6%E7%B3%BB%E7%BB%9Fairflow%E7%9A%84%E4%B8%80%E4%BA%9B%E5%91%BD%E4%BB%A4%E5%8F%8A%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[python写的调度系统，用python脚本，动态生成dag，跨dag依赖，是一个不错的调度系统，下面介绍一些我使用过程中用到的命令和问题的解决方案。 1.operator12345678BashOperatorPythonOperatorEmailOperatorHTTPOperatorSqlOperatorSensorDockerOperatorHiveOperator 2.给DAG实例传递参数执行命令 1airflow trigger_dag example_passing_params_via_test_command -c &apos;&#123;&quot;foo&quot;:&quot;bar&quot;&#125;&apos; 代码获取变量： 123def my_py_command(ds, **kwargs):logging.info(kwargs)logging.info(kwargs.get(&apos;dag_run&apos;).conf.get(&apos;foo&apos;)); 3.填补数据1234#清除dag在这段时间内的状态，清除后airflow会自动启动这些任务，如果dag设置了catchup=True;dependency_on_past=True;那么dag会按照时间顺序一天一天跑任务，这对于修补数据很有用哦airflow clear db2src_usersdb_byshell -s 2018-12-01 -e 2018-12-04#回填数据，当新建一个dag，需要补跑以前的数据，回填命令是个不错的选择airflow backfill db2src_usersdb_byshell -s 2018-12-03 -e 2018-12-04 4.根据depend_on_pastTrue or False来判断是否需要依赖start_time前段时间跑的相同的任务情况来运行现在的任务。 5.airflow卡住的问题连接元数据mysql库：select * from task_instance where state = ‘running’; 6.airflow自带变量：12345678910111213141516171819202122232425262728| Variable | Description || :------: | :---------: ||&#123;&#123; ds &#125;&#125; |the execution date as YYYY-MM-DD||&#123;&#123; ds_nodash &#125;&#125; |the execution date as YYYYMMDD||&#123;&#123; yesterday_ds &#125;&#125; |yesterday’s date as YYYY-MM-DD||&#123;&#123; yesterday_ds_nodash &#125;&#125; |yesterday’s date as YYYYMMDD||&#123;&#123; tomorrow_ds &#125;&#125; |tomorrow’s date as YYYY-MM-DD||&#123;&#123; tomorrow_ds_nodash &#125;&#125; |tomorrow’s date as YYYYMMDD||&#123;&#123; ts &#125;&#125; |same as execution_date.isoformat()||&#123;&#123; ts_nodash &#125;&#125; |same as ts without - and :||&#123;&#123; execution_date &#125;&#125; |the execution_date, (datetime.datetime)||&#123;&#123; prev_execution_date &#125;&#125; |the previous execution date (if available) (datetime.datetime)||&#123;&#123; next_execution_date &#125;&#125; |the next execution date (datetime.datetime)||&#123;&#123; dag &#125;&#125; |the DAG object||&#123;&#123; task &#125;&#125; |the Task object||&#123;&#123; macros &#125;&#125; |a reference to the macros package, described below||&#123;&#123; task_instance &#125;&#125; |the task_instance object||&#123;&#123; end_date &#125;&#125; |same as &#123;&#123; ds &#125;&#125;||&#123;&#123; latest_date &#125;&#125; |same as &#123;&#123; ds &#125;&#125;||&#123;&#123; ti &#125;&#125; |same as &#123;&#123; task_instance &#125;&#125;||&#123;&#123; params &#125;&#125; |a reference to the user-defined params dictionary||&#123;&#123; var.value.my_var &#125;&#125; |global defined variables represented as a dictionary||&#123;&#123; var.json.my_var.path &#125;&#125; |global defined variables represented as a dictionary with deserialized JSON object, append the path to the key within the JSON object||&#123;&#123; task_instance_key_str &#125;&#125; |a unique, human-readable key to the task instance formatted &#123;dag_id&#125;_&#123;task_id&#125;_&#123;ds&#125; ||conf |the full configuration object located at airflow.configuration.conf which represents the content of your airflow.cfg||run_id |the run_id of the current DAG run||dag_run | a reference to the DagRun object||test_mode | whether the task instance was called using the CLI’s test subcommand| 7.导入导出airflow变量12airflow variables --import variable.jsonairflow variables --export variable.txt 8.Template Not FoundTemplateNotFound: sh /data/airflow_dag/dags_migration/sh/export-variables.sh这是由于airflow使用了jinja2作为模板引擎导致的一个陷阱，当使用bash命令的时候，尾部必须加一个空格 12345t2 = BashOperator(task_id=‘sleep‘,bash_command=&quot;/home/batcher/test.sh&quot;, // This fails with `Jinja template not found` error#bash_command=&quot;/home/batcher/test.sh &quot;, // This works (has a space after)dag=dag) 9. 手动触发dag运行1airflow trigger_dag dag_id -r RUN_ID -e EXEC_DATE 10. 手动触发task运行1airflow run dag_id task_id EXEC_DATE 11. “Failed to fetch log file from worker”查看task_instance中hostname字段，存储的均为localhost；分析：修改/etc/hosts文件，删除127.0.0.1 hostname映射；worker log服务获取到hostname后，映射到ip后得到127.0.0.1，故无法访问到log。 12. airflow中每个task对应的执行priority计算方式dummy2 = DummyOperator( task_id=’dummy_’ + src_db, pool=’db’, priority_weight=weight, dag=dag) 所有后置依赖的priority_weight之和，最后一个任务的priority_weight如果没有自定义，默认为1，这样，在同一个pool中做到了任务优先运行；]]></content>
      <categories>
        <category>调度系统</category>
      </categories>
      <tags>
        <tag>airflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令相关]]></title>
    <url>%2F2019%2F07%2F28%2FLinux%E5%91%BD%E4%BB%A4%E7%9B%B8%E5%85%B3%2F</url>
    <content type="text"><![CDATA[关于linux操作系统的一些使用命令，看下面。 1. linux下查看某个文件或文件夹占用的磁盘空间大小1du -ah --max-depth=1 2.sed修改文件在每行行首或者行尾添加相同的字符串 12sed &apos;s/^/HEAD&amp;/g&apos; text.file 每行行首添加HEADsed &apos;s/$/&amp;TAIL/g&apos; text.file 每行行尾添加TAIL 如果要修改原文件，则添加 -i参数 12sed -i &apos;s/^/HEAD&amp;/g&apos; text.filesed -i &apos;s/$/&amp;TAIL/g&apos; text.file 递归替换 1find . -type f -print0 | xargs -0 sed -i &apos;s/10.1.0.33,10.1.0.44,10.1.0.48/$&#123;es_nodes&#125;/g&apos; 文件第一行添加字符串 1sed -i &quot;1i\添加内容&quot; filename 3.查看centos版本1cat /etc/redhat-release 4.查看cpu12cat /proc/cpuinfo |grep &quot;physical id&quot;|sort|uniq|wc -l 查看cpu核数cat /proc/cpuinfo | grep &quot;cpu cores&quot; | uniq 物理cpu个数 5.查看内存1free -h 6.查看磁盘容量1df -h 7.查看端口号对应进程号1netstat -tunlp|grep 端口号 8.查看未释放空间的进程1lsof | grep deleted 9.杀死未释放空间的进程1lsof | grep deleted | awk &apos;&#123;print $2&#125;&apos; | sort | uniq | xargs kill -9 10.grep12345grep -o &quot;ods\.[a-z|A-Z|_]*&quot; ods2report.py | grep &quot;_&quot; | sort | uniq -cgrep -o只显示匹配内容uniq -c计算重复行数量grep &quot;\&quot;db\&quot;&quot; *.py | awk -F &apos;:&apos; &apos;&#123;print $1&#125;&apos; | sort | uniqgrep -o &quot;ods\.[a-z|A-Z|_|0-9]*&quot; *.py | awk -F &apos;:&apos; &apos;&#123;print $2&#125;&apos; |grep &quot;_&quot; | sort | uniq -c 11.查看详细进程信息1top -c]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive相关]]></title>
    <url>%2F2019%2F07%2F21%2Fhive%E7%9B%B8%E5%85%B3%2F</url>
    <content type="text"><![CDATA[下面的内容包含hive的简单操作，增删改查，权限，一些异常的解决方案。 1. 在linux命令行端执行hql语句1hive -e &apos;show table tableName&apos; 2. 在linux命令行端执行hql文件1hive -f fileName 3. 按照分区查看hive表中的数据量1hive -e &apos;select dt, count(1) from test_hive.wps_android_uuid_userid group by dt&apos; 4. 添加分区1alter table test_hive.wps_android_uuid_userid add if not exists partition (dt=&apos;2018-08-04&apos;) 5. 赋表权限1grant select on table usersdb.account_src to user w_wangzhe 6. 赋库权限12345GRANT ALL ON DATABASE DEFAULT TO USER fatkun;GRANT ALL ON TABLE test TO GROUP kpi;REVOKE ALL ON TABLE test FROM GROUP kpi;GRANT ALL TO USER fatkun;REVOKE ALL FROM fatkun; 7. 重命名1alter table data_platform.td_request_log rename to data_platform.td_request_log_old 8. Errorwhile processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask 1set hive.auto.convert.join = false; 9.执行sql文件1hive -d etldate=&apos;2018-08-16&apos; -f loan_periods.hql 10.add jar1add jar /usr/hdp/current/hive-client/lib/commons-httpclient-3.0.1.jar; 11.add column1hive -e &quot;alter table report.user_detail_20180614 add columns(identifier_type string comment &apos;注册类型&apos;,channel string comment &apos;注册渠道&apos;)&quot; 12.load 文件到hive表本地文件： 1hive -e &quot;load data local inpath &apos;/data/code/app_list_0814.csv&apos; into table dim.dim_app_list&quot; hdfs文件： 1hive -e &quot;load data inpath &apos;/data/code/app_list_0814.csv&apos; into table dim.dim_app_list&quot; 13.列13.1修改列位置alter table factor.mf_bus_finc_app change column submit_op_no submit_op_no string after company_id 13.2增加列hive -e “alter table data_platform_new.face_request_log add columns(channel string)” 14.修改权限hive -e “grant select on table stg.risk_apply_users to user userName” 15.自定义函数12create function dateformat as &apos;com.kso.dw.hive.udf.DateFormat&apos; using jar &apos;hdfs://hdfs-ha/hiveudf/dw_hive_udf.jar&apos;;mysql -h10.0.1.160 -uadmin -pmd854NHmv3bF0kl9 hive4fac31f3 -e &apos;select name from dbs&apos; | xargs -n 1 -i echo &quot;create function &#123;&#125;.mymd5_kc as &apos;com.kso.dw.hive.udf.MyMd5_KeyCenter&apos; using jar &apos;hdfs://hdfs-ha/hiveudf/dw_hive_udf.jar&apos;;&quot; 16.全局替换1sed -i &apos;s/CREATE TABLE/CREATE EXTERNAL TABLE/g&apos; *.hql 17.not a file exceptionnot a file ks3://online-hadoop/ods/report/dt=2019-01-01/1 1set mapreduce.input.fileinputformat.input.dir.recursive=true; 18.exception1set hive.execution.engine=mr; 19. too many countersorg.apache.hadoop.mapreduce.counters.LimitExceededException: Too many counters: 121 max=120resolved:change the tez configuration 1tez.counters.max= 200 20.SHOW TRANSACTIONS12345ABORT TRANSACTIONS 4951;show locks;mysql:select * from hive_locks;select * from hive_locks where HL_TXNID &gt; 0; 21.acquiring locksFAILED: Error in acquiring locks: Lock acquisition for LockRequest(component:[LockComponent(type:EXCLUSIVE关闭事务： set hive.support.concurrency=false 22.事务表查询12345set hive.support.concurrency=true;set hive.exec.dynamic.partition.mode=nonstrict;set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;set hive.compactor.initiator.on=true;set hive.compactor.worker.threads=1; 23. 内部表转外部表1alter table default.test set TBLPROPERTIES(&apos;EXTERNAL&apos;=&apos;true&apos;); 24. 外部表转内部表1alter table tableA set TBLPROPERTIES(&apos;EXTERNAL&apos;=&apos;false&apos;) 25.修改元数据路径元数据库：123UPDATE dbs SET DB_LOCATION_URI=REPLACE(DB_LOCATION_URI,&apos;hdfs-ha&apos;,&apos;bjCluster&apos;);``` 元数据表： UPDATE sds SET LOCATION=REPLACE(LOCATION,’ks-jinrong-dw’,’online-hadoop’);UPDATE sds SET LOCATION=REPLACE(LOCATION,’hdfs-ha’,’bjCluster’); 1自定义函数： UPDATE func_ru SET RESOURCE_URI=REPLACE(RESOURCE_URI,’hdfs-ha’,’bjCluster’); 123# 26.set role admin# 27.控制map个数 set mapred.max.split.size=400000000;set mapred.min.split.size.per.node=400000000;set mapred.min.split.size.per.rack=400000000;set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[debezium实时同步mysql、postgresql数据介绍]]></title>
    <url>%2F2019%2F07%2F20%2Fdebezium%E5%AE%9E%E6%97%B6%E5%90%8C%E6%AD%A5mysql%E3%80%81postgresql%E6%95%B0%E6%8D%AE%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[给大家介绍一个实时同步数据库的组件debezium，它可以同步mysql、postgresql、mongo、oracle、sql server数据库到hdfs、kafka，功能强大，具体如下。 大致安装步骤： 1.插件decoderbufs或者wal2json2.postgresql配置logical等3.confluent平台搭建4.配置connector 参考文档：https://debezium.io/docs/connectors/ for postgresql to kafka 核心：一个active slot，多个connector 1. bin/connect-distributed etc/kafka/connect-distributed.properties三台服务器分别执行启动distributed服务（相同的slot.name、group.id保证复制同一个slot replication，保证在同一个组内） 123bin/connect-distributed etc/kafka/connect-distributed-8084.propertiesbin/connect-distributed etc/kafka/connect-distributed-8085.propertiesbin/connect-distributed etc/kafka/connect-distributed-8086.properties 2. bin/connect-standalone etc/kafka/connect-standalone.properties etc/kafka-connect-postgres/debezium.propertiesbin/kafka-console-consumer –zookeeper localhost:2182 –topic postgres.localhost.public.test –from-beginning 3. 安装decoderbufs、wal2json pluginhttps://github.com/debezium/postgres-decoderbufs/blob/master/README.mdhttps://github.com/eulerto/wal2json/blob/master/README.md 安装wal2json时出现的问题：Makefile:10: /usr/lib64/pgsql/pgxs/src/makefiles/pgxs.mk: No such file or directoryyum install postgresql10-devel即可 4. 配置参考文档：https://zhubingxu.me/2018/06/05/debezium-postgres/ share/java/下创建debezium文件夹，创建文件debezium.properties： 1234567891011121314name=events-debeziumtasks.max=1connector.class=io.debezium.connector.postgresql.PostgresConnectordatabase.hostname=localhostdatabase.port=5432database.user=postgresdatabase.password=postgresdatabase.dbname=postgresdatabase.history.kafka.bootstrap.servers=localhost:9092database.server.id=1database.server.name=postgres.localhostplugin.name=wal2jsoninclude.schema.changes=trueslot.name=my\_slot\_name 5. 异常Error while fetching metadata with correlation id 1 : {postgres.localhost.public.test=LEADER_NOT_AVAILABLE} https://stackoverflow.com/questions/35788697/leader-not-available-kafka-in-console-producer 配置debezium中$DEBEZIUM_HOME/etc/kafka/server.properties 指定参数advertised.host.name ERROR A logical replication slot named ‘debezium’ for plugin ‘wal2json’ and database ‘postgres’ is already active on the server.You cannot have multiple slots with the same name active for the same database (io.debezium.connector.postgresql.connection.PostgresReplicationConnection:104) 在创建connector的配置参数中添加新的slot.name，slot.name的规范必须为字母数字下划线。不指定的话默认为debezium，会产生冲突。 6. 分布式kafka connector配置：https://archive.cloudera.com/kafka/kafka/2/kafka-0.9.0-kafka2.0.1/connect.html分布式connector需要通过rest api进行增删改 https://mapr.com/docs/52/Kafka/Connect-distributed-mode.html connect-distributed.properties 连接参数配置：https://debezium.io/docs/connectors/postgresql/#connector-properties 7. 注意：table.whitelist格式：schemaName.tblname 如果启动connector出现权限不足时：需要给用户赋update权限： GRANT SELECT, UPDATE ON TABLE test TO debezium; 不监控无主键的表 8. 查看postgresql slota.登陆：psql -U user -d db -h host -p port -Wb.查看所有slot： select * from pg_replication_slots;c.添加slot：SELECT * FROM pg_create_physical_replication_slot(‘pg96_102’);d.删除slot：SELECT * FROM pg_drop_replication_slot(‘pg96_102’); for mysql to kafka 核心：启动多个connector即可 1. 账户及权限mysql user: debezium:******* GRANT SELECT, RELOAD, SHOW DATABASES, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO ‘debezium’ IDENTIFIED BY ‘******‘; 2. 配置https://debezium.io/docs/connectors/mysql/#setting-up-mysql 3. 启动bin/connect-standalone etc/kafka/connect-standalone.properties share/java/debezium/mysql-debezium-8087.properties 三台服务器分别运行： 123bin/connect-distributed etc/kafka/mysql-connect-distributed-8134.propertiesbin/connect-distributed etc/kafka/mysql-connect-distributed-8134.propertiesbin/connect-distributed etc/kafka/mysql-connect-distributed-8134.properties 4. kafka connector请求添加connector-task，随便在哪一台服务器添加1个task curl -X POST -H “Content-Type: application/json” –data @share/java/debezium/mysql-8134.json http://localhost:8134/connectors table.whitelist格式：dbname.tblname 测试环境test-hadoop：/mnt/confluent下生成的topic名称为：servername.dbname.tblname 5. 异常（未解决） 如果有解决方案，请联系我邮箱chenzuoli709@163.com，不胜感激。]]></content>
      <categories>
        <category>组件</category>
      </categories>
      <tags>
        <tag>实时同步</tag>
        <tag>mysql</tag>
        <tag>oracle</tag>
        <tag>postgresql</tag>
        <tag>mongo</tag>
        <tag>sql server</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[英国构成国]]></title>
    <url>%2F2019%2F07%2F07%2F%E8%8B%B1%E5%9B%BD%E6%9E%84%E6%88%90%E5%9B%BD%2F</url>
    <content type="text"><![CDATA[英国，全称为大不列颠及北爱尔兰联合王国（United Kingdom of Great Britain and Northern Ireland），世界第五大经济体。 听说，去了北爱尔兰，不要说英格兰好，不然会被打，哈哈 地理位置： 构成国： 各构成国详情：]]></content>
      <categories>
        <category>世界国家</category>
      </categories>
      <tags>
        <tag>英国</tag>
        <tag>北爱尔兰</tag>
        <tag>英格兰</tag>
        <tag>苏格兰</tag>
        <tag>威尔士</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[世界各国人民平均汽车拥有量]]></title>
    <url>%2F2019%2F06%2F29%2F%E4%B8%96%E7%95%8C%E5%90%84%E5%9B%BD%E4%BA%BA%E6%B0%91%E5%B9%B3%E5%9D%87%E6%B1%BD%E8%BD%A6%E6%8B%A5%E6%9C%89%E9%87%8F%2F</url>
    <content type="text"><![CDATA[【世界银行：每1000人拥有的汽车数量，美国为837辆最高，中国为173辆】 具体排名如下： 美国：837澳大利亚：747意大利：695加拿大：670日本：591德国：589英国：579法国：569马来西亚：433俄罗斯：373巴西：350墨西哥：297沙特：209土耳其：199伊朗：178南非：174中国：173印度尼西亚：87尼日利亚：64印度：22]]></content>
      <categories>
        <category>世界排名</category>
      </categories>
      <tags>
        <tag>汽车</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[说英语的国家]]></title>
    <url>%2F2019%2F06%2F28%2F%E8%AF%B4%E8%8B%B1%E8%AF%AD%E7%9A%84%E5%9B%BD%E5%AE%B6%2F</url>
    <content type="text"><![CDATA[有哪些以英语为母语的国家呢？ 参照下图，印度人说的最多。 英语学起来，走向全世界。]]></content>
      <categories>
        <category>语言</category>
      </categories>
      <tags>
        <tag>English</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[世界十大语言排名]]></title>
    <url>%2F2019%2F06%2F11%2F%E4%B8%96%E7%95%8C%E5%8D%81%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%8E%92%E5%90%8D%2F</url>
    <content type="text"><![CDATA[世界语言排名，瑞士社会学家按照母语、第二语言、国家经济实力、科学外交重要性、社会、文学地位等方面进行综合评价，得出如下排名。 瑞士社会家者George Weber提出了这样的语言评价体系（图）： 具体来说，评价语言地位需要按这6条标准加权评分综合考虑： 1. 以该语言为母语人数:最高得分 4 2. 以该语言为第二语言的人数: 最高得分 6 3. 使用该语言国家的经济实力: 最高得分8 4. 科学、外交中该语言的重要性:最高得分8 5. 使用该语言的国家数和人口数：最高得分7 6. 该语言的社会、文学地位：最高得分4分（如果是联合国工作语言加1分） 一种语言，在当今世界上处于什么样的排名，地位如何，主要取决于6个指标。 1 使用某种语言的母语人口数量。 （Number of native speakers of the language） 评分：4分 2 使用某种语言的非母语人口数量。 （Number of non-native speakers of the language） 评分：6分 3 使用这种语言的国家数量与人口。 （Number and population of countries using the language） 评分：7分 4 使用这种语言的国家的经济，科技与军事实力。 （Economic, scientific and military power of the countries using the language） 评分：8分 5 在外交，国际贸易，国际组织，学术交流等领域使用这种语言的频率。 （Number of major fields, such as diplomacy, international trade relations, international organizations and academic community, using the language globally） 评分：8分 6 在社会人文领域的声望。（例如：某种语言获得过多少次诺贝尔文学奖，某种语言有过多少世界名著等等） （International socio-literary prestige of the language） 评分：4分 （如果是联合国的官方语言，额外加3分） 上面6个指标，就是判断一种语言在当今世界的排名，地位的综合指标。满分是40分。按母语人口排序的前10名是： 12345678910（1）中文（占世界总人口20.7%）（2）英语（6.2%）（3）西班牙语（5.6%）（4）印地、乌尔都语（4.7%）（5）阿拉伯语（3.8%）（6）孟加拉语（3.5%）（7）巴西葡萄牙语（3.0%）（8）俄语（3.0%）（9）日语（2.3%）（10）德语（1.8%） 值得注意的是法语连前10名都没有进，仅排在第13位（1.4%），险胜排在第14位的韩语。 再看第二项指标：有多少人以该语言为第二语言： 12345678910（1）法语（约1亿8千万）（2）英语（约1亿5千万）（3）俄语（约1亿2千万）（4）葡萄牙语（约3000万）（5）阿拉伯语（约2400万）（6）西班牙语（约2200万）（7）中文（约2100万）（8）德语（约2000万）（9）日语（约1000万）（10）印地语 当然括号中的数字只是大致的估算，不是也不可能是科学统计，但先后顺序大致是不错的。 George Weber先生对其他4项指标也做了估算，限于篇幅不一一叙述，他最后排出了世界语言的前十名： 根据上面那6个指标，所做出的排名 12345678910 第一名：英语 37分 第二名：法语 23分 第三名：西班牙语 20分 第四名：俄语 16分 第五名：阿拉伯语 14分 第六名：汉语 13分 第七名：德语 12分 第八名：日语 10分 第九名：葡萄牙语 10分 第十名：印地语 9分 综上总结： 全球性交流媒介：英语 洲际交流媒介：法语、西班牙语、俄语、阿拉伯语、葡萄牙语]]></content>
      <categories>
        <category>世界排名</category>
      </categories>
      <tags>
        <tag>语言</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Turkish]]></title>
    <url>%2F2019%2F05%2F21%2FTurkish%2F</url>
    <content type="text"><![CDATA[土耳其，一个横跨欧亚大陆，南临地中海，东南与叙利亚、伊拉克接壤，西临爱琴海，并与希腊以及保加利亚接壤，东部与格鲁吉亚、亚美尼亚、阿塞拜疆和伊朗接壤，有热气球的浪漫国家，这边的人民对于工作有什么想法呢？ Where Turkish workers would like to move for work. US Germany Italy Canada UK France Australia Japan Russia China South Korea Brazil India Saudi Argentina (BCG) US.]]></content>
  </entry>
  <entry>
    <title><![CDATA[儿童贫困率排行]]></title>
    <url>%2F2019%2F05%2F19%2F%E5%84%BF%E7%AB%A5%E8%B4%AB%E5%9B%B0%E7%8E%87%E6%8E%92%E8%A1%8C%2F</url>
    <content type="text"><![CDATA[世界儿童贫困率排行，来看看。 Child poverty rate, 2016. (%) 🇨🇳CHN: 33.1%🇧🇷BRA: 30.1%🇹🇷TUR: 25.3%🇮🇳IND: 23.6%🇪🇸ESP: 22.1%🇺🇸USA: 20.9%🇲🇽MEX: 19.7%🇮🇹ITA: 18.3%🇨🇦CAN: 17.1%🇯🇵JPN: 13.9%🇦🇺AUS: 13.0%🇬🇧GBR: 11.8%🇫🇷FRA: 11.3%🇩🇪GER: 11.2%🇨🇭SUI: 9.5%🇸🇪SWE: 8.9%🇰🇷KOR: 7.1%🇫🇮FIN: 3.3%🇩🇰DEN: 2.9% (OECD)]]></content>
      <categories>
        <category>世界排名</category>
      </categories>
      <tags>
        <tag>儿童</tag>
        <tag>贫困率</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[世界失业率排行]]></title>
    <url>%2F2019%2F05%2F19%2F%E4%B8%96%E7%95%8C%E5%A4%B1%E4%B8%9A%E7%8E%87%E6%8E%92%E8%A1%8C%2F</url>
    <content type="text"><![CDATA[来看看倒数第一是谁，哈哈。 失业率排行：South Africa（南非）: 27%Nigeria（尼日利亚）: 23%Spain（西班牙）: 14.7%Turkey（土耳其）: 14.7%Brazil（巴西）: 12.7%Iran（伊朗）: 12.2%Italy（意大利）: 10.2%France（法国）: 8.7%Egypt（埃及）: 8.1%Pakistan（巴基斯坦）: 5.9%Canada（加拿大）: 5.7%Australia（澳大利亚）: 5.2%Indonesia（印度尼西亚）: 5%Russia（俄国）: 4.7%UK（英国）: 3.8%US（美国）: 3.6%India（印度）: 3.5%Germany（德国）: 3.2%Mexico（墨西哥）: 3.2%Japan（日本）: 2.5% 此列不包含一些发达国家和未公布失业率的国家。]]></content>
      <categories>
        <category>世界排名</category>
      </categories>
      <tags>
        <tag>失业率</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Starbucks]]></title>
    <url>%2F2019%2F05%2F14%2FStarbucks%2F</url>
    <content type="text"><![CDATA[Starbucks，founded at March 31, 1971. It is an American coffee company and coffeehouse chain. 2018年全球员工总数：29.1万 在欧洲的门店数量： 英国：1,030土耳其：470法国：175德国：168西班牙：154俄罗斯：135荷兰：106爱尔兰：82波兰：72瑞士：65罗马尼亚：46捷克共和国：40希腊：31匈牙利：28比利时：26挪威：23葡萄牙：23奥地利：19 （星巴克） 星巴克，一个卖咖啡的，不仅给了我们咖啡、空间，就业也是它对社会的价值。]]></content>
      <categories>
        <category>世界排名</category>
      </categories>
      <tags>
        <tag>Starbucks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[美国软件巨头Oracle简介]]></title>
    <url>%2F2019%2F05%2F11%2F%E7%BE%8E%E5%9B%BD%E8%BD%AF%E4%BB%B6%E5%B7%A8%E5%A4%B4Oracle%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[甲骨文公司（英语：Oracle，NASDAQ：ORCL）是一间全球性的大型企业软件公司。总部位于美国加州红木城的红木岸（Redwood Shores），现时首席执行官为公司创办人劳伦斯·埃里森（Lawrence J. Ellison）。甲骨文是继微软后，全球收入第二多的软件公司。随着中美之间的贸易摩擦升级，美国软件巨头Oracle公司撤离中国区研发中心，只留下销售部门。这个软件巨头什么来历呢？ 发展历史 1977年劳伦斯·埃里森、鲍勃·迈纳（Bob Miner）、埃德·奥茨（Ed Oates）在美国加州资成立公司，名为软件发展实验室（Software Development Laboratories，SDL)。其中创始人拉里·埃里森以670亿美元的身价排名全球第六。1978年，开发出第一版甲骨文系统（Oracle），以汇编语言写成；1979年，更名为关连式软件公司（Relational Software, Inc.，RSI)。1982年，推出甲骨文系统，公司也更名为甲骨文系统公司（Oracle Systems Corporation）；2016年，每年的研发投入$22亿美金，应用软件收入$70亿美金，中间件收入$10亿美金。30,000应用软件客户，30,000中间件客户，270,000数据库客户。Oracle 在云端 SaaS 上的收入已为全球最大。 产品 主要分两类：1.服务器及工具 数据库服务器：12c 应用服务器：Oracle WebLogic Application Server 开发工具：Oracle JDeveloper，Oracle Designer，Oracle Developer，等等 2.应用软件 应用软件包与2010年9月20日甲骨文OpenWorld大会上推出的Oracle Fusion Application，一个全面的模块化的应用包； 企业资源计划（ERP）软件。已有10年以上的历史。2005年，并购了开发企业软件的仁科软件公司以增强在这方面的竞争力； 客户关系管理（CRM）软件。自1998年开始研发这种软件。2005年，并购了开发客户关系管理软件的希柏软件公司（Siebel）； 人力资源管理（HCM），收购了仁科（PeopleSoft）软件； 操作系统SolarisOracle Linux 虚拟技术Oracle VMVirtualBox Java平台JavaGlassFish（Sun Java System Application Server）WebLogic 数据库管理系统Oracle数据库Berkeley DBMySQLJava DB 云计算Oracle Cloud下图是Oracle Cloud在全球市场份额占比： 其它软件NetBeansSun Grid EngineSun Studio 都有自己的操作系统了，真的厉害]]></content>
      <categories>
        <category>公司</category>
      </categories>
      <tags>
        <tag>软件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[全球各国人均GDP排行]]></title>
    <url>%2F2019%2F05%2F07%2F%E5%85%A8%E7%90%83%E5%90%84%E5%9B%BD%E4%BA%BA%E5%9D%87GDP%E6%8E%92%E8%A1%8C%2F</url>
    <content type="text"><![CDATA[在这个世界上，人均GDP的提高，不仅需要天时地利，更重要的是人和。来看看全球各国人均GDP排行吧，并介绍下人均GDP最高的国家：卢森堡大公国。 2018年度全球各国人均GDP排名如下：来介绍下人均收入最高的卢森堡大公国吧： 基本信息 语言：卢森堡语、德语、法语 首都：卢森堡市 货币：欧元 国土面积：0.25万平方公里 简介卢森堡是欧盟成员国，因境内有欧洲法院、欧洲审计院、欧洲投资银行等多个欧盟机构被称为继布鲁塞尔和斯特拉斯堡之后的欧盟“第三首都”。卢森堡实行君主立宪制。 国家元首为卢森堡大公，也是目前欧洲唯一的一个大公国。而行政权则由内阁行使。国会共有60个席位，议员任期为5年。 地理位置卢森堡位于西欧内陆，地势北高南低，东邻德国，南接法国，北部和西部同比利时接壤。北部为阿登高原，森林茂密，南部为丘陵。气候温和，属温带海洋性气候，风景优美。首都卢森堡城有“花都”之称。铁矿丰富。这里也是中世纪的要塞。最高点为布尔格普拉兹峰，海拔约550米。 经济自1999年以来，卢森堡一直是欧元区的一部分。卢森堡的经济过去以工业为主，现在卢森堡则是全球最大的金融中心之一。卢森堡是欧元区内最重要的私人银行中心及全球第二大的投资信托中心（仅次于美国）。 1）银行：仅次于美国的世界第二、欧洲最大的基金管理中心； 2）阿塞洛尔—米塔尔集团（Arcelor-Mittal）：卢第一大企业，世界第一大钢铁集团； 3）欧洲卫星公司（SES GLOBAL）：成立于1985年，拥有卫星数量52颗，居欧洲首位、世界第二，其卫星信号全球覆盖率达99.999%。1.22亿欧洲家庭可接收该公司卫星转播的2400套电视、电台节目； 4）卢森堡货运航空公司（Cargolux Airlines International）：成立于1970年，是欧洲最大全货运航空公司，拥有波音747货机26架，员工1856人，航线90多条，覆盖全球50多个国家和地区； 5）卢森堡广播电视公司（RTL）：该公司系卢与德 [5] 国联合组建的欧洲最大的视听媒体集团，拥有40个电视台和33个广播电台。 教育教育体制中卢、德、法三语循序渐进，并行不悖。小学低年级用卢森堡语授课，高年级开始用德语讲习，中学开始再转化成法语。熟练掌握这三门语言是当地中学毕业的必要条件。 人种卢森堡的外国侨民特别多，占全国人口的三成以上，最大的移民团体是葡萄牙人和意大利人。他们也同时带来了自己的语言。不过，葡萄牙语和意大利语基本只限于移民团体内部交流，在大范围内运用并不广泛。 宗教多数信奉天主教，亦有部分信奉其他宗教（包含基督新教和犹太教）。]]></content>
      <categories>
        <category>世界排名</category>
      </categories>
      <tags>
        <tag>GDP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[截至2019年5月5日世界上最富有的人排行]]></title>
    <url>%2F2019%2F05%2F05%2F%E6%88%AA%E8%87%B32019%E5%B9%B45%E6%9C%885%E6%97%A5%E4%B8%96%E7%95%8C%E4%B8%8A%E6%9C%80%E5%AF%8C%E6%9C%89%E7%9A%84%E4%BA%BA%E6%8E%92%E8%A1%8C%2F</url>
    <content type="text"><![CDATA[截至2019年的最富有的人。（以十亿美元计） 马云在2018年7月已跌出亚洲首富，现为印度人穆克什·安巴尼，世界排名如下： 12345678910🇺🇸杰夫·贝索斯：161 亚马逊 电子商务 美国🇺🇸比尔·盖茨：102 微软 软件 美国🇫🇷伯纳德·阿诺特：94 LVMH集团总裁 奢侈品 法国🇺🇸沃伦·巴菲特：90 伯克希尔哈撒韦 投资、咨询 美国🇺🇸马克·扎克伯格：73 Facebook 社交 美国🇺🇸拉里·埃里森：67 甲骨文Oracle 软件服务 美国🇪🇸阿曼西奥·奥特加：66 Zara 服装零售 西班牙🇲🇽卡洛斯·斯利姆：61 卡尔索集团 商业、电信 墨西哥🇮🇳穆克什·安巴尼：56 信诚工业集团 商业 印度🇺🇸迈克尔·布隆伯格：55 彭博 媒体、慈善 美国]]></content>
      <categories>
        <category>世界排名</category>
      </categories>
      <tags>
        <tag>世界财富排名</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[全球最受欢迎的无广告网站-维基百科]]></title>
    <url>%2F2019%2F05%2F04%2F%E5%85%A8%E7%90%83%E6%9C%80%E5%8F%97%E6%AC%A2%E8%BF%8E%E7%9A%84%E6%97%A0%E5%B9%BF%E5%91%8A%E7%BD%91%E7%AB%99-%E7%BB%B4%E5%9F%BA%E7%99%BE%E7%A7%91%2F</url>
    <content type="text"><![CDATA[全球最受欢迎的网站排行，前三当属Google、YouTube、Facebook，没有广告、最受欢迎的网站呢，当属wikipedia莫属。没有广告的维基百科，收入从哪里来呢，谁在运营？ 下面来看看该网站的基本信息： 创建日期：2001-01-15创始人：吉米·威尔士与拉里·桑格持有者：维基媒体基金会（非营利组织）总部：美国网站类型：自由内容、自由编辑的网络百科全书名称来源：Wikipedia是混成词，分别取自于网站核心技术“Wiki”以及英文中百科全书之意的“encyclopedia”语言：301种官方网站：维基百科 哈哈，你能访问吗？ 根据知名的Alexa Internet其网络流量统计数字指出全世界总共有近3.65亿名民众使用维基百科，且维基百科也是全球浏览人数排名第五高的网站，同时也是全世界最大的无商业广告的网站。 目前网站运营资金来源于捐款，在wikipedia18岁生日的时候，Google捐款310万美元，10年接收捐款总额超过7500万美元。]]></content>
      <categories>
        <category>世界排名</category>
      </categories>
      <tags>
        <tag>维基百科</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[区块链三大公链Dapp平台ETH、EOS、TRON对比]]></title>
    <url>%2F2019%2F04%2F27%2F%E5%8C%BA%E5%9D%97%E9%93%BE%E4%B8%89%E5%A4%A7%E5%85%AC%E9%93%BEDapp%E5%B9%B3%E5%8F%B0ETH%E3%80%81EOS%E3%80%81TRON%E5%AF%B9%E6%AF%94%2F</url>
    <content type="text"><![CDATA[区块链三大公链Dapp平台ETH、EOS、TRON对比，根据创始人经历、平台共识机制、平台发展历程、目前发展现状等方面进行对比，寻找最有可能实现未来去中心化操作系统的平台。 一、创始人1.ETH以太坊创始人：维塔利克·布特林（Vitalik Buterin） 出生日期：1994年1月31日 国籍：俄罗斯裔加拿大人 学历：加拿大滑铁卢大学肄业 区块链经历：2012年17岁时从他父亲那里了解了比特币，开始研究比特币、为比特币杂志写文章转比特币稿费，当时出版社给他一篇文章5个比特币；2013年18岁时获得奥林匹亚资讯奖铜牌，经常去访问其他国家的比特币社区开发人员，讨论比特币的发展与问题；2014年19岁自加拿大滑铁卢大学休学；该年11月，公布《以太坊白皮书》初版，开始募集开发者；2015年20岁获得硅谷知名的亿万富翁设立的泰尔奖学金10万美元成立非营利组织以太坊基金会，全职在以太坊工作；在迈阿密的比特币会议公开发表以太坊计画，该年7月，启动以太坊计画众售募资，募得3.1万枚比特币（当时约合1840万美元）2016年21岁以太坊最初版本Frontier问世、以太币开始在世界各地交易所公开交易2017年22岁被《财星》杂志评选为2016年40岁以下的40大杰出人物 2.EOS柚子创始人：丹尼尔·拉里默（Daniel Larimer） 出生日期：未查到 国籍：美国 学历：2003年毕业于佛吉尼亚大学计算机系本科学士学位 区块链经历：2009年对比特币感兴趣，开始了解；2013年创建BitShares去中心化交易所，是一个拥有钱包, 账本, 交易所, 货币系统，社群与一身的产品，与之对应的是BTS(比特股)虚拟货币的发行，目前市值排名51；2016年离开BitShares创建Steem区块链平台和利用区块链技术实现的社交app steemit，该平台可以对用户的创作予以代币奖励；项目完成后，2017年7月发布EOS白皮书，提供分散式应用程序托管﹑智能合约功能与分散式储存的企业方案，解决比特币和以太坊等区块链的可扩展性问题，并消除用户的交易费用。成立了Block.One公司并搭建了EOSIO平台，并发行以ERC-20方式发行1亿枚EOS代币 3.TRON波场创始人：孙宇晨 出生日期：1990年 国籍：中国 学历：北京大学、宾夕法尼亚大学硕士 区块链经历：2013年以前投资比特币，获得二十倍以上收益；由于比特币的投资经历，孙宇晨活跃于美国比特币社区，并对加密货币，去中心化清算协议产生了极其浓厚的兴趣。经过长期调查与研究，他对于诞生于加州硅谷的全球第一个分布式清算支付网络协议——Ripple协议产生了极其浓厚的兴趣。2013年底加入RippleLabs，成为Ripple协议缔造者与研发者中的一员；2014年，他回国创立锐波并兼任CEO，锐波也成为中国首家从事去中心化清算系统产品开发的互联网科技公司；2017年，孙宇晨在“世界区块链峰会上”发表《From it to bit》主题演讲，讲述了互联网的发展历史，阐释了web 4.0的观点。7月，随后推出了自己所做的项目：波场TRON，发布波场白皮书，基于区块链的开源去中心化内容娱乐协议，致力于利用区块链与分布式存储技术，构建一个全球范围内的自由内容娱乐体系，这个协议可以让每个用户自由发布，存储，拥有数据，并通过去中心化的自治形式，以数字资产发行，流通，交易方式决定内容的分发、订阅、推送，赋能内容创造者，形成去中心化的内容娱乐生态。2018年7月，完成了对于BitTorrent及其旗下所有产品的收购，并将其并入到波场生态中。 二、区块链共识机制先来介绍下三种共识机制的概念 POW：Proof Of Work工作量证明机制：通过计算机随机不停地计算得到指定hash值后获得记账权，并将区块链接到区块链上的机制，每个获得记账权的矿工会获得一定的代币，作为记账的奖励，这个过程俗称挖矿。 POS：Proof Of Stake权益证明机制：人们对于POW日趋中心化的算力分布(矿池)心怀忌惮之际，产生了权益证明机制，即对于验证人/节点的奖励，不是通过算力挖矿，而是通过持币而产生利息，这里就要引入一个概念叫做—币龄，币龄=币量x持有天数。这是根据你持有货币的量和时间，给你发利息的一个制度。当你获得了利息以后，你的所有币龄将被清空，你的持币时间将从0重新算起。 DPOS：Delegated Proof Of Stake委托权益证明机制：可以说DPOS是POS共识机制理念的一个变种。先通过选举，产生若干超级节点；后续记账权将以相同概率分配于超级节点中。它有点像是议会制度或人民代表大会制度。如果代表不能履行他们的职责(当轮到他们时，没能生成区块)，他们会被除名，网络会选出新的超级节点来取代他们。DPOS让每一个持有代币的人都有权利通过投票给验证人的方式行使自己的权利，利用科技的手段实现民主治理。 1.ETH 第一阶段，边境（Frontier，2015年7月）以太坊的第一次版本发布，允许开发人员对以太坊进行挖矿，并基于以太坊进行 DApp 与工具软件的开发。 第二阶段，家园（Homestead，2016年3月）发布了第一个生产环境版本，对许多协议进行了优化改进，为之后的升级奠定了基础，并且加快了交易速度。 第三阶段，大都会（Metropolis，2017年10月）第三阶段分为两次升级，分别命名为拜占庭（Byzantium，2017年10月）和君士坦丁堡（Constantinople，2019年1月），将会使得以太坊更轻量、更快速、更安全。 第四阶段，宁静（Serenity，时间待定）这个版本将会使用期待已久的 PoS 共识，其中将会使用 Casper 共识算法。 目前第三阶段已升级完成，所以ETH仍然使用POW共识机制。 2.EOSDPOS目前已选出21个超级节点进行选举出块。 3.TRONDPOS 第一阶段：Exudos，出埃及记数据自由-基于点对点的分布式的内容上传、存储和分发机制。出埃及记阶段，波场（TRON）将建立在以IPFS为代表的分布式存储技术之上，为用户提供一个可以完全自由可依赖的数据发布，存储，传播平台。 第二阶段：Odyssey，奥德赛（2019年1月-2020年6月，2019年5月发布2.0，9月发布3.0）内容赋能-经济激励赋能内容生态。区块链技术，将为内容产生，分发，传播建立一整套充分竞争、回报公平的经济机制，激励个体，赋能内容，从而不断拓展系统的边界。 第三阶段：Great Voyage（2020年7月-2021年7月）伟大航程，人人发行数字价值。波场（TRON）基于区块链的优势，解决了收益衡量、红利发放和支持者管理三大难题，实现了从“粉丝经济”向“粉丝金融”的重大转变，波场（TRON）基于区块链以波场币（TRX）为官方代币的自治经济体系使得个人内容生产者在体系内的每一笔收入和支出都公开、透明且不可篡改，通过智能合约，支持者们可以自动参与内容生产者的数字资产购买并按照约定自动共享红利成长，不需要任何第三方进行监督即可公正地完成全部流程。 第四阶段：Apollo，阿波罗（2021年8月-2023年3月）价值自由流动-去中心化的个体专属代币交易。当每一个波场（TRON）体系内的内容生产者都可以发行自己的专属代币，则系统必须拥有一整套完整的去中心化交易所解决方案，方能实现价值的自由流动。 第五阶段：Star Trek，星际旅行（2023年4月-2025年9月）流量变现-去中心化的博弈与预测市场。全球博弈市场规模2014年超过4500亿美元。波场（波场（TRON））内容平台所带来的流量，为构建去中心化的线上博弈平台提供了可能。开发者可以通过波场（TRON）自由搭建线上博弈平台，提供全自治的博弈预测市场功能。 第六阶段：Eternity，永恒之地（2025年10月-2027年9月）流量转化-去中心化的游戏。2016年，全球电子游戏市场规模达996亿美元，其中手机游戏市场规模461亿美元，占比42%。波场（波场（TRON））为构建去中心化的线上游戏平台提供了可能。开发者可以通过波场（TRON）自由搭建游戏平台，实现游戏开发众筹，并为普通投资者提供参与投资游戏的可能。 目前第二阶段升级到1.0版本。 平台功能ETH1.Smart Contract：智能合约2.EVM：以太坊虚拟机，提供智能合约运行的分布式区块链环境3.ICO：发币融资（如：BNB）4.DAPPs5.转账 EOS1.Smart Contract：智能合约2.ICO：发币融资3.DAPPs4.转账 TRON1.内容上传、存储和分发2.给予内容创作者奖励3.ICO（未实现）4.去中心化的博弈与预测市场（未实现）5.去中心化游戏（未实现）6.DAPPs7.转账 三、平台目前发展现状实时数据据 DAppTotal4月29日数据显示，过去一周，综合对比 ETH、 EOS、 TRON四大公链的 DApp生态情况发现： 总用户量(个)EOS(292,337)&gt; TRON(87,261)&gt; ETH(31,678)； 总交易次数(笔)EOS(26,393,841)&gt; TRON(9,856,747)&gt; IOST(2,360,126)&gt; ETH(373,918)； 总交易额(美元)EOS(144,852,700)&gt; TRON(88,426,176)&gt; ETH(39,182,195)； 跨四条公链按用户量 TOP3 DAppsEOS Global(EOS)、 Endless Game(EOS)、 Lore Free(EOS)； 按交易次数 TOP3 DAppsHash Baby(EOS)、 TRONbet(TRON)、 Dice(EOS)； 按交易额 TOP3 DAppsTRONbet(TRON)、 EOS Global(EOS)、 TronWoW(TRON)。 统计数据尴尬，这个网站：State of Dapps 没有统计TRON平台的数据。 转账交易速度TPS由于平台共识机制不同，导致去中心化程度、运行速度也不同，下面是几大平台交易速度：其中EOS，TPS可达3500.比其他几大平台都快。 这些是目前公共区块链平台的一些基本信息，希望对大家有用，如有错误的地方还请指正，联系方式：chenzuoli709@163.com，一起交流学习。]]></content>
      <categories>
        <category>数字货币</category>
      </categories>
      <tags>
        <tag>Dapp</tag>
        <tag>区块链</tag>
        <tag>公链</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[世界上钻石价格排行]]></title>
    <url>%2F2019%2F04%2F27%2F%E4%B8%96%E7%95%8C%E4%B8%8A%E9%92%BB%E7%9F%B3%E4%BB%B7%E6%A0%BC%E6%8E%92%E8%A1%8C%2F</url>
    <content type="text"><![CDATA[世界上最贵的钻石，在哪里呢，来看看。 1.Koh-i-Noor: 现在英国，产地印度，21.12 g，无价；2.The Sancy: 现在印度，11.046 g，无价3.The Cullinan: 现在英国，产地南非，1905年被发现时621.35g，后来被拆分成105颗，价值4亿美元；4.The Hope Diamond: 现在美国，产地印度9.11g，价值3.5亿美元；5.Millennium Star: 属于戴比尔斯集团，产地扎伊尔，40.6.8g，价值1.29亿美元；6.Centenary Diamond: 属于戴比尔斯集团，产地南非，54.77g，价值1亿美元；7.Pink Star: 属于戴比尔斯集团，产地南非，11.92g，价值7千万美元；8.The Regent Diamond: 现在法国，产地印度，28.12g，价值6200万美元；]]></content>
      <categories>
        <category>世界排名</category>
      </categories>
      <tags>
        <tag>钻石</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[全球奢侈珠宝品牌排名]]></title>
    <url>%2F2019%2F04%2F24%2F%E5%85%A8%E7%90%83%E5%A5%A2%E4%BE%88%E7%8F%A0%E5%AE%9D%E5%93%81%E7%89%8C%E6%8E%92%E5%90%8D%2F</url>
    <content type="text"><![CDATA[珠宝首饰一直是人们爱美时的装饰品，一直都是如此，下面来看下全球珠宝首饰排名，了解世界品牌。 10 萧邦choppard国家：瑞士创建日期：1860年创始人：路易•尤利斯•萧邦简介：除了制作奢华的瑞士手表外，萧邦的房子也以其奢华的珠宝系列而闻名。 Chopard的日常珠宝系列仅采用最优质的材料制成，采用厚度为18K的黄金和最高等级的宝石制成。不仅如此，萧邦还非常注重细节和精确度，为其已经很昂贵的产品线增加了更多价值。 9 Mikimoto御木本国家：日本创建日期：1893年创始人：御木本幸吉简介：Mikimoto的创始人Kokichi Mikimoto不仅因其收藏而闻名，而且因为他发明并传播了使用养殖珍珠制作珠宝首饰的事实。 Mikimoto的系列仅选用最好的珍珠，包括南海珍珠，粉红海螺珍珠，大溪地珍珠，白珍珠和其他稀有标本。最重要的是，Mikimoto的珠宝系列仅使用18k金和铂金作为金属部件和顶级钻石。只有最好的丝线用于有珍珠串的首饰。 8 Bvlgari宝格丽国家：意大利创建日期：1884年创始人：索帝里欧·宝格丽简介：宝格丽毫无疑问是一个着名的奢侈品牌，从时装到手表再到珠宝。而对于后者而言，这个以罗马为基础的品牌将优雅和奢侈品完美结合，并且不失其对传统的偏好。即使在今天，Bvlagri的系列仍然标榜着该房子的标志性特征，包括用于中心件的大型宝石，大胆的形状以及凸圆形宝石的使用（这一传统可追溯到1960年代的意大利魅力）。除了最好的宝石外，宝格丽仅使用18K黄金作为其收藏品。 7 伯爵伯爵国家：瑞士创建日期：1874创始人：乔治．伯爵简介：Piaget是普通人的另一个熟悉的名字，最初是作为汝拉瑞士部分的制表公司开始的。随着业务的增长，该公司很快就进入了珠宝行业，并在其中脱颖而出，为那些能够负担公司要求的价格的人们制作奢侈品。今天，Piaget以旧世界概念和现代设计相结合为荣，现在设计时尚线条和大胆的角度。但其最着名的外观是玫瑰，它已成为伯爵的标志性设计。 6 Graff格拉夫国家：英国创建日期：1960年创始人：劳伦斯·格拉夫简介：格拉夫是顶级品牌，在富人和精英中非常受欢迎。使Graff的系列与众不同的不仅仅是用于制作昂贵单品的宝石和金属的工艺或质量。相反，它是Graff在其珠宝系列中使用的宝石的大小。他们是巨大的，格拉夫的创始人劳伦斯格拉夫喜欢这样。 5 Tiffany＆Co。蒂芙尼国家：美国创建日期：1837年创始人：查理斯·路易斯·蒂芙尼和泰迪·杨简介：甚至大众都知道蒂芙尼在珠宝方面是一个巨大的奢侈名称，主要是因为他们的产品线包括日常穿着的件，无论什么场合。他们广泛的收藏不仅限于女性，蒂芙尼也适合男性和儿童。自1837年开始运营以来，Tiffany的创作产生了经典设计，由专业工匠制作。那些被归类为超豪华的人往往需要数年才能完成。 4 Buccellatibucellati国家：意大利创建日期：1919年创始人：Mario Buccellati简介：Buccellati用最好的意大利金制作优雅的珠宝，彰显罗马的传统。这家总部位于罗马的珠宝公司制作了罗马风格的设计，并将其融入其收藏中。罗马风格的项链和手镯袖口只是他们最畅销的一些。 Buccellati也为能够提供某些设计而感到自豪，这些设计使其珠宝具有非常吸引人的外观，如使用高品质的宝石和钻石刷金属和哑光，以及厚重的结壳。 3 Van Cleef＆Arpels梵克雅宝国家：法国创建日期：1896年创始人：Alfred Van Cleef和Salomon Arpels简介：当Estelle Arpels和Alfred Van Cleef决定将他们的合作作为永久性安排时，Van Cleef＆Arpels成立。虽然它的大部分系列都展现了旧世界物品中的优雅风格，但它还有其他系列产品，散发着自己的风格和阶级。这座房子展示了一个庞大的系列，融合了传统和叙事风格以及技术专长。 2卡地亚卡地亚国家：法国创建日期：1847年创始人：路易-弗朗索瓦·卡地亚简介：列表中的另一个家喻户晓的名字，卡地亚是一个已存在多年的名字。 卡地亚成立于1860年，一直是皇室成员的珠宝商，他们希望拥有个性化的系列。 黑豹是卡地亚最知名的设计，经过不断的修改和重新概念化，以吸引客户不断变化的品味。 卡地亚以其装饰艺术历史而闻名，但也创造了几条线条来庆祝旧世界的优雅。 1 Harry Winston温斯顿国家：美国创建日期：1932创始人：哈里温斯顿简介：一个在珠宝行业引起共鸣的名字，Harry Winston于1932年开始创业，并一直处于领先地位。 Harry Winston的系列仅使用最好的宝石和最好的金属，仅由珠宝工艺大师设计。 Harry Winston家居的产品不仅优雅而奢华，而且耐用，并且很容易通过时间的考验。]]></content>
      <categories>
        <category>世界排名</category>
      </categories>
      <tags>
        <tag>珠宝</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[以太坊（Ethereum）简介]]></title>
    <url>%2F2019%2F04%2F23%2F%E4%BB%A5%E5%A4%AA%E5%9D%8A%EF%BC%88Ethereum%EF%BC%89%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[以太坊是一个开源的有智能合约功能的公共区块链平台。通过其专用加密货币以太币（Ether，又称“以太币”）提供去中心化的虚拟机（称为“以太虚拟机”Ethereum Virtual Machine）来处理点对点合约。 以太坊（Ethereum）简介 概念：是一个开源的有智能合约功能的公共区块链平台。通过其专用加密货币以太币（Ether，又称“以太币”）提供去中心化的虚拟机（称为“以太虚拟机”Ethereum Virtual Machine）来处理点对点合约。 以太坊的概念首次在2013至2014年间由程序员维塔利克·布特林受比特币启发后提出，大意为“下一代加密货币与去中心化应用平台”，在2014年透过ICO众筹得以开始发展。 截至2018年6月，以太币是市值第二高的加密货币，以太坊亦被称为“第二代的区块链平台”，仅次于比特币。 创始人Vitalik Buterin（V神）国籍：俄裔加拿大人出生日期：1994年1月31日事迹：以太坊创始人、以太坊白皮书作者 特点1.智能合约（smart contract）存储在区块链上的程序，由各节点运行，需要运行程序的人支付手续费给节点的矿工或权益人。 2.代币（tokens）智能合约可以创造代币供分布式应用程序使用。分布式应用程序的代币化让用户、投资者以及管理者的利益一致。代币也可以用来进行首次代币发行。 3.叔块（uncle block）将因为速度较慢而未及时被收入母链的较短区块链并入，以提升交易量。使用的是有向无环图的相关技术。 4.权益证明（proof-of-stake）相较于工作量证明更有效率，可节省大量在挖矿时浪费的计算机资源，并避免特殊应用集成电路造成网络中心化。 5.支链（Plasma）用较小的分支区块链运算，只将最后结果写入主链，可提升供单位时间的工作量。 6.状态通道（state channels）原理类似比特币的闪雷网络，可提升交易速度、降低区块链的负担，并提高可扩展性。尚未实现，开发团队包括雷电网络（Raiden Network）和移动性网络（Liquidity Network）。 7.分片（sharding）减少每个节点所需纪录的数据量，并透过平行运算提升效率。 8.分布式应用程序以太坊上的分布式应用程序不会停机，也不能被关掉。 发展历程1.激活：边境以太坊的公共区块链在2015年7月30日引导。最初的以太坊版本称为边境（Frontier，也有“前锋”的意思），用的是[工作量证明]（proof-of-work）的算法，目前转换成[权益证明]（proof-of-stake）。 2.硬分叉自最初版本以来，以太坊网络成功进行了数次硬分叉。第一次分叉调整了未来挖矿的难度，确保未来的用户会有转换至权益证明的动机。当前第五个分叉正在开发中。 3.第二次分叉：家园2016年春季进行了第二次分叉，发布了第一个稳定版本，称作“家园”（Homestead）。 4.第三次分叉：DAO和区块链分叉2016年六月，以太坊上的一个去中心化自治组织被骇，造成市值五千万美元的以太币被移动到只有该黑客可以控制的“分身DAO”。因为程序不允许黑客立即提取这些以太币，以太坊用户有时间讨论如何处理此事，考虑的方案包括取回以太币和关闭DAO，而DAO去中心化的本质也表示没有中央权力可以立即反应，而需要用户的共识。最后在2016年7月20日，以太坊进行硬分叉，作出一个向后不兼容的改变，让所有的以太币（包括被移动的）回归原处，而不接受此改变的区块链则成为古典以太坊。这是第一次有主流区块链为了补偿投资人，而透过分叉来更动交易记录。 在这次分叉之后，造成了在两个区块链之间进行重放攻击的可能，加上其他网络攻击，让以太坊和古典以太坊又各自进行了数次分叉来避免攻击。 5.第四次分叉：减重和防DDoS2016年11月底进行了第四次的分叉。这次分叉为区块链减重（de-bloat），并加入一些避免网络攻击的设计。因为沟通疏失，这次分叉短暂造成以太坊的两个主要客户端程序 Parity 和 Geth 失去共识而产生意外的分叉，但问题在数小时内即被找出并修正。 发展与挑战2018年9月，比特币核心开发者Jeremy Rubin在美国科技媒体TechCrunch上发表文章《ETH的崩溃无法避免》，称就算以太坊网络继续存续，ETH的价值也会必然归零。以太坊创始人Vitalik在回应中承认了问题的存在：“如果以太坊不改变，Jeremy Rubin的言论可能是对的”。此番言论造成ETH的价钱一度下挫。同时，许多以太坊的项目开始转移到EOS﹑波场等的其他公链上，有人担心以太坊将被取代。在ETH的价格影响下，以太坊的全网算力开始收缩，按etherscan.io的算力统计显示，9月到11月以太坊全网算力下跌了20％，从近300TH/s收缩至240TH/s。 2018年12月10日，Vitalik在推特上宣称，未来采用基于[权益证明 (PoS)]的分片技术的区块链“效率将提高数千倍”。 2019年，以太坊项目进行君士坦丁堡硬分叉，这是一个刺激以太坊网络改变其核心共识机制算法的代码，这一段代码引导之后以太坊便会面临所谓的“冰河时代”，在该网络上的创建新区块的难度将会不断提升，最终减慢到完全停止。在该硬分叉升级之后，以太坊区块链的状态将“永久性”的改变。]]></content>
      <categories>
        <category>数字货币</category>
      </categories>
      <tags>
        <tag>以太坊</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[人们最有可能帮助陌生人的十大国家]]></title>
    <url>%2F2019%2F04%2F23%2F%E4%BA%BA%E4%BB%AC%E6%9C%80%E6%9C%89%E5%8F%AF%E8%83%BD%E5%B8%AE%E5%8A%A9%E9%99%8C%E7%94%9F%E4%BA%BA%E7%9A%84%E5%8D%81%E5%A4%A7%E5%9B%BD%E5%AE%B6%2F</url>
    <content type="text"><![CDATA[2018年, 人们最有可能帮助陌生人的十大国家: 排名如下： 利比亚: 83% 伊拉克: 81% 科威特: 80% 利比里亚: 80% 塞拉利昂: 80% 巴林: 74% 冈比亚: 74% 沙特阿拉伯: 74% 肯尼亚: 72% 美国: 72% (World Giving Index)]]></content>
      <categories>
        <category>世界排名</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[2018年全球道路质量排行]]></title>
    <url>%2F2019%2F01%2F10%2F%E9%81%93%E8%B7%AF%E8%B4%A8%E9%87%8F%E6%8E%92%E8%A1%8C%2F</url>
    <content type="text"><![CDATA[Quality of roads, 2018. (of 140 countries) index as below:1.🇸🇬Singapore 新加坡2.🇨🇭Switzerland 瑞士3.🇳🇱Netherlands 荷兰 6.🇯🇵Japan 日本7.🇫🇷France 法国9.🇦🇪UAE 阿拉伯联合酋长国11.🇺🇸USA 美国13.🇪🇸Spain 西班牙19.🇩🇪Germany 德国25.🇨🇦Canada 加拿大26.🇬🇧UK 英国33.🇹🇷Turkey 土耳其42.🇨🇳China 中国51.🇮🇳India 印度69.🇵🇰PAK 巴基斯坦104.🇷🇺Russia 俄国110.🇲🇳Mongolia 蒙古112.🇧🇷Brazil 巴西132.🇳🇬Nigeria 尼日利亚]]></content>
      <categories>
        <category>世界排名</category>
      </categories>
      <tags>
        <tag>道路质量</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[全球数字货币市值排名第三瑞波币XRP介绍]]></title>
    <url>%2F2019%2F01%2F06%2F%E7%91%9E%E6%B3%A2%E5%B8%81%EF%BC%88XRP%EF%BC%89%2F</url>
    <content type="text"><![CDATA[下面介绍瑞波币（XRP）的信息，希望对大家数字货币及投资有帮助。 概述： 旨在消除比特币对集中交换的依赖，比比特币使用更少的电力，并且比比特币更快地执行交易； Ripple加密货币协议于2012年推出，其主要目标是确保“任何规模的安全，即时和几乎免费的全球资金运营，无需任何退款”。该协议支持使用法定货币，加密货币，货物或任何其他单位（如旅客奖励里程或移动会议纪要）付款。 Ripple数字货币系统确认交易不是采矿，而是网络参与者的共识。 这种方法消除了对比特币中使用的集中交换的依赖。 Ripple也比比特币使用更少的电力，而交易执行得更快。 创始人： Ripple coin于2004年由加拿大温哥华市的网络开发人员Ryan Fugger首次实施跨境支付； 2005年，Fugger开始将Ripplepay建成金融服务，通过全球网络为在线社区成员提供安全支付选项； 在此协议的基础上，2011年5月出现了一种新的数字货币系统，其中发布了自己的加密货币XRP； 发行量：1000亿枚 发行时间：2011年 用途： 在Ripple中，用户通过使用以任意现实世界资产（美元，黄金，飞行里程等）计价的加密签名交易在他们之间进行支付。 为此，Ripple保留了一个分类账，用于记录彼此信任的用户之间的债务。 通过这种方式，所有资产都表示为债务。 当在彼此信任的两个用户之间进行支付时，根据每个用户设置的限制调整相互信用额度的余额。 为了在没有直接建立信任关系的用户之间发送资产，系统尝试在两个用户之间找到路径，使得路径的每个链接在具有信任关系的两个用户之间。 然后沿路径平衡所有，同时原子性调整。 客户：欧洲进出口银行、SendFriend、JNFX、FTCS、科威特Ahli银行、Transpaygo、BFC Bahrain、ConnectPay、GMT、WorldCom Finance、Olympia Trust company、Pontual/USEND和Rendimento等200家商业银行和金融机构。 事件： 1.2011-04-18，瑞波币是Ripple网络的基础货币，它可以在整个ripple网络中流通，总数量为1000亿 2.2015-01-20，Ripple Labs任命前白宫顾问Gene Sperling为董事 Sperling表示：“我很高兴加入Ripple Labs，他们的使命是通过一个通用的互联网协议大幅提高跨境支付的速度和效率。“与货币无关的Ripple协议是一项独特的技术，可以从根本上改变通信银行业务，并导致实时支付系统。 3.2016-10-20，Ripple和R3在跨境银行支付方面取得突破 该试验发生在旧金山的Ripple和由数十家银行支持的金融创新联盟R3之间。周四发布的公告涉及到12家银行，其中包括巴克莱和BMO,这些银行使用Ripple的货币XRP来为跨境结算提供流动性 4.2017-10-11：瑞波全球支付网络产品签署九个新用户 计划进行跨境资金转移 新成员包括国际支付处理服务商Bexs Banco de Cambio、为优步和GoDaddy提供支付服务的dLocal。目前该网络成员超过100，包括Credit Agricole、Currencies Direct、IFX、TransferGo、Cuallix, Krungsri和Rakbank。 5.2018年2月6日，交易平台BitMEX对外公布一份Ripple调查报告，其中阐述瑞波币早期的分类账本中已遗失32570个区块，无法修复并获得其中的数据，而无法完整审核整个瑞波币区块链和1000亿XRP币的完整路径； 6.2018年2月，西联汇款宣布与Ripple公司合作，测试通过Ripple进行资金交易并实现资本最优化。 7.2018年某段时期一度超越以太坊成为第二大加密货币，目前第三位。]]></content>
      <categories>
        <category>数字货币</category>
      </categories>
      <tags>
        <tag>XRP</tag>
        <tag>瑞波币</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[全球游戏公司游戏营收排行]]></title>
    <url>%2F2019%2F01%2F05%2F%E6%B8%B8%E6%88%8F%E5%B8%82%E5%9C%BA%E8%90%A5%E6%94%B6%E6%8E%92%E5%90%8D%2F</url>
    <content type="text"><![CDATA[游戏市场市场研究机构Newzoo发布2018年全球游戏市场报告，报告中显示，腾讯游戏相关营收增长9%，达到197亿美元，占据全球游戏市场近15%的份额。索尼以142亿美元营收排名第二，微软营收98亿美元排名第三。 游戏营收排名：1.腾讯2.索尼3.微软4.苹果5.动视暴雪6.谷歌7.网易8.EA9.任天堂10.万代南梦宫]]></content>
      <categories>
        <category>世界排名</category>
      </categories>
      <tags>
        <tag>游戏</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[恒星币（XLM）]]></title>
    <url>%2F2019%2F01%2F04%2F%E6%81%92%E6%98%9F%E5%B8%81%EF%BC%88XLM%EF%BC%89%2F</url>
    <content type="text"><![CDATA[下面介绍全球数字货币市值排名第九的恒星币（XLM） 概述：代码是基于瑞波币的基础上修改的。用于搭建一个数字货币与法定货币之间传输的去中心化网关，是一个用于价值交换的开源协议； 该协议由非营利性、非股票型组织因此，基金的创始人无法从其经营或出售其股份中获益； Stellar Development Foundation基金会提供支持； 平台完全对外开源； 该平台承诺发布涵盖其活动的各种报告：关于雇员工资的报告;关于流氓补助金工作人员的报告;预算;分布流明数;流明的分布机制等； 在平台上可以看到发行量，每周产生的流量（每年产生1%的认为通胀），基金的发展将要花多少钱（总数的5%）； 大多数免费发放，5%作为运营使用，25%流向非营利组织； 拥有大量恒星币的组织或个人必须拥有5年以上才能出售其资产； Stellar平台的组织者已经尽一切可能从这个项目中排除任何赌博组件； 2019年3月，IBM发布公告称，包括Banco Bradesco，Bank Rusan和Rizal Commercial Banking Corporation在内的六家国际银行签署了在World Wire上发行自己的稳定币的意向书，这是IBM利用Stellar公共区块链的支付网络。 创始人：瑞波币的前创始人Jed McCaleb，电驴（BT下载软件）的创始人 发行量：1000亿枚，95%用于免费发放。目前流通量约为191亿枚： 50%通过直接分发计划分配给全世界； 25%通过增加覆盖计划分配给非营利组织以给予金融服务匮乏的人群； 20%通过比特币计划分配； 5%留作运营费用恒星币运营。 发行时间：2014年 用途：去中心化网关，通过转换全球各国之间的稳定加密货币的方式，进行跨境支付 客户：IBM、菲律宾的RCBC、巴西的Banco Bradesco、韩国的Bank Busan 2017年10月Stellar宣布与IBM合作，成为IBM Blockchain平台战略的一部分，为跨国界提供更便宜，更快速的支付]]></content>
      <categories>
        <category>数字货币</category>
      </categories>
      <tags>
        <tag>XLM</tag>
        <tag>恒星币</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[德国的世界品牌]]></title>
    <url>%2F2019%2F01%2F04%2F%E5%BE%B7%E5%9B%BD%E7%9A%84%E4%B8%96%E7%95%8C%E5%93%81%E7%89%8C%2F</url>
    <content type="text"><![CDATA[Companies based in Germany:德国的世界品牌： 品牌如下： 1234567891011121314151617181920- Volkswagen 大众 汽车- Allianz 安联 保险- Mercedes Benz 奔驰 汽车- Audi 奥迪 汽车- BMW 宝马 汽车- Porsche 保时捷 汽车- Lufthansa 汉莎 航空- Siemens 西门子 电子电器工程- BASF 巴斯夫 化工- Bayer 拜耳 医疗保健、化工及农业- Fresenius 费森尤斯 医药- Merck 默克 生物科技- Linde 林德 工业机械- ThyssenKrupp Group蒂森克虏伯 工业工程、钢铁- SAP 思爱普 企业管理、咨询- Deutsche Telekom 德国电信 电信- Aldi 阿尔迪 食品连锁超市- Bosch 博世 工业、交通- Lidl 历德 零售（欧洲的沃尔玛）- Adidas 阿迪达斯 服装 你用到了哪些呢？]]></content>
      <categories>
        <category>世界排名</category>
      </categories>
      <tags>
        <tag>德国</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[尼古拉·特斯拉]]></title>
    <url>%2F2019%2F01%2F03%2F%E5%B0%BC%E5%8F%A4%E6%8B%89%C2%B7%E7%89%B9%E6%96%AF%E6%8B%89%2F</url>
    <content type="text"><![CDATA[下面介绍 尼古拉·特斯拉 这个交流电之父。 24岁的特斯拉全无天才的气质，甚至可以说，是一个失败者。1882年，成为一家电话公司的工程师。特斯拉开始展现才华，设计出第一台感应电机模型。1884年，带着前雇主的推荐信，特斯拉第一次来到美国，见到了传说中的爱迪生，成为他的助手。后来辞职后创业。爱迪生是直流电的死忠，特斯拉主推交流电。可以说，直流电与交流电之争决定两家公司的生死。再后来，交流电取代直流电，成为主流，奠定了现代电力的基础。因此，特斯拉，而非爱迪生才是真正的“电气时代之父”。1897年就获得了无线电技术的专利。1898年，特斯拉制造了能产生人工地震的振荡器，在输入频率时，差点将纽约市夷为平地。1899年，特斯拉造出一大堆球状闪电，这是迄今为止，世界上唯一一次在实验室制造出球状闪电。1901年，特斯拉建造了沃登克里弗塔，用于横跨大西洋的无线电能传输实验。1917年，特斯拉就向美国海军提出雷达的概念。特斯拉先于伦琴发现X射线。还发明了遥控器、发动机火花塞、霓虹灯、现代电动机。建立了第一次成功记录接收了来自外太空无线电电波，他在一百多年前就持有晶体管的专利世界上第一个水利发电站——尼亚加拉水电站。现在手机吹嘘的“无线充电技术”，其实是特斯拉100年前玩剩下的。特斯拉的无线照明特斯拉还设计过一种”没有机翼，没有副翼，没有螺旋桨，没有其他外部装置的飞机“。飞行速度极高，完全通过反作用实现续航和驱动。 他每天只睡2个小时，独自取得700多项发明专利，合作开发1000种以上。他被诺贝尔物理学奖提名11次，全部让贤。作为交流电的发明人，一年之内，就可以靠专利费，成为世界首富。他却毅然将“交流电专利”撕毁，免费向社会开放。最终，一生贫困潦倒。1943年，尼古拉·特斯拉在贫穷、孤独中去世。今天，提起特斯拉，大部分人想到的只是一辆电动车，而非一位科学家。]]></content>
      <categories>
        <category>人物</category>
      </categories>
      <tags>
        <tag>特斯拉</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[印度处于领先地位的产品]]></title>
    <url>%2F2019%2F01%2F02%2F%E5%8D%B0%E5%BA%A6%E5%A4%84%E4%BA%8E%E9%A2%86%E5%85%88%E5%9C%B0%E4%BD%8D%E7%9A%84%E4%BA%A7%E5%93%81%2F</url>
    <content type="text"><![CDATA[印度处于领先地位的产品： 香蕉 芒果 番木瓜 柠檬 水牛奶 山羊奶 辣椒 生姜 鹰嘴豆 小米 黄麻 木材燃料 你用过或者吃过吗？]]></content>
      <categories>
        <category>世界排名</category>
      </categories>
      <tags>
        <tag>印度</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[各国诺贝尔奖数]]></title>
    <url>%2F2019%2F01%2F02%2F%E5%90%84%E5%9B%BD%E5%BE%97%E8%AF%BA%E5%A5%96%E6%95%B0%2F</url>
    <content type="text"><![CDATA[Nobel prizes.诺贝尔奖数： 排名如下：US: 377 美国UK: 130 英国Germany: 108 德国France: 70 法国Sweden: 32 瑞典Japan: 27 日本Canada: 26 加拿大Switzerland: 26 瑞士Russia 25 俄国Austria: 21 奥地利Netherlands: 21 荷兰Italy: 20 意大利Poland: 14 波兰Denmark: 13 丹麦Norway: 13 挪威Hungary: 13 匈牙利Australia: 12 澳大利亚Israel: 12 以色列Belgium: 11 比利时India: 10 印度South Africa: 10 南非China: 8 中国Spain: 8 西班牙 中国： 大陆：第十四世达赖喇嘛 刘晓波 莫言 屠呦呦 台湾：丁肇中 李遠哲 李政道 杨振宁]]></content>
      <categories>
        <category>世界排名</category>
      </categories>
      <tags>
        <tag>诺贝尔奖</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019年最健康的国家指数]]></title>
    <url>%2F2019%2F01%2F01%2F%E6%9C%80%E5%81%A5%E5%BA%B7%E7%9A%84%E5%9B%BD%E5%AE%B6%E6%8C%87%E6%95%B0%EF%BC%8C2019%E5%B9%B4%2F</url>
    <content type="text"><![CDATA[最健康的国家指数，2019年 排名如下：1.西班牙2.意大利3.冰岛4.日本6.瑞典7.澳大利亚8.新加坡9.挪威10.以色列12.法国16.加拿大17.韩国19.英国23.德国26.希腊35.美国40.波兰48.匈牙利51.土耳其52.中国53.墨西哥54.阿根廷（彭博社） 希望你所在的国家，没有食品安全问题。]]></content>
      <categories>
        <category>世界排名</category>
      </categories>
      <tags>
        <tag>健康</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[中本聪与比特币]]></title>
    <url>%2F2018%2F12%2F31%2F%E4%B8%AD%E6%9C%AC%E8%81%AA(Satoshi%20Nakamoto)%26%E6%AF%94%E7%89%B9%E5%B8%81%2F</url>
    <content type="text"><![CDATA[介绍下比特币与中本聪的故事，点击更多 中本聪(Satoshi Nakamoto) 【BTC设计初衷】并不希望数字加密货币被某国政府或中央银行控制，而是希望其成为全球自由流动、不受政府监管和控制的货币。 【BTC发展概述】比特币协议及其相关软件Bitcoin-Qt的创造者，但真实身份未知。于2008年发表了一篇名为《比特币：一种点对点式的电子现金系统》（Bitcoin: A Peer-to-Peer Electronic Cash System）的论文，描述了一种被他称为“比特币”的电子货币及其算法。2009年，他发布了首个比特币软件，并正式启动了比特币金融系统。2010年，他逐渐淡出并将项目移交给比特币社区的其他成员。2015年，加州大学洛杉矶分校金融学教授Bhagwan Chowdhry曾提名中本聪为2016年诺贝尔奖经济学奖的候选人。Bhagwan Chowdhry说：“比特币的发明简直可以说是革命性的。中本聪的贡献不仅将会彻底改变我们对金钱的思考方式，很可能会颠覆央行在货币政策方面所扮演的角色，并且将会破坏如西联这样高成本汇款的服务，彻底消除如Visa，MasterCard、PayPal他们收取2-4%的中间人交易税，消除费事且昂贵的公证和中介服务，事实上它将彻底改变法律合约的方式。” 【中本聪身份猜测】1.中情局特勤小组有阴谋论者认为比特币其实是由美国金融机构与政府联手打造的一款骗局工具，目的是地巨大利差引诱投资者巨额投入，再以利差吸取这些投资以平衡美国政府财政。因此中本聪并不是一个人，而是一个小组的代号。这种说法并未得到任何机构或个人的认可，但在反比特币者当中认同度很高。随着近期比特币等加密货币价格的暴跌，这种说法开始在一些比特币持有者当中流传。 2.望月新一2012年5月，计算机科学家泰德·尼尔森认为中本聪就是日本数学家望月新一，认为其足够聪明，研究领域包含比特币所使用的数学算法。更重要的是，望月不使用常规的学术发表机制，而是习惯是独自工作，发表论文后让其他人自己理解。然而也有人提出质疑，认为设计比特币所需的密码学并非望月的研究兴趣。望月本人亦予以否认。 3.尼克·萨博2013年12月，博客作家Skye Grey通过对中本论文的计量文体学分析得出结论，认为其真实身份是前乔治华盛顿大学教授尼克·萨博。萨博热衷于去中心化货币，还发表过一篇关于“比特黄金”（bit gold）的论文，被认为是比特币的先驱。他也是一个著名的从90年代起就喜欢使用化名的人。在2011年5月的一篇文章中，萨博谈起比特币创造者时表示：“在我认识的人里面，对这个想法足够感兴趣，并且能付诸实施的，本来只有我自己、戴维（Wei Dai）、哈尔·芬尼三个人，后来中本出现了（假定中本不是芬尼也不是戴维）。” 4.多利安·中本最为公众所熟知的猜测发生在2014年3月6日。新闻周刊记者Leah McGrath Goodman发表文章称自己已经找到真正的中本，是一个居住在加利福尼亚州的日裔美国人，名叫多利安·中本，而“哲史”是他出生时的名字。除了名字相同以外，Goodman还找到了一些佐证，其中最有力的一条是，当Goodman在当面采访并提出比特币的问题时，多利安的回答看起来确认了其比特币之父的身份：“我已经不再参与它了，不能讨论它。它已经被转交给其他人。他们现在在负责。我已经没有任何联系。”这段话的真实性亦得到了当时在场的洛杉矶郡警察的确认。报道被公开后受到了包括比特币社区在内舆论的质疑和批评，但同时也引起了媒体的巨大兴趣。记者们蜂拥而至多利安的住宅外蹲守，甚至追逐他的汽车。然而在后来的正式访谈中，多利安否认了自己与比特币的全部联系，称自己从未听说过，只是误解了Goodman的提问，以为她问的是自己之前从军方承接的保密性工作。当天晚些时候，中本聪本人也站出来否认。他在P2P基金会的账户在尘封五年之后发了第一条消息，称：“我不是多利安·中本。” 5.克雷格·史蒂芬·怀特2015年12月，《连线杂志》报道说澳大利亚学者克雷格·史蒂芬·怀特很有可能是中本聪的本尊。同时也指出，也许只是他精心设计的一个高明的骗局想让我们相信他就是中本聪本人。直到2016年5月2日，澳大利亚企业家克雷格·史蒂芬·怀特公开承认自己就是发明比特币的中本聪，首度有人公开承认。其证据是中本聪的加密签名档，但被质疑该档只要是稍微高端一点的黑客都能在暗网中找到下载，早就在不少计算机高手圈流传，另一证据是早期第1及第9区块比特币地址的私钥，但此私钥如果是早期比特币开发人员或其亲近者都有可能拿到。最关键证明是导入比特币至2009年的比特币第一笔交易地址，该地址被视为是中本聪所有，并要求表演汇回，BBC记者将0.017个比特币导入，但最终没有汇回。BBC刊退出和他的访谈片段，自称他就是比特币发明者。但克雷格声明与证据的真实性受到普遍的质疑，在最后阶段要求演示关键证据时，克雷格拒绝并发布了一篇顾左右而言他的博客文章。 6.Vincent van Volkmer自2018年以来，互联网声称美国艺术家Vincent van Volkmer是中本聪。对此同时也有一系列证据，例如：他谈到他是一名数学家和密码学家的事实，他也与拥有导致阻碍技术相关知识的专家保持着良好的联系。不过，他自己也反驳了他就是中本聪的这种说法。 7.其它猜测还有一些其他个人或团体被认为是中本聪的真身。其中包括：芬兰经济社会学家Dr Vili Lehdonvirta及爱尔兰密码学研究生Michael Clear。两人分别否认。德国及美国研究人员Neal King、Vladimir Oksman和Charles Bry。他们曾共同申请注册一项与比特币相关的专利，而比特币项目官方网站的域名bitcoin.org恰好注册于专利申请提交之后的第三天。三人均否认此猜测。比特币基金会首席科学家Gavin Andresen、比特币交易平台Mt. Gox创始人Jed McCaleb，或某个政府机构。[1]美国企业家及安全研究员Dustin D. Trammell，但他公开否认。也有人认为Satoshi Nakamoto的名字实际上是四家公司名字的组合，包括三星（Samsung）、东芝（Toshiba）、中道（Nakamichi）和摩托罗拉（Motorola），暗示着比特币其实是这四家公司联手开发并以Satoshi Nakamoto，即“中本聪”的化名来发表。]]></content>
      <categories>
        <category>数字货币</category>
      </categories>
      <tags>
        <tag>数字货币</tag>
        <tag>中本聪</tag>
        <tag>比特币</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[三星集团]]></title>
    <url>%2F2018%2F12%2F30%2F%E4%B8%89%E6%98%9F%E9%9B%86%E5%9B%A2%2F</url>
    <content type="text"><![CDATA[下面介绍下三星集团的基本信息： 三星集团1.公司名称：三星集团2.成立时间：1938年3.创始人：李秉哲4.现任会长：李健熙5.总部位置：韩国首尔6.公司分类：电子、金融、机械、化学7.全球员工人数：20万8.年营收：2119.41亿美元（2018年），约等于14200亿人民币9.全球500强：第12位10.旗下所有业务：电子： 三星电子：消费型电子（手机、显示器）、内存、闪存等； 三星SDI：太阳能电池、燃料电池、能源储存； 三星SDS：IT相璃基板、等离子过滤器、显像管和玻璃； 三星航空：三星贝尔427，为贝尔、波音等公司的产品提供服务； 三星半导体：主要业务为生产SD卡，世界最大的存储芯片制造商；机械 三星重工：主要业务为造船； 三星工程：主要业务为制造电子零件装备、军用飞机零组件； 三星道逹尔：主要业务为制造塑料、化工产品、石油产品。； 三星石油化学：主要业务为PTA； 三星精密化学：主要业务为制造电子化学材料、精密化学制品； 三星BP化学：主要业务为制造硝酸、H2、VAM；金融保险 三星生命保险：主要业务为人寿保险和金融服务； 三星火灾海上保险：主要业务为人寿保险和金融服务； 三星信用卡 [18] ：主要业务为信用卡业务，贷款，租赁服务； 三星证券：主要业务为资产管理、中介业务； 三星投资信托管理：主要业务为投资信托； 三星风险投资：主要业务为风险投资业务；其他 三星物产：主要业务有贸易部门和建设部门； 三星第一毛织：主要业务为是时装、纺织、化工、电子材料相关； 三星第一广告：主要业务为是广告代理业务； 三星新罗酒店：主要业务为是酒店相关业务； 三星爱宝乐园：位于京畿道龙仁市的游乐园，是韩国第二大游乐园，由庆典世界、加勒比海湾、爱宝乐园赛车场组成； 三星首尔医院：位于韩国首尔的医院，韩国最大、最具影响力的医院，隶属于三星集团； 三星狮：韩国职业棒球捧场数最多的球队；]]></content>
      <categories>
        <category>世界排名</category>
      </categories>
      <tags>
        <tag>三星</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018世界医药顶尖大学]]></title>
    <url>%2F2018%2F12%2F29%2FTop%20universities%20in%20medicine%2C%202018%2F</url>
    <content type="text"><![CDATA[Top universities in medicine, 2018.2018世界医药顶尖大学： 排名如下： 123456789101. Harvard 哈佛大学 美国2. Oxford 牛津大学 英国 3. Cambridge 剑桥大学 英国4. Stanford 斯坦福大学 美国5. John Hopkins 约翰·霍普金斯大学 美国6. Karolinska Institutet 卡罗林斯卡学院 瑞典7. Uni of California, LA 加州大学洛杉矶分校 美国8. Yale 耶鲁大学 美国9. MIT 麻省理工学院 美国10. University College London 伦敦大学学院 英国]]></content>
      <categories>
        <category>世界排名</category>
      </categories>
      <tags>
        <tag>医药大学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019世界上城市生活成本排行]]></title>
    <url>%2F2018%2F12%2F29%2FWorld's%20most%20expensive%20cities%2C%202019.%20(cost%20of%20livi%2F</url>
    <content type="text"><![CDATA[World’s most expensive cities, 2019. (cost of living)2019世界上城市生活成本排行： 排名如下： 1234567891011=1.🇸🇬Singapore 新加坡 新加坡=1.🇫🇷Paris 巴黎 法国=1.🇭🇰Hong Kong 香港 中国4.🇨🇭Zurich 苏黎世 瑞士=5.🇨🇭Geneva 日内瓦 瑞士=5.🇯🇵Osaka 大阪 日本=7.🇰🇷Seoul 首尔 韩国=7.🇩🇰Copenhagen 哥本哈根 丹麦=7.🇺🇸New York 纽约 美国=10.🇮🇱Tel Aviv 特拉维夫 以色列=10.🇺🇸Los Angeles 洛杉矶 美国 The Economist Intelligence Unit, 2019]]></content>
      <categories>
        <category>世界排名</category>
      </categories>
      <tags>
        <tag>城市生活成本</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019世界上最著名的运动员]]></title>
    <url>%2F2018%2F12%2F28%2FThe%20world%E2%80%99s%20most%20famous%20athletes%2C%202019%2F</url>
    <content type="text"><![CDATA[The world’s most famous athletes, 2019.2019世界上最著名的运动员： 1234567891011121314151617181920排名 人名 中文名 职业 就职国家 国籍 1 Ronaldo 罗纳尔多 足球 西班牙 葡萄牙2 LeBron 勒布朗·詹姆斯 篮球 🇺🇸美国 美国3 Messi 里奥·梅西 足球 🇦🇷阿根廷 阿根廷4 Neymar 内马尔·达席尔瓦 足球 法国 巴西5 McGregor 康纳·麦格雷戈 格斗 🇮🇪爱尔兰 爱尔兰6 Federer 罗杰·费德勒 网球 🇨🇭瑞士 瑞士7 Kohli 维拉·哥利 板球 🇮🇳印度 印度8 Nadal 拉菲尔·纳达尔 网球 🇪🇸西班牙 西班牙12 Pogba 保罗·博格巴 足球 🇫🇷法国 几内亚14 Mbappe 姆巴佩 足球 🇫🇷法国 法国17 Serena 塞雷娜·威廉姆斯 网球 🇺🇸美国 美国19 Özil 梅苏特·厄齐尔 足球 🇩🇪德国 德国21 󠁧Hamilton 理查德·汉密尔顿 篮球 🇺🇸美国 美国30 Salah 穆罕默德·萨拉赫 足球 🇪🇬埃及 埃及33 Bale 加雷斯·贝尔 足球 威尓士 威尓士38 Ramos 塞尔吉奥·拉莫斯 足球 🇪🇸西班牙 西班牙41 Ninja Ninja 电竞 🇺🇸美国 美国 58 Benzema 卡里姆·本泽马 足球 🇫🇷法国 法国73 Kroos 托尼·克罗斯 足球 🇩🇪德国 德国]]></content>
      <categories>
        <category>世界排名</category>
      </categories>
      <tags>
        <tag>运动员</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[The world's Top 100 Airports for 2018]]></title>
    <url>%2F2018%2F12%2F27%2FThe%20world's%20Top%20100%20Airports%20for%202018%2F</url>
    <content type="text"><![CDATA[The world’s Top 100 Airports for 2018, as voted for by air travellers around the world during the 2017/2018 survey periodPlease note that over 500 airports were covered in the survey but we only feature the top 100 listing here (this listing may not be reproduced without the consent of Skytrax). as blow: 1 Singapore Changi 1 20172 Seoul Incheon 3 20173 Tokyo Haneda 2 20174 Hong Kong 5 20175 Doha Hamad 6 20176 Munich 4 20177 Centrair Nagoya 7 20178 London Heathrow 9 20179 Zurich 8 201710 Frankfurt 10 2017 15 Taiwan Taoyuan 21 2017 18 Shanghai Hongqiao 18 2017 28 London City 36 2017 33 Beijing Capital 25 2017 37 Paris CDG 32 2017 51 San Francisco 39 2017]]></content>
      <categories>
        <category>世界排名</category>
      </categories>
      <tags>
        <tag>Airports</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[日本近几年的经济数据]]></title>
    <url>%2F2018%2F12%2F26%2FJapan%20Economy%20Data%2F</url>
    <content type="text"><![CDATA[Japan Economy Data ![日本近几年经济数据](Japan Economy Data/japan_economy_data.jpg) 你看到了什么？]]></content>
      <categories>
        <category>经济</category>
      </categories>
      <tags>
        <tag>日本</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[全球各国家离婚率排行]]></title>
    <url>%2F2018%2F12%2F26%2FHighest%20Divorce%20Rate%20by%20Country%2F</url>
    <content type="text"><![CDATA[Highest Divorce Rate by Country全球各国家离婚率排行： 1.Luxembourg : 87% 卢森堡2.Spain: 65% 西班牙3.France: 55% 法国4.Russia: 51% 俄国5.United States: 46% 美国6.Germany: 44% 德国7.United Kingdom: 42% 英国8.New Zealand: 42% 新西兰9.Australia: 38% 澳大利亚10.Canada 38% 加拿大. .. India 1% 印度 你们国家的离婚率大概是多少呢？let me know in the comments.]]></content>
      <categories>
        <category>世界排名</category>
      </categories>
      <tags>
        <tag>离婚率</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019全球国家手机网络连接速度排行]]></title>
    <url>%2F2018%2F11%2F26%2FAverage%20speed%20of%20mobile%20internet%20connections%2C%202019%2F</url>
    <content type="text"><![CDATA[Average speed of mobile internet connections, 2019. (in MBPS)2019全球国家手机网络连接速度排行（MB/s) 1.ISL: 73.93 以色列2.NOR: 70.29 挪威3.CAN: 65.68 加拿大4.AUS: 56.70 澳大利亚5.SIN: 54.96 新加坡6.KOR: 52.53 韩国7.FRA: 43.34 法国8.USA: 33.19 美国9.JPN: 32.08 日本10.HK: 32.00 香港11.GER: 31.46 德国12.KSA: 30.80 沙特阿拉伯13.GBR: 30.12 英国14.CHN: 30.08 中国15.RUS: 19.16 俄国16.IND: 10.13 印度 (Ookla) 你在哪个国家呢？]]></content>
      <categories>
        <category>世界排名</category>
      </categories>
      <tags>
        <tag>移动通信</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[windows_C盘垃圾清理]]></title>
    <url>%2F2018%2F11%2F03%2Fwindows-C%E7%9B%98%E5%9E%83%E5%9C%BE%E6%B8%85%E7%90%86%2F</url>
    <content type="text"><![CDATA[windows c盘垃圾清理程序，清理各类软件的缓存文件、日志文件等，长时间未清洗，可以空出10多G的空间出来，具体程序如下，创建一个abc.bat可执行文件（名字随便取），点击运行即可： 12345678910111213141516171819@echo off echo 正在清除系统垃圾文件，请稍等...... del /f /s /q %systemdrive%\*.tmp del /f /s /q %systemdrive%\*._mp del /f /s /q %systemdrive%\*.log del /f /s /q %systemdrive%\*.gid del /f /s /q %systemdrive%\*.chk del /f /s /q %systemdrive%\*.old del /f /s /q %systemdrive%\recycled\*.* del /f /s /q %windir%\*.bak del /f /s /q %windir%\prefetch\*.* rd /s /q %windir%\temp &amp; md %windir%\temp del /f /q %userprofile%\小甜饼s\*.* del /f /q %userprofile%\recent\*.* del /f /s /q &quot;%userprofile%\Local Settings\Temporary Internet Files\*.*&quot; del /f /s /q &quot;%userprofile%\Local Settings\Temp\*.*&quot; del /f /s /q &quot;%userprofile%\recent\*.*&quot; echo 清除系统LJ完成！ echo. &amp; pause 随便放在电脑的一个位置，点击运行即可。]]></content>
      <categories>
        <category>windows</category>
      </categories>
      <tags>
        <tag>windows</tag>
        <tag>垃圾清理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019年全球最具价值的餐饮品牌]]></title>
    <url>%2F2018%2F10%2F26%2F2019%E5%B9%B4%E5%85%A8%E7%90%83%E6%9C%80%E5%85%B7%E4%BB%B7%E5%80%BC%E7%9A%84%E9%A4%90%E9%A5%AE%E5%93%81%E7%89%8C%2F</url>
    <content type="text"><![CDATA[2019年全球最具价值的餐饮品牌，拒绝食品垃圾。 品牌名称 国家 成立日期1.星巴克 美国 19712.麦当劳 美国 19553.赛百味 美国 19654.肯德基 美国 19525.提姆霍顿 加拿大 19646.达美乐披萨 美国 19607.汉堡王 美国 1952 公司能经营六七十年，背后是对品质、服务的追求，难道我们不应该支持吗？让地沟油、毒奶粉死去吧。 海底捞 中国 1994真功夫 中国 1990永和大王 中国 1995德克士 中国 1994喜家德 中国 2002味多美 中国 1996嘉和一品 中国 2004渝是乎 中国 2015呷哺呷哺 台湾 1998 半个世纪后见。]]></content>
      <categories>
        <category>世界排名</category>
      </categories>
      <tags>
        <tag>餐饮</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阿里巴巴mysql数据库binlog的增量订阅与消费组件canal]]></title>
    <url>%2F2018%2F09%2F05%2F%E9%98%BF%E9%87%8C%E5%B7%B4%E5%B7%B4mysql%E6%95%B0%E6%8D%AE%E5%BA%93binlog%E7%9A%84%E5%A2%9E%E9%87%8F%E8%AE%A2%E9%98%85%E4%B8%8E%E6%B6%88%E8%B4%B9%E7%BB%84%E4%BB%B6canal%2F</url>
    <content type="text"><![CDATA[首先介绍下canal他可以做什么，基于日志增量订阅&amp;消费支持的业务, 监控mysql数据，将mysql增量数据从binlog中获取过来实现数据库的镜像、数据库实时备份、多级索引、业务cache刷新等，具体参考阿里开源项目代码：canal github canaldbkafka简介canaldbkafka是连接canal和kafka的一个中间件。目的是实现数据库某个表格数据变更转变成消息流的形式，以便后续业务消费kafka的消息流。 canal wiki:https://github.com/alibaba/canal/wiki 消息的类型canal的binlog 会被解析成以下3中类型的消息。其他的类型被过滤掉了。 insert12345678910111213141516171819202122232425&#123; &quot;data&quot;: &#123; &quot;need_sub&quot;: &#123; &quot;type&quot;: &quot;int(11)&quot;, &quot;updated&quot;: true, &quot;value&quot;: &quot;0&quot; &#125;, &quot;order_description&quot;: &#123; &quot;type&quot;: &quot;varchar(1024)&quot;, &quot;updated&quot;: true, &quot;value&quot;: &quot;&quot; &#125;, &quot;pay_amount&quot;: &#123; &quot;type&quot;: &quot;int(11)&quot;, &quot;updated&quot;: true, &quot;value&quot;: &quot;0&quot; &#125;, &quot;pay_order&quot;: &#123; &quot;type&quot;: &quot;varchar(30)&quot;, &quot;updated&quot;: true, &quot;value&quot;: &quot;&quot; &#125; &#125;, &quot;type&quot;: &quot;insert&quot;&#125; delete12345678910111213141516171819202122232425&#123; &quot;data&quot;: &#123; &quot;need_sub&quot;: &#123; &quot;type&quot;: &quot;int(11)&quot;, &quot;updated&quot;: true, &quot;value&quot;: &quot;0&quot; &#125;, &quot;order_description&quot;: &#123; &quot;type&quot;: &quot;varchar(1024)&quot;, &quot;updated&quot;: true, &quot;value&quot;: &quot;&quot; &#125;, &quot;pay_amount&quot;: &#123; &quot;type&quot;: &quot;int(11)&quot;, &quot;updated&quot;: true, &quot;value&quot;: &quot;0&quot; &#125;, &quot;pay_order&quot;: &#123; &quot;type&quot;: &quot;varchar(30)&quot;, &quot;updated&quot;: true, &quot;value&quot;: &quot;&quot; &#125; &#125;, &quot;type&quot;: &quot;delete&quot;&#125; updatedata对象是各字段类型、是否被更新、值。olddata对象是之前的状态。 123456789101112131415161718192021222324252627&#123; &quot;data&quot;: &#123; &quot;Quota&quot;: &#123; &quot;type&quot;: &quot;tinyint(4)&quot;, &quot;updated&quot;: false, &quot;value&quot;: &quot;0&quot; &#125;, &quot;ReqAmount&quot;: &#123; &quot;type&quot;: &quot;int(11)&quot;, &quot;updated&quot;: true, &quot;value&quot;: &quot;100&quot; &#125; &#125;, &quot;olddata&quot;: &#123; &quot;Quota&quot;: &#123; &quot;type&quot;: &quot;tinyint(4)&quot;, &quot;updated&quot;: false, &quot;value&quot;: &quot;0&quot; &#125;, &quot;ReqAmount&quot;: &#123; &quot;type&quot;: &quot;int(11)&quot;, &quot;updated&quot;: false, &quot;value&quot;: &quot;0&quot; &#125; &#125;, &quot;type&quot;: &quot;update&quot;&#125; 使用说明编译安装123456789101112131415mvn compilemvn packagell target/canal-dbkafka #可部署total 0drwxr-xr-x 5 xxx staff 170B 12 21 21:26 bindrwxr-xr-x 3 xxx staff 102B 12 21 21:26 confdrwxr-xr-x 24 xxx staff 816B 12 21 21:26 libdrwxr-xr-x 2 xxx staff 68B 12 21 21:26 logsll target/canal-dbkafka/bin #startmy.sh为启动示例-rwxr-xr-x 1 xxx staff 271B 12 21 21:26 startmy.sh-rwxr-xr-x 1 xxx staff 2.5K 12 21 21:26 startup.sh-rwxr-xr-x 1 xxx staff 1.0K 12 21 21:26 stop.sh 启动说明已startmy.sh为例 123456789101112#!/bin/bashcurrent_path=`pwd`case &quot;`uname`&quot; in Linux) bin_abs_path=$(readlink -f $(dirname $0)) ;; *) bin_abs_path=`cd $(dirname $0); pwd` ;;esaccd $&#123;bin_abs_path&#125; &amp;&amp; ./startup.sh testdb thetable 127.0.0.1:2181 127.0.0.1:9092 testdb 是canal配置的destination thetable kafka的具体topic 127.0.0.1:2181 是canal配置HA 对应的zookeeper的地址 127.0.0.1:9092 是kafka的地址 使用注意事项 mysql binlog模式设置为row模式 为了保证数据库消息的顺序性，将消息存储kafka的时候组件采用了同步的方式 canal 必须配置zookeeper ha的模式 https://github.com/alibaba/canal/wiki/AdminGuide#ha%E6%A8%A1%E5%BC%8F%E9%85%8D%E7%BD%AE 之前使用针对的是数据库中的一个表在canal配置中已经过滤所以消息中没有表名 可以说是个设计的缺陷。 高可用及分布式监控多个mysqlcanal分服务端和客户端，我们需要监控多个mysql时，可以配置多个instance，具体编辑服务端配置文件canal.properties： 123456789############################################################# destinations ##############################################################canal.destinations=dest21,dest14# conf root dircanal.conf.dir = ../conf# auto scan instance dir add/remove and start/stop instancecanal.auto.scan = truecanal.auto.scan.interval = 5 其中dest21和dest14为不同的instance，目录结构如下： 12345-rwxr-xr-x 1 root root 2882 Aug 27 18:44 canal.propertiesdrwxr-xr-x 2 root root 4096 Sep 5 19:08 dest14drwxr-xr-x 2 root root 4096 Sep 5 19:09 dest21-rwxr-xr-x 1 root root 3038 Jun 19 17:18 logback.xmldrwxr-xr-x 3 root root 4096 Jun 19 17:18 spring dest14和dest21目录分别为监控不同mysql的配置文件放置位置，具体如下： 12345678910111213141516171819202122232425262728293031323334353637################################################### mysql serverIdcanal.instance.mysql.slaveId=14# position infocanal.instance.master.address=1.1.1.1:3306canal.instance.master.journal.name=canal.instance.master.position=canal.instance.master.timestamp=# table meta tsdb infocanal.instance.tsdb.enable=truecanal.instance.tsdb.dir=$&#123;canal.file.data.dir:../conf&#125;/$&#123;canal.instance.destination:&#125;canal.instance.tsdb.url=jdbc:h2:$&#123;canal.instance.tsdb.dir&#125;/h2;CACHE_SIZE=1000;MODE=MYSQL;#canal.instance.tsdb.url=jdbc:mysql://127.0.0.1:3306/canal_tsdbcanal.instance.tsdb.dbUsername=canalcanal.instance.tsdb.dbPassword=canal#canal.instance.standby.address =#canal.instance.standby.journal.name =#canal.instance.standby.position =#canal.instance.standby.timestamp =# username/passwordcanal.instance.dbUsername=canalcanal.instance.dbPassword=*****canal.instance.defaultDatabaseName=canal.instance.connectionCharset=UTF-8# table regex#canal.instance.filter.regex=.*\\..*canal.instance.filter.regex=event_collection\.user_location_lng_lat# table black regexcanal.instance.filter.black.regex=################################################# 你需要修改的地方： 12345canal.instance.mysql.slaveId -- 不同的instance分配不同的slaveId，因为canal监控mysql的原理就是伪装成mysql的slave来获取binlog日志的canal.instance.master.address -- 配置监控的mysql ip地址canal.instance.dbUsername -- 连接mysql的用户名canal.instance.dbPassword -- 连接mysql的密码canal.instance.filter.regex -- 监控mysql中的哪个库，哪个表 其中监控mysql的哪个库哪个表编写格式如下： 123.*\\..* --表示监控mysql所有库所有表test\..* --表示监控mysql test库下的所有表test\.test --表示监控mysql test库下的test表 阿里巴巴，我们程序员的梦想，开源的canal还是不错的，希望大家借助这篇文章能够熟练掌握canal的简单使用，如果遇到什么问题，欢迎一起讨论，在下方留言或者mail我：chenzuoli@gmail.com]]></content>
      <categories>
        <category>监控组件</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>canal</tag>
        <tag>监控</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[北京租房小中介骗局]]></title>
    <url>%2F2018%2F07%2F29%2F%E5%8C%97%E4%BA%AC%E7%A7%9F%E6%88%BF%E5%B0%8F%E4%B8%AD%E4%BB%8B%E9%AA%97%E5%B1%80%2F</url>
    <content type="text"><![CDATA[给大家说说我亲身经历的租房骗局，小中介如何骗你的钱。我会不定期在网站发布个人生活中遇到的各种骗局，发布到【个人黑名单】分类中，请大家持续关注，避免遇到同样的坑，打击坑蒙拐骗，让那些只顾赚钱，不顾服务的商家，淘汰掉。 中介公司美丽家园房地产有限公司、昊园恒业房地产有限公司 产品大熊公寓 总部地址北京市朝阳区财满街8号楼2单元703室 个人情况首先说说我的情况，码农一枚跟朋友一起租房，3个人，租2间房，一间2人，一间1人，无奈北京房租太贵，链家、我爱我家中介费太高，只好找小中介，不受中介费。 租房第一个坑：预付款时，一定看好是“订金”还是“定金”找到一个小中介公司，在一个小区里面租下15平米左右的主卧2000元，收了一个月押金。当时保证押金可以退回，家具家电均可以上门维修。于是就签合同，另一个朋友，在签合同的时候，了解到，如果中途退租的话，需要找到接盘侠，才能退租，把押金退给你，他不干，于是签了一半的合同终止了，但是中介不干啊，他说合同是要收钱的，200块，你说气不气人，还有，跟他们争论的时候，还问他们索要订金200块，但是那个条子上写的是“定金”，意思就是你已经预付款一些金钱把这个房子定下来了，于是发现情况不对，就对他们客气地说话，说我们之前没弄清楚情况，麻烦谅解一下，最后扣了100块，定金退了。所以大家以后预付款的时候，一定看好是“订金”还是“定金”。 第二个坑：房租付款方式，自付最好，不要和第三方借贷平台签约租金结算方式为58月付，58月付是一个借贷平台，通过借贷的方式，一次性借贷1年的房租也就是20000元，58月付一下子就把钱打给了中介公司，然后58每个月从你的银行卡中扣除2000元，这样中介公司实际就获得了20000元的借款，可以拿去投资或者业务扩张了，而实际是你借的钱。这种借贷是上了征信的，如果你有违约，征信就会出现问题，所以我没一次敢怠慢。 第三个坑：房屋中的家电不能用租下房子之前看过了房子，家电、桌椅、床等设施都齐全，灯、空调、冰箱都使用过了，确实是好的，于是签了合同，住进来才发现洗衣机没法用，缺少一根水管，于是联系他们，配一根洗衣机水管，于是他就叫我们自己弄，公司没有这项规定，也没有相应的业务人员做这个事情，于是只能自己网上买水管了啊，合同都签了。还有一次，灯不亮了，也叫他们过来维修，同样的理由，叫我们自己网上买个灯管，我们真是服了，对于服务这么差的中介，绝对不会合作第二次。 第三个坑：重新签合同，导致交租金提前美丽家园被收购，母公司重新过来签合同，之前是每月16日交租，现在12日交租，房租提前4天交，他们的业务员说会把这4天的钱退给我们，但是一直没退。 第四个坑：退租，乱扣租金退租的时候，房管一副高高在上的样子，好像谁欠他钱似的，查看了房间，检查了家电，押金中扣了我们300元，我去你大爷，住了10个月，给你换了灯管、镇流器、洗衣机水管，还给我们说损坏费用。这300块，100块写在单子上付给公司，另外200块说私下给他转过去，这是拿回扣啊，好好好给你这个死胖子。好了，这押金搞定了，还有房租的事情，因为每次交房租都是交的下个月的，于是我们最后一个月交的是下个月的，应该退我们1个月的房租，但是~一脸横肉的房管，硬生生给我们算成了26天，我跟他算数，他不跟我算数，一直按照他的那套算法来，他妹的，你们还差我4天前没退呢，最后也懒得跟这种人打交道了，就26天了，拿了单子走人了。最后说是押金和租金会在1-2个月内退回，你妹的，你们工作效率这么低吗，要这么长时间。我拿了押金条，快速离开了这个人。这里附上退租押金条： 第五个坑：不退押金，原因是财务正在处理一个月到了确实退了我们租金，但是押金没有退给我们，于是找他们要，第一次，他说提交给财务，第二次去催催财务，第三次去催催财务，这几次，态度极其恶劣，如果不是钱在他手里，我就骂死他，于是又忍气吞声1个月半月过去了，承诺给我们的1-2个月已经超过了半个月，继续联系，我们同一个套间里面的其他朋友也都在催，于是叫我们联系客服，客服电话一直打不通，一次没打通过，于是跟他理论，自己去总部要钱，他说不知道总部位置，我晕死，你一个业务人员不知道总部的位置，还干啥啊，回家种田吧，又联系一次，说是找找总部位置，给我发了一个地址：朝阳区牌坊街8号楼2单元703，好了，我带着沉重的心情去总部要钱，没把握要到钱啊，位置离我2个小时车程。到了，没有公司牌子，很像一个租户的屋，于是敲门，问问，果然是的。于是恼火了，我2个小时到这里，你给我一个错误的地址？打电话过去骂他，我们互相对骂，这几年没骂人了，骂的痛快，你不让我好过，我也不让你好过。无奈啊，找物业问了，也是说没有这个房地产公司。于是找当初带我们看房子的业务员，问他总部位置，说是：朝阳区财满街8号楼2单元703，于是有话2小时去这个地方，这个地方是对的，找到他们领导，说明了情况，登记了紧急退租表，给我们说是8月10号退租。其实这时候心还是有点怀疑他们说的话，但是不信也没什么办法啊，于是回家了，今天一天都在处理这个事情了，上午9点出发，下午4点才到家，等吧。这里附上总部图片：还有房管的电话：15510302000，欢迎去骚扰他，骂他更好。 好了，希望大家记住这几点，并告诉身边的朋友们，多谢了。]]></content>
      <categories>
        <category>个人黑名单</category>
      </categories>
      <tags>
        <tag>租房</tag>
        <tag>骗局</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[区块链应用]]></title>
    <url>%2F2018%2F07%2F25%2F%E5%8C%BA%E5%9D%97%E9%93%BE%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[区块链技术，会颠覆传统互联网信息技术行业，成为下一个风口，你要把握住吗？下面来聊聊区块链在目前各大行业中的一些应用，和目前开发区块链应用用到了平台和技术。 一、分类1.公有链任何人均可参加和退出 联盟链加入和退出需要经过联盟授权 3.私有链权利完全控制在一个组织中 二、开发平台1.以太坊2.Hyperledger：IBM开源3.商用区块链组件：共识层（可插拔） 智能合约层 通讯层 数据抽象层 加密抽象层 身份服务 策略服务 API 互操作、模块化三、Hyperledger应用1.金融服务提升透明度，减少交易时间、降低风险数字贸易链：银行间交易跨境支付：swift组织绿色资产管理平台：IBM与中国合作数字身份：菲律宾银行账户的问题房地产交易：房产属主，解决房产产权鉴定的问题 2.供应链环节真实性，透明度更高，效率更高海鲜供应链追踪：偷猎，食品供应链钻石供应链：防止钻石引起的武装冲突食品安全：每个环节的食品安全 3.医疗患者不同医疗组织、机构之间的病例共享医疗记录：病例共享州际医疗许可：美国各州之间的医疗共享 四、开发语言以太坊：c++、java、python、go、solidityHyperledger：js]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>blockchain</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[区块链技术存在的问题]]></title>
    <url>%2F2018%2F07%2F14%2F%E5%8C%BA%E5%9D%97%E9%93%BE%E6%8A%80%E6%9C%AF%E5%AD%98%E5%9C%A8%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[区块链技术，可以说是可以颠覆传统行业规则的一个革命性技术，去中性化、不可篡改、可回溯等特点，让全球化加速，让世界变得可以信赖，这不就是我们想要的世界吗，但是目前区块链经过了几年的发展，各种乱象横生，需要我们清晰地认识，别走弯路，下面介绍下区块链技术目前所存在的问题，开发者们或者区块链创业者们需要了解的。 1.标准不统一区块链是什么，目前业界还没有一个统一的清晰明确的概念。没有清晰统一的概念界定，又缺少权威的机构对区块链产品进行评定，这极易造成在涉及区块链的项目谈判、实施过程中出现问题，更谈不上区块链的大规模推广和应用。市场上已有的区块链应用也是“鱼龙混杂”，无法有效评价产品质量。区块链亟需建立一套统一的标准规范来界定其内涵和外延，并说明评判的方法，从而引导市场健康发展。然而区块链技术仍在不断创新变化，应用场景也在不断地探索中，过早的标准化会限制区块链技术的创新和行业的发展。因此，为适应目前区块链行业的发展阶段，区块链的标准化工作应从满足用户的角度出发，以测试某个区块链系统对用户需求的匹配度为原则，开展功能和性能测试的“黑盒”标准化，而不是过早地对区块链技术进行规范。 2.衍生市场混乱处于炒作高峰期的区块链技术，不仅受到社会大众的广泛关注，而且存在着被不法分子所利用进行欺诈的情况。目前市场上存在着大量的打着数字货币的旗号，进行传销、诈骗、非法融资，这些数字货币利用门户网站、微博、微信公众号、贴吧等渠道进行宣传和招商活动，进行炒作，而不真正地拿着投资者的钱去研究、开发区块链引用上。 3.安全威胁在大量资本融入到区块链行业中时，区块链技术得到了飞速的发展，同时，安全问题也得到了广泛的关注，近期许多数字货币交易平台出现黑客攻击，盗取用户的数字货币达到百亿元级别，这是交易平台技术上未达到安全的要求。而在区块链财务类系统中，私钥是用户身份的唯一凭证，在有些应用中，需要将用户的私钥跟用户身份进行绑定，这样就需要通过平台来对用户的私钥进行管理，这种情况下，秘钥的管理会存在安全问题，而这个问题，并不能通过区块链系统自身来解决，而是需要区块链系统外部来解决。 4.难以监管区块链技术采用去中心化的理念，摆脱了传统中心化的管理机制所带来的诸多问题，但去中心化也意味着，主体不明确，监管困难，缺少对主体的有效控制，比如在上次的黑客勒索时间，犯罪分子以比特币作为交易赎金，导致其身份极其难以追查。 大概介绍到这里，未完待续，我们一起来见证区块链的崛起，有意见，欢迎mail我，邮箱chenzuoli709@gmail.com.]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>blockchain</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka命令行基本操作]]></title>
    <url>%2F2018%2F07%2F14%2Fkafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[这里先介绍下kafka的基本知识及应用场景：它是一个分布式、高吞吐量、容错性好的消息队列，基于生产者、消费者模型来实现消息的生产消费，在我们的应用系统中，可以起到数据流缓存、解耦合、高效的作用。下面是kafka命令行的基本操作。 对于kafka集群的安装配置，这里就不讲解了，apache官网有详细配置方案，可参考：apache kafka配置详解注：本次使用操作的基本环境为kafka 1.1.0、centos 7.4 1.后台启动kafka broker1./bin/kafka-server-start.sh -daemon config/server.properties 2.关闭kafka broker：1./bin/kafka-server-stop.sh 3.创建topic：1./bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 2 --partitions 2 --topic test 4.创建生产者：1./bin/kafka-console-producer.sh --broker-list hadoop31:9092,hadoop32:9092,hadoop33:9092 --topic test 5.创建消费者：1./bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic test --from-beginning 6.查看所有topic：1./bin/kafka-topics.sh --list --zookeeper localhost:2181 7.查看topic状态：1./bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic test 8.查看topic消费情况：1./bin/kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list hadoop31:9092,hadoop32:9092,hadoop33:9092 --topic sparktest --time -1 9.删除某个topic：1./bin/kafka-topics.sh --delete --zookeeper hadoop31:2181,hadoop32:2181,hadoop33:2181 --topic sparktest 注意设置配置参数delete.topic.enable=true，然后重启kafka和zookeeper才可以生效； 好了，到这里还没完，后期会持续更新，我会把我工作中使用到的经过测试没问题的kafka知识记录下来分享给大家，如果有什么问题，请大家mail我，希望大家支持。]]></content>
      <categories>
        <category>消息队列</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[比特币原理]]></title>
    <url>%2F2018%2F07%2F05%2F%E6%AF%94%E7%89%B9%E5%B8%81%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;大家好，今天来说说18年很火的比特币，由于现代社会的全球化进程加快，各个国家之间的信息交换，信息共享出现了许许多多的问题，比如你出国旅游，还得更换货币，还得办个护照来证明你是个人，还有就是各个银行或金融机构对货币的监督和管理，一旦这些机构出现问题，那么我们的钱就这样没了，这是不是很亏呢，今天将的区块链技术，就可以解决这个问题，下面来详细讲解它的运行原理和应用场景。 一、去中心化1.如何确认付款方是否有足够的比特币进行支付？&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;去中心化网络，舍去银行等金融机构（他们通过用户账户余额来解决这个问题）的依赖，比特币的解决方案是每笔交易都必须把以前的交易记录作为基础。 2.转账记录的存储和维护2.1如何进行同步，互联网上的计算机交易记录同步；2.2如何防止黑客篡改记录；2.3如何防止同一笔比特币收入被重复使用&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这个时候就需要用到区块链技术，区块链仅仅维护一个交易链，每个人将自己的转账记录发布到网络上，矿工收集这些转账记录，生成一个区块，世界上有许多矿工，那么到底哪一个矿工生成的区块才能链接到区块链的末尾呢？这时，就出现了一个机制，每个矿工在生成了这个区块后，需要对前一个区块链上前一个区块的sha256函数值+这个新区块的基本信息+这个新区块所包含的所有交易记录+随机数进行sha256函数计算，得到一个hash值前72位均为0，那么找到符合要求的随机数需要进行2的72次方sha256函数运算，计算机大概平均需要10分钟左右算出来，然后发布到区块链网络上，在这10分钟之内，一般只有一个矿工能够计算出符合要求的随机数，所以就避免了多个矿工同时生成区块而无法判断到底将哪一个区块链接到区块链的尾端的问题了。矿工得到符合要求的随机数后，发布到网络上，网络上的其他计算机会进行校验：随机数校验，交易记录校验。一切都没有问题后，就讲该区块添加到自己电脑上区块链的末尾，完成交易记录的同步 二、不可篡改：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;针对转账交易来进行说明，利用非对称加密算法，达到不可篡改的目的，具体如下,如，小红转账给小白50元，这条记录 1.原始记录进行SHA256加密得到hash值1；2.小红利用她的私钥对hash值1进行加密得到hash值2；3.其他人利用小红的公钥（公钥是公开的），对hash值2进行解密得到hash值3；&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果hash值3等于hash值1，那么说明这个签名是针对这条记录的，并且这条记录是小红发出的，接受到的记录与原始记录相同，未被篡改。 三、记录可回溯&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;比特币的每条交易记录都记录在区块链上，当小红转账给小白时，会先计算所有转账给小红的比特币数量，来确认小红有足够的比特币进行交易，所以记录可回溯。 四、比特币问答1.比特币是如何发行的？&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;新比特币作为对矿工的奖励，进入比特币网络进行流通，每生成21万个区块，奖励减半，从第0个区块到第21万个区块，每生成一个区块，奖励给矿工50个比特币，从第21万个区块开始的21万个区块，每生成一个区块，奖励给矿工25个比特币，从第42万个区块开始的21万个区块，每生成一个区块，奖励给矿工12.5个区块，以此类推。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从第693个区块开始，对矿工的奖励为0，也就是不再有新的比特币流入比特币网络，到时，累计有2100万个比特币流入到比特币网络，矿工的收入将完全来自于每笔比特币转账交易的交易费，交易费只是比特币在账户之间转移，不是新产生的比特币。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;也就是说，比特币网络上的比特币总量永远不会超过2100万个。 2.比特币存在什么地方？&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;比特币一般存在比特币客户端软件的数据文件里&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果把数据文件弄丢了，比如计算机硬盘坏了，就永远地失去了里面的比特币，而且比特币网络里流通的比特币总量也会减少。 3.比特币转账和支付宝转账有啥区别？&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;比特币不是任何银行和金融机构发行的，使用比特币不需要绑定银行卡，不需要任何身份证明，不需要手机短信认证，只要能上网，只要安装了比特币客户端软件，就可以转账或收款，所有的账户不受任何机构监督和管理，转错了人，没有后悔药，完全没有挽回的余地。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在中国大陆，支付宝转账转的一般是人民币，人民币是中国人民银行发行的，人民币的发行量由中国人民银行根据社会发展需要决定。使用支付宝，需要绑定银行卡，转账或收款受支付宝和银行管理。转错了人，可以找支付宝和银行协调，有可能挽回损失。 4.比特币转账的手续费怎么算？&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;比特币软件会给一个推荐值和最低值，但具体多少由付款方自己定。既然手续费自己定，那么付款方将手续费设为最低值会怎么样呢？&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;请看下面这张图片：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;比特币网络中，支付最少交易费是可以的。但是请注意，当交易量大到超出网络可处理时，矿工会选择手续费更高的交易记录到账本，而您的交易可能永远被搁置，无法确认。 5.比特币所使用的主要技术和特点：5.1利用sha256和非对称加密算法制作签名；5.2利用区块链中的区块存储比特币交易记录；5.3设置额外的工作，从而控制单位时间内生成区块的个数，同时保护比特币网络；5.4将一定数额的比特币和区块内的所有交易费奖励给成功生成该区块的矿工激励更多矿工加入比特币网络，促进比特币网络的茁壮成长；5.5比特币转账不依赖任何银行或其他金融机构；&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;好了，到这里基本上就讲完了，这是我最近从youtube网站上看过很多次讲解的各种比特币、区块链的视频才了解的，国内的资源还很少，天朝也不看好比特币，但是区块链技术是在积极推动的，希望大家看完了总结之后，对比特币的原理有一定的了解，如果有什么不对的地方，请留言指正，或者发送到邮箱chenzuoli709@gmail.com. 附上资源：比特币原理https://www.youtube.com/watch?v=obRzfcvMshM&amp;t=0s&amp;list=LL6p-2jKOMljSPte26mVy3Vw区块链开放前景及学习平台https://www.youtube.com/watch?v=8YY8yuKqziw&amp;t=0s&amp;list=LL6p-2jKOMljSPte26mVy3Vw许知远对话搜狗CEO王小川将区块链https://www.youtube.com/watch?v=pV2DxjxpKu4&amp;t=0s&amp;list=LL6p-2jKOMljSPte26mVy3Vw 五、数字货币投资&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;鄙人还是非常非常看好比特币、区块链技术的，也在火币平台上购买了一些比特币，期待它在以后的日子里带我实现财务自由，想参与的伙伴们，点击链接注册https://www.huobi.br.com/zh-cn/topic/invited/?invite_code=2i9d3确认邀请码：2i9d3]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>bitcoin</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018-06-19]]></title>
    <url>%2F2018%2F06%2F19%2FETL%E5%B7%A5%E5%85%B7%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[大家好，好久没更新了，疲于工作和团队建设，都是一团糟，但是代码还是敲了一些，见我的github/ETL.git项目，写的是基本清洗工具类，点击查看详情。 https://github.com/chenzuoli/ETL.git该项目包括如下内容： ETL数据基本清洗包括以下分类： 1.日期时间；2.数值；3.字符串；4.字符；5.金钱；6.数据库（mysql、postgresql、mongodb、hbase、hdfs、memcached）；7.加解密（md5、sha、base64、aes、rsa）；8.文件；9.http服务；10.正则表达式；11.个人信息：身份证号、手机号、姓名清洗和扩展；后期会不断更新，望大家指正，多谢。]]></content>
      <categories>
        <category>工具类</category>
      </categories>
      <tags>
        <tag>tools</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读取配置文件工具类]]></title>
    <url>%2F2018%2F04%2F03%2F%E8%AF%BB%E5%8F%96%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E5%B7%A5%E5%85%B7%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[我们在编程过程中，尤其是应用程序，需要经常更改的配置参数或者某些使用较多的固定值，我们可以把它提取出来，放到一个配置文件中，当我们需要修改这个值时，就可以做到不重新发布应用，或者不更改许多的代码，这样，即降低了程序代码的后期维护成本，也降低了程序代码的耦合性，这是我们每个合格的程序员应该具备的基本技能。下面来介绍一个读取配置文件的工具类。 maven项目引入依赖123456&lt;!-- https://mvnrepository.com/artifact/log4j/log4j --&gt;&lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.17&lt;/version&gt;&lt;/dependency&gt; 代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374package com.payegis.czl.util;import org.apache.log4j.Logger;import java.io.File;import java.io.InputStreamReader;import java.util.Enumeration;import java.util.HashMap;import java.util.Map;import java.util.Properties;/** * User: chenzuoli * Date: 2018/3/20 * Time: 15:13 * Description: 读取配置文件工具类 * Ps: Properties */public class PropertiesUtils &#123; private static Logger logger = Logger.getLogger(PropertiesUtils.class); private static Properties props; private static String configHome = System.getenv(&quot;pesdk_home&quot;); private static String configFilePath = configHome + File.separator + &quot;conf&quot; + File.separator + &quot;db.properties&quot;; static &#123; readProperties(configFilePath); logger.info(&quot;配置文件加载成功。&quot;); &#125; public static void main(String[] args) &#123; logger.info(get(&quot;psqlPassword&quot;)); &#125; /** * 加载配置文件 * * @param fileName */ private static void readProperties(String fileName) &#123; try &#123; props = new Properties(); InputStreamReader inputStreamReader = new InputStreamReader(new FileInputStream(new File(fileName)), &quot;utf-8&quot;); props.load(inputStreamReader); &#125; catch (Exception e) &#123; logger.error(&quot;加载配置文件失败！&quot;); e.printStackTrace(); &#125; &#125; /** * 根据key读取对应的value * * @param key * @return */ public static String get(String key) &#123; return props.getProperty(key); &#125; /** * 得到所有的配置信息 * * @return */ public static Map&lt;?, ?&gt; getAll() &#123; Map&lt;String, String&gt; map = new HashMap&lt;String, String&gt;(); Enumeration&lt;?&gt; enu = props.propertyNames(); while (enu.hasMoreElements()) &#123; String key = (String) enu.nextElement(); String value = props.getProperty(key); map.put(key, value); &#125; return map; &#125; 使用方法首先在本地环境变量中配置一个环境变量，名称为pesdk_home，当然你自己也可以随便定义，然后在该环境变量对应的路径下创建conf文件夹，再在conf文件夹下创建db.properties文件，你的配置项就可以添加到该配置文件中了，使用的时候，直接调用get方法，传入响应的key就可以获得value，赶紧试试吧。 ps如果大家在使用logger打印不出任何东西的时候，可能原因是你没有配置log4j的打印等级，这里就粘贴一下log4j的配置文件吧。这个配置文件的功能是error及fatal级日志打印到一个文件中，info及warn打印到另一个文件中，分日期打印。 123456789101112131415161718192021222324252627# Root logger optionlog4j.rootLogger=INFO, file, stdout # Direct log messages to a log filelog4j.appender.file=org.apache.log4j.RollingFileAppenderlog4j.appender.file.File=DFTSystemWeb2.loglog4j.appender.file.MaxFileSize=10MBlog4j.appender.file.MaxBackupIndex=1log4j.appender.file.layout=org.apache.log4j.PatternLayoutlog4j.appender.file.layout.ConversionPattern=[%d&#123;dd/MM/yy HH:mm:ss:sss z&#125;] %5p %c&#123;1&#125;:%L - %m%n# Direct log messages to stdoutlog4j.appender.stdout=org.apache.log4j.ConsoleAppenderlog4j.appender.stdout.Target=System.outlog4j.appender.stdout.layout=org.apache.log4j.PatternLayoutlog4j.appender.stdout.layout.ConversionPattern=[%d&#123;dd/MM/yy HH:mm:ss:sss z&#125;] %5p %c&#123;1&#125;:%L - %m%nlog4j.logger.org.eclipse.jetty=INFOlog4j.logger.org.springframework=INFOlog4j.logger.com.mchange=ERRORlog4j.logger.org.hibernate=INFO#log4j.logger.org.hibernate.type=tracelog4j.logger.com.tulando.common.filter.MethodProfileAspect=info,ProfileAspectlog4j.appender.ProfileAspect=org.apache.log4j.RollingFileAppenderlog4j.appender.ProfileAspect.File=api-profile.loglog4j.appender.ProfileAspect.MaxFileSize=1024KBlog4j.appender.ProfileAspect.MaxBackupIndex=5log4j.appender.ProfileAspect.Append=truelog4j.appender.ProfileAspect.layout=org.apache.log4j.PatternLayoutlog4j.appender.ProfileAspect.layout.ConversionPattern=%d&#123;yyyy-MM-dd HH:mm:ss,SSS&#125; [%c]-[%p] %m%n 整个方式到这里就配置完成了，如果在使用的过程中，有什么问题，或者有值得优化的地方，请联系我chenzuoli709@gmail.com.]]></content>
      <categories>
        <category>工具类</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[操作mysql工具类]]></title>
    <url>%2F2018%2F04%2F02%2F%E6%93%8D%E4%BD%9Cmysql%E5%B7%A5%E5%85%B7%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[下面介绍的是操作mysql的工具类，集成增删改查等功能方法，使用dbcp数据库连接池，让你的程序更高效。具体请看详情。 备注：代码环境jdk8（jdk7也可以） maven项目依赖12345678910111213141516&lt;!-- https://mvnrepository.com/artifact/mysql/mysql-connector-java --&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.41&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-dbcp2&lt;/artifactId&gt; &lt;version&gt;2.1.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-pool2&lt;/artifactId&gt; &lt;version&gt;2.4.2&lt;/version&gt;&lt;/dependency&gt; 数据库连接配置文件配置文件jdbc.properties放置在项目resources目录下，配置如下： 123456789driverClassName=com.mysql.jdbc.Driverurl=jdbc:mysql://localhost:3306/zs1?useSSL=falseusername=rootpassword=rootinitialSize=10maxIdle=5minIdle=2autoReconnect=trueautoReconnectForPools=true 具体代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270import com.payegis.czl.model.QueryLogHistory;import org.apache.commons.dbcp2.BasicDataSourceFactory;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import javax.sql.DataSource;import java.io.IOException;import java.sql.*;import java.util.*;/** * User: 陈作立 * Date: 2018/2/2 * Time: 13:39 * Description: 操作mysql数据库工具类 * Ps: mysql */public class DBCPUtil &#123; private static Logger loger = LoggerFactory.getLogger(DBCPUtil.class); private static DataSource dataSource = null; static &#123; loger.info(&quot;---------开始初始化数据库连接池---------&quot;); Properties prop = new Properties(); try &#123; prop.load(DBCPUtil.class.getClassLoader().getResourceAsStream(&quot;jdbc.properties&quot;)); dataSource = BasicDataSourceFactory.createDataSource(prop); &#125; catch (IOException e) &#123; loger.error(&quot;---------加载[jdbc.properties]失败---------&quot;, e); &#125; catch (Exception e) &#123; loger.error(&quot;----------初始化数据库连接池异常失败---------&quot;, e); &#125; loger.info(&quot;---------数据库连接池初始化完成---------&quot;); &#125; /** * 获取数据库连接 * * @return */ public static Connection getConnection() &#123; Connection conn = null; if (conn != null) &#123; return conn; &#125; try &#123; conn = dataSource.getConnection(); &#125; catch (SQLException e) &#123; loger.error(&quot;---------数据库连接池获取连接异常---------&quot;, e); &#125; return conn; &#125; /** * 关闭数据库连接 * * @param connection 数据库连接 */ public static void close(Connection connection) &#123; if (connection != null) &#123; try &#123; connection.close(); &#125; catch (SQLException e) &#123; loger.error(&quot;---------关闭Connection异常---------&quot;, e); &#125; &#125; &#125; /** * 关闭数据库连接 * * @param conn 数据库连接 * @param stat 预编译 */ public static void close(Connection conn, Statement stat) &#123; try &#123; if (stat != null) &#123; stat.close(); &#125; &#125; catch (SQLException e) &#123; loger.error(&quot;---------关闭Connection、PreparedStatement异常---------&quot;, e); &#125; finally &#123; close(conn); &#125; &#125; /** * 关闭数据库连接 * * @param conn 数据库连接 * @param stat 预编译 * @param rs 结果集 */ public static void close(Connection conn, Statement stat, ResultSet rs) &#123; try &#123; if (rs != null) &#123; rs.close(); &#125; &#125; catch (SQLException e) &#123; loger.error(&quot;---------关闭ResultSet异常---------&quot;, e); &#125; finally &#123; close(conn, stat); &#125; &#125; /** * 执行查询 * * @param sql * @param params * @return */ public static List&lt;Map&lt;String, Object&gt;&gt; executeQuery(String sql, Object... params) &#123; List&lt;Map&lt;String, Object&gt;&gt; rowDataList = new ArrayList&lt;Map&lt;String, Object&gt;&gt;(); Connection conn = null; PreparedStatement stat = null; ResultSet resultSet = null; try &#123; conn = getConnection(); stat = conn.prepareStatement(sql); stat.setFetchSize(10000); setStatParams(stat, params); resultSet = stat.executeQuery(); rowDataList = getResultList(resultSet); &#125; catch (SQLException e) &#123; loger.error(&quot;---------数据查询异常[&quot; + sql + &quot;]---------&quot;, e); &#125; finally &#123; close(conn, stat, resultSet); &#125; return rowDataList; &#125; /** * 更新数据 * * @param sql sql语句 * @param params 参数 * @return 更新成功:true 更新失败:false */ public static boolean executeUpdate(String sql, Object... params) &#123; boolean isUpdated = false; Connection conn = null; PreparedStatement stat = null; try &#123; conn = getConnection(); conn.setAutoCommit(false); stat = conn.prepareStatement(sql); setStatParams(stat, params); int updatedNum = stat.executeUpdate(); isUpdated = updatedNum == 1; conn.commit(); &#125; catch (SQLException e) &#123; try &#123; conn.rollback(); &#125; catch (SQLException e1) &#123; e1.printStackTrace(); &#125; loger.error(&quot;---------更新失败! sql:[&quot; + sql + &quot;], params:[&quot; + Arrays.toString(params) + &quot;]---------&quot;, e); &#125; finally &#123; close(conn, stat); &#125; return isUpdated; &#125; /** * 执行批处理 * * @param sqlList sql语句集合 * @return */ public static boolean executeBatch(List&lt;String&gt; sqlList) &#123; if (sqlList == null || sqlList.isEmpty()) &#123; return true; &#125; Connection conn = null; Statement stat = null; try &#123; conn = getConnection(); conn.setAutoCommit(false); stat = conn.createStatement(); for (String sql : sqlList) &#123; stat.addBatch(sql); &#125; stat.executeBatch(); conn.commit(); return true; &#125; catch (SQLException e) &#123; try &#123; conn.rollback(); loger.error(&quot;---------批处理异常，执行回滚---------&quot;); &#125; catch (SQLException e1) &#123; loger.error(&quot;---------回滚异常---------&quot;, e1); &#125; loger.error(&quot;---------执行批处理异常---------&quot;); loger.error(&quot;---------批处理异常sql：&quot; + Arrays.toString(sqlList.toArray())); &#125; finally &#123; try &#123; if (conn != null) &#123; conn.setAutoCommit(true); &#125; &#125; catch (SQLException e) &#123; loger.error(&quot;---------设置自动提交异常---------&quot;, e); &#125; close(conn, stat); &#125; return false; &#125; /** * 获取列名及数据 * * @param rs 数据集 * @return */ private static List&lt;Map&lt;String, Object&gt;&gt; getResultList(ResultSet rs) throws SQLException &#123; List&lt;Map&lt;String, Object&gt;&gt; rowDataList = new ArrayList&lt;Map&lt;String, Object&gt;&gt;(); List&lt;String&gt; colNameList = getColumnName(rs); while (rs.next()) &#123; Map&lt;String, Object&gt; rowData = new HashMap&lt;String, Object&gt;(); for (String colName : colNameList) &#123; rowData.put(colName, rs.getObject(colName)); &#125; if (!rowData.isEmpty()) &#123; rowDataList.add(rowData); &#125; &#125; return rowDataList; &#125; /** * 获取列名 * * @param rs 数据集 * @return */ private static List&lt;String&gt; getColumnName(ResultSet rs) throws SQLException &#123; List&lt;String&gt; columnList = new ArrayList&lt;String&gt;(); try &#123; ResultSetMetaData metaData = rs.getMetaData(); int columnCount = metaData.getColumnCount(); for (int i = 1; i &lt;= columnCount; i++) &#123; columnList.add(metaData.getColumnName(i)); &#125; &#125; catch (SQLException e) &#123; loger.info(&quot;------获取表列表异常------&quot;, e); throw e; &#125; return columnList; &#125; /** * 设置参数 * * @param stat 预编译 * @param params 参数 */ private static void setStatParams(PreparedStatement stat, Object... params) throws SQLException &#123; if (stat != null &amp;&amp; params != null) &#123; try &#123; for (int len = params.length, i = 1; i &lt;= len; i++) &#123; stat.setObject(i, params[i - 1]); &#125; &#125; catch (SQLException e) &#123; loger.error(&quot;------设置sql参数异常---------&quot;); throw e; &#125; &#125; &#125;&#125; 好了，到这里就结束了，这个类基本可以满足操作mysql的需求了，大家放心使用吧，如果有什么问题，或者可以优化的地方，欢迎大家email我chenzuoli709@gmail.com]]></content>
      <categories>
        <category>工具类</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>utils</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[身份证号校验工具类IdentityUtil]]></title>
    <url>%2F2018%2F03%2F30%2F%E8%BA%AB%E4%BB%BD%E8%AF%81%E5%8F%B7%E6%A0%A1%E9%AA%8C%E5%B7%A5%E5%85%B7%E7%B1%BBIdentityUtil%2F</url>
    <content type="text"><![CDATA[一个人的身份证号，每个字符都有他独特的含义，前2位代表省、自治区、直辖市代码，3-4位代表地级市、盟、自治州代码，5-6位代表县、县级市、区代码，7-14位代表出生年月日，15-17位代表当天出生的顺序号，奇数代表男，偶数代表女，18位为校验码，由0-9、X组成，这个校验码的由来，是由前17位数字计算得来，具体计算方式，可以参考下述代码。 本代码介绍的是校验身份证的合法性工具类，具体如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128import java.util.Calendar;import java.util.GregorianCalendar;import java.util.HashMap;import java.util.Map;/** * 身份证验证的工具（支持5位或18位省份证） * 身份证号码结构： * 17位数字和1位校验码：6位地址码数字，8位生日数字，3位出生时间顺序号，1位校验码。 * 地址码（前6位）：表示对象常住户口所在县（市、镇、区）的行政区划代码，按GB/T2260的规定执行。 * 出生日期码，（第七位 至十四位）：表示编码对象出生年、月、日，按GB按GB/T7408的规定执行，年、月、日代码之间不用分隔符。 * 顺序码（第十五位至十七位）：表示在同一地址码所标示的区域范围内，对同年、同月、同日出生的人编订的顺序号， * 顺序码的奇数分配给男性，偶数分配给女性。 * 校验码（第十八位数）： * 十七位数字本体码加权求和公式 s = sum(Ai*Wi), i = 0,,16，先对前17位数字的权求和； * Ai:表示第i位置上的身份证号码数字值.Wi:表示第i位置上的加权因.Wi: 7 9 10 5 8 4 2 1 6 3 7 9 10 5 8 4 2； * 计算模 Y = mod(S, 11) * 通过模得到对应的校验码 Y: 0 1 2 3 4 5 6 7 8 9 10 校验码: 1 0 X 9 8 7 6 5 4 3 2 */public class IdentityUtil &#123; final static Map&lt;Integer, String&gt; zoneNum = new HashMap&lt;Integer, String&gt;(); static &#123; zoneNum.put(11, &quot;北京&quot;); zoneNum.put(12, &quot;天津&quot;); zoneNum.put(13, &quot;河北&quot;); zoneNum.put(14, &quot;山西&quot;); zoneNum.put(15, &quot;内蒙古&quot;); zoneNum.put(21, &quot;辽宁&quot;); zoneNum.put(22, &quot;吉林&quot;); zoneNum.put(23, &quot;黑龙江&quot;); zoneNum.put(31, &quot;上海&quot;); zoneNum.put(32, &quot;江苏&quot;); zoneNum.put(33, &quot;浙江&quot;); zoneNum.put(34, &quot;安徽&quot;); zoneNum.put(35, &quot;福建&quot;); zoneNum.put(36, &quot;江西&quot;); zoneNum.put(37, &quot;山东&quot;); zoneNum.put(41, &quot;河南&quot;); zoneNum.put(42, &quot;湖北&quot;); zoneNum.put(43, &quot;湖南&quot;); zoneNum.put(44, &quot;广东&quot;); zoneNum.put(45, &quot;广西&quot;); zoneNum.put(46, &quot;海南&quot;); zoneNum.put(50, &quot;重庆&quot;); zoneNum.put(51, &quot;四川&quot;); zoneNum.put(52, &quot;贵州&quot;); zoneNum.put(53, &quot;云南&quot;); zoneNum.put(54, &quot;西藏&quot;); zoneNum.put(61, &quot;陕西&quot;); zoneNum.put(62, &quot;甘肃&quot;); zoneNum.put(63, &quot;青海&quot;); zoneNum.put(64, &quot;宁夏&quot;); zoneNum.put(65, &quot;新疆&quot;); zoneNum.put(71, &quot;台湾&quot;); zoneNum.put(81, &quot;香港&quot;); zoneNum.put(82, &quot;澳门&quot;); zoneNum.put(91, &quot;外国&quot;); &#125; final static int[] PARITYBIT = &#123;&apos;1&apos;, &apos;0&apos;, &apos;X&apos;, &apos;9&apos;, &apos;8&apos;, &apos;7&apos;, &apos;6&apos;, &apos;5&apos;, &apos;4&apos;, &apos;3&apos;, &apos;2&apos;&#125;; final static int[] POWER_LIST = &#123; 7, 9, 10, 5, 8, 4, 2, 1, 6, 3, 7, 9, 10, 5, 8, 4, 2&#125;; /** * 身份证验证 *@param s 号码内容 *@return 是否有效 null和&quot;&quot; 都是false */ public static boolean checkIDCard(String certNo)&#123; if(certNo == null || (certNo.length() != 15 &amp;&amp; certNo.length() != 18)) return false; final char[] cs = certNo.toUpperCase().toCharArray(); //校验位数 int power = 0; for(int i=0; i&lt;cs.length; i++)&#123; if(i==cs.length-1 &amp;&amp; cs[i] == &apos;X&apos;) break;//最后一位可以 是X或x if(cs[i]&lt;&apos;0&apos; || cs[i]&gt;&apos;9&apos;) return false; if(i &lt; cs.length -1)&#123; power += (cs[i] - &apos;0&apos;) * POWER_LIST[i]; &#125; &#125; //校验区位码 if(!zoneNum.containsKey(Integer.valueOf(certNo.substring(0,2))))&#123; return false; &#125; //校验年份 String year = certNo.length() == 15 ? getIdcardCalendar() + certNo.substring(6,8) :certNo.substring(6, 10); final int iyear = Integer.parseInt(year); if(iyear &lt; 1900 || iyear &gt; Calendar.getInstance().get(Calendar.YEAR)) return false;//1900年的PASS，超过今年的PASS //校验月份 String month = certNo.length() == 15 ? certNo.substring(8, 10) : certNo.substring(10,12); final int imonth = Integer.parseInt(month); if(imonth &lt;1 || imonth &gt;12)&#123; return false; &#125; //校验天数 String day = certNo.length() ==15 ? certNo.substring(10, 12) : certNo.substring(12, 14); final int iday = Integer.parseInt(day); if(iday &lt; 1 || iday &gt; 31) return false; //校验&quot;校验码&quot; if(certNo.length() == 15) return true; return cs[cs.length -1 ] == PARITYBIT[power % 11]; &#125; private static int getIdcardCalendar() &#123; GregorianCalendar curDay = new GregorianCalendar(); int curYear = curDay.get(Calendar.YEAR); int year2bit = Integer.parseInt(String.valueOf(curYear).substring(2)); return year2bit; &#125; public static void main(String[] args) &#123; boolean mark = checkIDCard(&quot;650105195604040056&quot;); System.out.println(mark); &#125;&#125; 到这里就结束了，如有什么问题，请联系chenzuoli709@gmail.com]]></content>
      <categories>
        <category>工具类</category>
      </categories>
      <tags>
        <tag>utils</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java操作hbase工具类]]></title>
    <url>%2F2018%2F03%2F29%2Fjava%E6%93%8D%E4%BD%9Chbase%E5%B7%A5%E5%85%B7%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[HBase是一个基于HDFS的数据库，拥有高可用、大量数据存储、列式存储等特点，在非结构化数据与半结构化数据存储方面，有很大的优势。我们一般测试时使用hbase shell命令行的方式来操作hbase数据库比较方便，但是在数据逻辑处理比较复杂时，那肯定是用它提供的API来操作更方便啦，下面就来给出一个java版操作hbase的工具类，提供给大家，我自己也一直使用这个类。 备注：本工具类使用的环境：hbase1.4.1 jdk1.8 hadoop3.0 maven项目添加依赖123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&lt;!--hadoop/hbase都要依赖(RPC通信)，注意protobuf-java的版本，hbase1.4.1自带的protobuf-java版本是2.5.0的，所以如果你的程序是跑在服务器上的，需要跟服务器一致，不然会出现NoClsssFoundError--&gt;&lt;!--https://mvnrepository.com/artifact/com.google.protobuf/protobuf-java--&gt;&lt;dependency&gt; &lt;groupId&gt;com.google.protobuf&lt;/groupId&gt; &lt;artifactId&gt;protobuf-java&lt;/artifactId&gt; &lt;version&gt;3.5.1&lt;/version&gt;&lt;/dependency&gt;&lt;!--hbase--&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;version&gt;$&#123;zookeeper.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-client&lt;/artifactId&gt; &lt;version&gt;$&#123;hbase.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-common&lt;/artifactId&gt; &lt;version&gt;$&#123;hbase.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;!-- https://mvnrepository.com/artifact/org.apache.hbase/hbase --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase&lt;/artifactId&gt; &lt;version&gt;1.4.1&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt;&lt;/dependency&gt;&lt;!-- https://mvnrepository.com/artifact/org.apache.hbase/hbase-spark --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-spark&lt;/artifactId&gt; &lt;version&gt;1.2.0-cdh5.14.0&lt;/version&gt;&lt;/dependency&gt;&lt;!-- https://mvnrepository.com/artifact/org.apache.hbase/hbase-server --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-server&lt;/artifactId&gt; &lt;version&gt;1.4.1&lt;/version&gt;&lt;/dependency&gt;&lt;!-- https://mvnrepository.com/artifact/fastutil/fastutil 这里使用fastutil，对比javautil自带集合类，它的读写性能更优，尤其在大数据的情况下，所以当你写的mr或者spark程序，使用到fastutil，会提升一些性能--&gt;&lt;dependency&gt; &lt;groupId&gt;fastutil&lt;/groupId&gt; &lt;artifactId&gt;fastutil&lt;/artifactId&gt; &lt;version&gt;5.0.5&lt;/version&gt;&lt;/dependency&gt; 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474package com.payegis.czl.util;import it.unimi.dsi.fastutil.objects.ObjectArrayList;import net.sf.json.JSONObject;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.HBaseConfiguration;import org.apache.hadoop.hbase.HColumnDescriptor;import org.apache.hadoop.hbase.HTableDescriptor;import org.apache.hadoop.hbase.TableName;import org.apache.hadoop.hbase.client.*;import org.apache.hadoop.hbase.filter.*;import org.apache.hadoop.hbase.io.compress.Compression;import org.apache.hadoop.hbase.util.Bytes;import org.apache.log4j.Logger;import java.io.BufferedReader;import java.io.FileInputStream;import java.io.IOException;import java.io.InputStreamReader;import java.util.*;/** * User: chenzuoli * Date: 2018/3/29 * Time: 9:20 * Description: Java操作HBase工具类 * Ps: Java HBase */public class HBaseUtil &#123; public static Configuration conf; public static Connection connection; public static Admin admin; public static Table table; private static Logger logger = Logger.getLogger(HBaseUtil.class); static &#123; try &#123; conf = HBaseConfiguration.create(); conf.set(&quot;hbase.zookeeper.property.clientPort&quot;, &quot;2181&quot;); conf.set(&quot;hbase.zookeeper.quorum&quot;, &quot;dev11,dev13,dev14&quot;); connection = ConnectionFactory.createConnection(conf); admin = connection.getAdmin(); logger.info(&quot;初始化hbase连接成功！&quot;); &#125; catch (IOException e) &#123; logger.error(&quot;初始化hbase连接异常！&quot;); e.printStackTrace(); &#125; catch (Exception e) &#123; logger.error(&quot;初始化hbase连接异常！&quot;); e.printStackTrace(); &#125; &#125; /** * @Description: 建表，如果表存在，那么不创建。如果未指定列族名称，默认定义一个cf1 * @Param: [tableName, familyName] * @return: boolean * @Author: CHEN ZUOLI * @Date: 2018/3/29 * @Time: 9:24 */ public static boolean createTable(String tableName, String familyName) &#123; boolean flag = false; if (familyName == null || familyName.length() == 0) &#123; familyName = &quot;cf1&quot;; &#125; TableName tbl = TableName.valueOf(tableName); Admin admin = null; try &#123; admin = connection.getAdmin(); if (admin.tableExists(tbl)) &#123; logger.info(&quot;Table &quot; + tbl.getNameAsString() + &quot; is already exists!&quot;); return flag; &#125; HTableDescriptor tableDescriptor = new HTableDescriptor(tbl); tableDescriptor.addFamily(new HColumnDescriptor(familyName).setCompressionType(Compression.Algorithm.SNAPPY)); admin.createTable(tableDescriptor); logger.info(&quot;Create table &quot; + tbl.getNameAsString() + &quot; success!&quot;); flag = true; &#125; catch (IOException e) &#123; logger.error(&quot;Create table failed!&quot;); e.printStackTrace(); &#125; catch (Exception e) &#123; logger.error(&quot;Create table failed!&quot;); e.printStackTrace(); &#125; return flag; &#125; /** * @Description: 插入一条数据到hbase * @Param: [connection, tableName, rowkey, columnFamily, key, value] * @return: void * @Author: CHEN ZUOLI * @Date: 2018/3/28 * @Time: 14:06 */ public static void insertOne(String tableName, String rowkey, String columnFamily, String key, String value) &#123; Table table = null; try &#123; table = connection.getTable(TableName.valueOf(tableName)); Put put = new Put(Bytes.toBytes(rowkey)); put.addColumn(Bytes.toBytes(columnFamily), Bytes.toBytes(key), Bytes.toBytes(value)); table.put(put); &#125; catch (IOException e) &#123; logger.error(&quot;insert hbase failed: &quot; + rowkey + &quot;,&quot; + columnFamily + &quot;,&quot; + key + &quot;,&quot; + value); e.printStackTrace(); &#125; catch (Exception e) &#123; logger.error(&quot;insert hbase failed: &quot; + rowkey + &quot;,&quot; + columnFamily + &quot;,&quot; + key + &quot;,&quot; + value); e.printStackTrace(); &#125; finally &#123; closeTableAndResult(table, null); &#125; &#125; /** * @Description: 批量插入数据到hbase * @Param: [filePath, tableName, familyName] * @return: void * @Author: CHEN ZUOLI * @Date: 2018/3/30 * @Time: 13:31 */ public static void insertBatch(String filePath, String tableName, String familyName) &#123; ObjectArrayList&lt;Put&gt; puts = new ObjectArrayList&lt;&gt;(); Table table = null; FileInputStream fis = null; BufferedReader br = null; try &#123; table = connection.getTable(TableName.valueOf(tableName)); fis = new FileInputStream(filePath); br = new BufferedReader(new InputStreamReader(fis)); String line = br.readLine(); while (line != null) &#123; JSONObject lineJsonObject = JSONObject.fromObject(line); String rowkey = MD5Utils.strToMd5_16(UUID.randomUUID().toString()); Set&lt;String&gt; keys = lineJsonObject.keySet(); Put put = new Put(Bytes.toBytes(rowkey)); for (String key : keys) &#123; put.addColumn(Bytes.toBytes(familyName), Bytes.toBytes(key), Bytes.toBytes(lineJsonObject.optString(key))); puts.add(put); &#125; line = br.readLine(); &#125; table.put(puts); &#125; catch (IOException e) &#123; logger.error(&quot;insert batch data to hbase failed!&quot;); e.printStackTrace(); &#125; catch (Exception e) &#123; logger.error(&quot;insert batch data to hbase failed!&quot;); e.printStackTrace(); &#125; finally &#123; try &#123; if (table != null) table.close(); if (fis != null) fis.close(); if (br != null) br.close(); &#125; catch (IOException e) &#123; logger.error(&quot;close table or stream failed!&quot;); e.printStackTrace(); &#125; &#125; &#125; /** * @Description: 批量插入数据到hbase * @Param: [rows, tableName, familyName] * @return: void * @Author: CHEN ZUOLI * @Date: 2018/4/2 * @Time: 10:19 */ public static void insertBatch(List&lt;Map&lt;String, Object&gt;&gt; rows, String tableName, String familyName) &#123; ObjectArrayList&lt;Put&gt; puts = new ObjectArrayList&lt;&gt;(); Table table = null; try &#123; table = connection.getTable(TableName.valueOf(tableName)); for (Map&lt;String, Object&gt; row : rows) &#123; String rowkey = MD5Utils.strToMd5_16(UUID.randomUUID().toString()); Put put = new Put(Bytes.toBytes(rowkey)); for (Map.Entry&lt;String, Object&gt; kv : row.entrySet()) &#123; String key = kv.getKey(); Object value = kv.getValue(); if (value == null) &#123; put.addColumn(Bytes.toBytes(familyName), Bytes.toBytes(key), null); &#125; else &#123; put.addColumn(Bytes.toBytes(familyName), Bytes.toBytes(key), Bytes.toBytes(value.toString())); &#125; &#125; puts.add(put); &#125; table.put(puts); &#125; catch (IOException e) &#123; logger.error(&quot;insert batch data to hbase failed!&quot;); e.printStackTrace(); &#125; catch (Exception e) &#123; logger.error(&quot;insert batch data to hbase failed!&quot;); e.printStackTrace(); &#125; finally &#123; closeTableAndResult(table, null); &#125; &#125; /** * @Description: 删除一张表 * @Param: [tableName] * @return: boolean * @Author: CHEN ZUOLI * @Date: 2018/3/29 * @Time: 9:40 */ public static boolean dropTable(String tableName) &#123; boolean flag = false; try &#123; admin.disableTable(TableName.valueOf(tableName)); admin.deleteTable(TableName.valueOf(tableName)); flag = true; &#125; catch (IOException e) &#123; logger.error(&quot;delete &quot; + tableName + &quot; table failed!&quot;); e.printStackTrace(); &#125; catch (Exception e) &#123; logger.error(&quot;delete &quot; + tableName + &quot; table failed!&quot;); e.printStackTrace(); &#125; return flag; &#125; /** * @Description: 根据rowkey删除一条记录 * @Param: [tablename, rowkey] * @return: boolean * @Author: CHEN ZUOLI * @Date: 2018/3/29 * @Time: 9:40 */ public static boolean deleteOneRowByRowkey(String tablename, String rowkey) &#123; boolean flag = false; try &#123; Delete d = new Delete(rowkey.getBytes()); table.delete(d); logger.info(&quot;delete row &quot; + rowkey + &quot; success!&quot;); flag = true; &#125; catch (IOException e) &#123; logger.error(&quot;delete row &quot; + rowkey + &quot; failed!&quot;); e.printStackTrace(); &#125; catch (Exception e) &#123; logger.error(&quot;delete row &quot; + rowkey + &quot; failed!&quot;); e.printStackTrace(); &#125; return flag; &#125; /** * @Description: 批量删除rowkey * @Param: [tablename, rowkeyList] * @return: boolean * @Author: CHEN ZUOLI * @Date: 2018/3/29 * @Time: 9:47 */ public static boolean deleteBatchRowByRowkey(String tablename, List&lt;String&gt; rowkeyList) &#123; boolean flag = false; ObjectArrayList&lt;Delete&gt; listDelete = new ObjectArrayList&lt;&gt;(); try &#123; for (int i = 0; i &lt; rowkeyList.size(); i++) &#123; Delete delete = new Delete(rowkeyList.get(i).getBytes()); listDelete.add(delete); &#125; table.delete(listDelete); logger.info(&quot;delete row list &quot; + rowkeyList + &quot; success!&quot;); flag = true; &#125; catch (IOException e) &#123; logger.error(&quot;delete row &quot; + rowkeyList + &quot; failed!&quot;); e.printStackTrace(); &#125; catch (Exception e) &#123; logger.error(&quot;delete row &quot; + rowkeyList + &quot; failed!&quot;); e.printStackTrace(); &#125; return flag; &#125; /** * @Description: 查询表中所有数据 * @Param: [tableName] * @return: List&lt;HashMap&lt;String,HashMap&lt;String,String&gt;&gt;&gt; * @Author: CHEN ZUOLI * @Date: 2018/3/29 * @Time: 9:51 */ public static List&lt;HashMap&lt;String, HashMap&lt;String, String&gt;&gt;&gt; queryAll(String tableName) &#123; ObjectArrayList&lt;HashMap&lt;String, HashMap&lt;String, String&gt;&gt;&gt; rowMapList = new ObjectArrayList&lt;&gt;(); // &lt;familyName, &lt;columnName, columnValue&gt;&gt; Table table = null; ResultScanner rs = null; try &#123; table = connection.getTable(TableName.valueOf(tableName));// ResultScanner rs = table.getScanner(new Scan().setMaxVersions()); // 获取所有版本数据 rs = table.getScanner(new Scan()); for (Result r : rs) &#123; rowMapList.add(resolveResult(r)); &#125; &#125; catch (IOException e) &#123; logger.error(&quot;Get all table data failed!&quot;); e.printStackTrace(); &#125; finally &#123; closeTableAndResult(table, rs); &#125; return rowMapList; &#125; /** * @Description: 单条件查询, 根据rowkey查询唯一一条记录 * @Param: [tableName, rowKey] * @return: List&lt;HashMap&lt;String,HashMap&lt;String,String&gt;&gt;&gt; * @Author: CHEN ZUOLI * @Date: 2018/3/29 * @Time: 10:47 */ public static List&lt;HashMap&lt;String, HashMap&lt;String, String&gt;&gt;&gt; queryByCondition(String tableName, String rowKey) &#123; ObjectArrayList&lt;HashMap&lt;String, HashMap&lt;String, String&gt;&gt;&gt; rowMapList = new ObjectArrayList&lt;&gt;(); // &lt;familyName, &lt;columnName, columnValue&gt;&gt; Table table = null; try &#123; Get get = new Get(rowKey.getBytes());// get.setMaxVersions(); // 获取所有版本数据 table = connection.getTable(TableName.valueOf(tableName)); Result r = table.get(get); rowMapList.add(resolveResult(r)); logger.info(&quot;获得到rowkey: &quot; + new String(r.getRow())); &#125; catch (IOException e) &#123; logger.error(&quot;Get table one data failed!&quot;); e.printStackTrace(); &#125; finally &#123; closeTableAndResult(table, null); &#125; return rowMapList; &#125; /** * @Description: 单条件按查询，查询多条记录 * @Param: [tableName] * @return: List&lt;HashMap&lt;String,HashMap&lt;String,String&gt;&gt;&gt; * @Author: CHEN ZUOLI * @Date: 2018/3/29 * @Time: 13:16 */ public static List&lt;HashMap&lt;String, HashMap&lt;String, String&gt;&gt;&gt; queryByCondition(String tableName, String familyName, String columnName, String columnValue) &#123; ObjectArrayList&lt;HashMap&lt;String, HashMap&lt;String, String&gt;&gt;&gt; rowMapList = new ObjectArrayList&lt;&gt;(); // &lt;familyName, &lt;columnName, columnValue&gt;&gt; Table table = null; ResultScanner rs = null; try &#123; table = connection.getTable(TableName.valueOf(tableName)); Filter filter = new SingleColumnValueFilter(Bytes.toBytes(familyName), Bytes.toBytes(columnName), CompareFilter.CompareOp.EQUAL, Bytes.toBytes(columnValue)); // 当列columnName的值为columnValue时进行查询 Scan s = new Scan(); s.setFilter(filter); rs = table.getScanner(s); for (Result r : rs) &#123; rowMapList.add(resolveResult(r)); &#125; &#125; catch (Exception e) &#123; logger.error(&quot;query with one filter failed!&quot;); e.printStackTrace(); &#125; finally &#123; closeTableAndResult(table, rs); &#125; return rowMapList; &#125; /** * @Description: 组合条件查询 * @Param: [tableName] * @return: List&lt;HashMap&lt;String,HashMap&lt;String,String&gt;&gt;&gt; * @Author: CHEN ZUOLI * @Date: 2018/3/29 * @Time: 13:26 */ public static List&lt;HashMap&lt;String, HashMap&lt;String, String&gt;&gt;&gt; queryByCondition(String tableName, String familyName, HashMap&lt;String, String&gt; paramMap) &#123; ObjectArrayList&lt;HashMap&lt;String, HashMap&lt;String, String&gt;&gt;&gt; rowMapList = new ObjectArrayList&lt;&gt;(); // &lt;familyName, &lt;columnName, columnValue&gt;&gt; Table table = null; ResultScanner rs = null; try &#123; table = connection.getTable(TableName.valueOf(tableName)); FilterList filterList = new FilterList(); for (Map.Entry&lt;String, String&gt; entry : paramMap.entrySet()) &#123; Filter filter = new SingleColumnValueFilter(Bytes.toBytes(familyName), Bytes.toBytes(entry.getKey()), CompareFilter.CompareOp.EQUAL, Bytes.toBytes(entry.getValue())); filterList.addFilter(filter); &#125; Scan scan = new Scan(); scan.setFilter(filterList); rs = table.getScanner(scan); for (Result r : rs) &#123; rowMapList.add(resolveResult(r)); &#125; rs.close(); &#125; catch (Exception e) &#123; logger.error(&quot;query with more filter failed!&quot;); e.printStackTrace(); &#125; finally &#123; closeTableAndResult(table, rs); &#125; return rowMapList; &#125; /** * @Description: 查询hbase，匹配rowkey前缀为dianRong的行 * @Param: [tableName] * @return: java.util.List&lt;java.util.HashMap&lt;java.lang.String,java.util.HashMap&lt;java.lang.String,java.lang.String&gt;&gt;&gt; * @Author: CHEN ZUOLI * @Date: 2018/4/3 * @Time: 20:21 */ public static List&lt;HashMap&lt;String, HashMap&lt;String, String&gt;&gt;&gt; rowkeyFuzzyQuery(String tableName)&#123; ObjectArrayList&lt;HashMap&lt;String, HashMap&lt;String, String&gt;&gt;&gt; rowMapList = new ObjectArrayList&lt;&gt;(); Table table = null; ResultScanner rs = null; try &#123; table = connection.getTable(TableName.valueOf(tableName)); Scan scan = new Scan(); Filter filter = new RowFilter(CompareFilter.CompareOp.EQUAL, new RegexStringComparator(&quot;dianRong.*&quot;)); scan.setFilter(filter); rs = table.getScanner(scan); for (Result r : rs) &#123; rowMapList.add(resolveResult(r)); &#125; &#125; catch (Exception e) &#123; logger.error(&quot;query with more filter failed!&quot;); e.printStackTrace(); &#125; finally &#123; closeTableAndResult(table, rs); &#125; return rowMapList; &#125; /** * @Description: 解析查询hbase得到的结果，放入到HashMap中 * @Param: [result] * @return: java.util.HashMap&lt;String,HashMap&lt;String,String&gt;&gt; * @Author: CHEN ZUOLI * @Date: 2018/3/29 * @Time: 13:52 */ public static HashMap&lt;String, HashMap&lt;String, String&gt;&gt; resolveResult(Result result) &#123; HashMap&lt;String, HashMap&lt;String, String&gt;&gt; rowMap = new HashMap&lt;&gt;(); // &lt;familyName, &lt;columnName, columnValue&gt;&gt; HashMap&lt;String, String&gt; kvMap = new HashMap&lt;&gt;(); NavigableMap&lt;byte[], NavigableMap&lt;byte[], NavigableMap&lt;Long, byte[]&gt;&gt;&gt; map = result.getMap(); for (Map.Entry&lt;byte[], NavigableMap&lt;byte[], NavigableMap&lt;Long, byte[]&gt;&gt;&gt; entry : map.entrySet()) &#123; String familyName = new String(entry.getKey()); NavigableMap&lt;byte[], NavigableMap&lt;Long, byte[]&gt;&gt; valueInfoMap = entry.getValue(); for (Map.Entry&lt;byte[], NavigableMap&lt;Long, byte[]&gt;&gt; valueInfo : valueInfoMap.entrySet()) &#123; String key = new String(valueInfo.getKey()); NavigableMap&lt;Long, byte[]&gt; values = valueInfo.getValue(); Map.Entry&lt;Long, byte[]&gt; firstEntry = values.firstEntry(); Long timestampLastest = firstEntry.getKey(); String valueLastest = new String(firstEntry.getValue()); logger.info(&quot;familyName: &quot; + familyName + &quot;, key: &quot; + key + &quot;, value: &quot; + valueLastest + &quot;, timestamp: &quot; + timestampLastest);// for (Map.Entry&lt;Long, byte[]&gt; vals : values.entrySet()) &#123;// Long timestamp = vals.getKey();// String value = new String(vals.getValue());// &#125; kvMap.put(key, valueLastest); rowMap.put(familyName, kvMap); &#125; &#125; return rowMap; &#125; public static void closeTableAndResult(Table table, ResultScanner rs)&#123; try &#123; if (rs != null) rs.close(); if (table != null) table.close(); &#125; catch (IOException e) &#123; logger.error(&quot;close table failed!&quot;); e.printStackTrace(); &#125; &#125;&#125; 好了，文章到这里就结束了，如果大家在使用过程中，遇到什么问题，请联系我chenzuoli709@gmail.com。]]></content>
      <categories>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark累加器的使用方法]]></title>
    <url>%2F2018%2F03%2F17%2FAccumulator%20must%20be%20registered%20before%20send%20to%20executor%2F</url>
    <content type="text"><![CDATA[运行spark程序，使用到了累加器Accumulator，目前使用的是spark2.3.0，累加器Accumulator的定义方法变了，具体查看详细内容。 之前spark1.6.0时，累加器的定义及使用方式为： 123456Accumulator&lt;Integer&gt; accum = sc.accumulator(0);sc.parallelize(Arrays.asList(1, 2, 3, 4)).foreach(x -&gt; accum.add(x));// ...// 10/09/29 18:41:08 INFO SparkContext: Tasks finished in 0.317106 saccum.value();// returns 10 在spark2.3.0中，累加器的定义方式应该为： 123456LongAccumulator accum = jsc.sc().longAccumulator();sc.parallelize(Arrays.asList(1, 2, 3, 4)).foreach(x -&gt; accum.add(x));// ...// 10/09/29 18:41:08 INFO SparkContext: Tasks finished in 0.317106 saccum.value();// returns 10 之前的方式已被标记为Deprecated。也可以如此，先定义，在注册到SparkConf： 1234LongAccumulator countDftResult = new LongAccumulator();LongAccumulator countFailed = new LongAccumulator();sc.register(countDftResult); // 注册累加器sc.register(countFailed); 如果不注册，会出现Accumulator must be registered before send to executor异常。到这里就基本可以使用累加器了，谢谢大家，如果有什么问题，请留言。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git服务器端配置详解]]></title>
    <url>%2F2018%2F03%2F10%2FGit%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%AB%AF%E9%85%8D%E7%BD%AE%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[我们在公司中，一个项目在开发过程中必定要涉及到同事之间的协同作战，此时代码管理就必不可少了，程序员用的最多的就是git了吧，但是公司的代码是禁止上传到github上的，所以需要自己搭建一个内部的git server服务器供公司内部使用，下面来具体就介绍。 本安装教程适用于centos服务器，其他版本linux服务器步骤一样，运行命令会不相同，自己转换即可。配置git server，原理就是将客户端的公钥id_rsa.pub添加到服务端的密钥文件authorized_keys中，多个用户另起一行拼接到该文件中即可。下面介绍安装的两种方法： 方法一：yum安装安装git软件12yum install git -ygit --version git –version可以查看安装的git版本 12[git@hadoop3 gitrepo]$ git --versiongit version 1.8.3.1 系统是centos7，自带git版本太低，但可以使用。 添加git用户组和用户12groupadd gituseradd git -g git 创建登录证书1234su gitcdmkdir .ssh &amp;&amp; chmod 700 .sshtouch .ssh/authorized_keys &amp;&amp; chmod 600 .ssh/authorized_keys 这里注意，一定要设置authorized_keys文件的权限为600，权限太大或太小都会造成cannot access的问题，我就遇到过这个坑。 免秘钥复制客户端的公钥到服务端的authorized_keys文件中 初始化git仓库创建一个git仓库文件夹，专门存放项目的地方，并创建一个项目，初始化： 12345mkdir /srv/git -pcd /srv/gitmkdir project.gitcd project.gitgit init --bare bare的意思就是初始化一个裸仓库，并不存储用户push的数据，只存储元数据。 提交项目到git server下面的命令是在客户端（另一台机器，可以是windows，也可以是linux）上执行的： 12345678cd myprojectgit initgit config --global user.email “chenzuoli709@163.com”git config --global user.name “chenzuoli”git add .git commit -m “initial commit”git remote add origin git@gitserver:/srv/git/project.gitgit push origin master 注意：提交的时候需要将gitserver更换成自己的git服务器的ip或者映射域名。 克隆项目1git clone git@gitserver:/srv/git/project.git 也是要替换gitserver的。 方法二：自定义安装下载最新稳定版gitgit最新版下载 添加git用户组和用户12groupadd gituseradd git -g git 上传解压1234su gitcd $GIT_HOMExz -d git-2.9.5.tar.xztar xvf git-2.9.5.tar 环境准备1yum install curl-devel expat-devel gettext-devel openssl-devel zlib-devel perl-devel gcc -y 预编译、编译、安装123cd git-2.9.5./configure -prefix=$GIT_HOMEmake &amp;&amp; make install 其中GIT_HOME是自己指定的git安装目录。 配置配置跟方法一一样 添加链接上述步骤配置完成后，我们在push clone时会出现如下问题： 12345678bash: git-receive-pack: command not foundfatal: Could not read from remote repository.Please make sure you have the correct access rightsand the repository exists.bash: git-upload-pack: command not foundfatal: Could not read from remote repository.Please make sure you have the correct access rightsand the repository exists. 原因是自定义安装时git的执行文件不在/usr/bin下，添加链接即可： 12345ln -s /usr/local/gitserver/install/bin/git /usr/bin/gitln -s /usr/local/gitserver/install/bin/git-shell /usr/bin/git-shellln -s /usr/local/gitserver/install/bin/git-upload-archive /usr/bin/git-upload-archiveln -s /usr/local/gitserver/install/bin/git-upload-pack /usr/bin/git-upload-packln -s /usr/local/gitserver/install/bin/git-receive-pack /usr/bin/git-receive-pack 到这里配置就基本完成了，谢谢大家，如果有什么问题，请留言。]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ambari管理监控hadoop生态系统的环境安装及问题解答]]></title>
    <url>%2F2018%2F03%2F10%2Fambari%E7%AE%A1%E7%90%86%E7%9B%91%E6%8E%A7hadoop%E7%94%9F%E6%80%81%E7%B3%BB%E7%BB%9F%E7%9A%84%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85%E5%8F%8A%E9%97%AE%E9%A2%98%E8%A7%A3%E7%AD%94%2F</url>
    <content type="text"><![CDATA[首先来介绍下ambari，它是一个apache的一个顶级项目，hadoop生态组件的监控、管理工具，相比较于cloudera公司的CDH，它的特点是完全开源，一键部署安装、管理、监控大数据各组件，省时省力，下面就来介绍ambari环境是如何安装的。 本安装教程适用于操作系统centos7，在某一台服务器上安装，原理就是虚拟化该服务器成多个virtual box，然后启动ambari服务，管理这些虚拟机。 安装安装步骤可以参考官网：ambari.apache.org 环境准备1yum install lrzsz openssl openssh-clients git maven -y 下载安装VirtualBox、VagrantVirtualBox选择最新稳定版.rpm文件下载即可上传到服务器使用yum安装 下载ambari-vagrant123git clone https://github.com/u39kun/ambari-vagrant.gitcat ambari-vagrant/append-to-etc-hosts.txt &gt;&gt; /etc/hosts --配置ip、域名映射vagrant --生成密钥 启动VMs1cd ambari-vagrant 你可以看到在该文件夹下有许多centos的版本，官方说centos6.8对ambari的兼容性最好，我们就用centos6.8吧。 123cd centos6.8cp ~/.vagrant.d/insecure_private_key . --将密钥复制到当前文件夹，注意不要少了组后面的一个点，代表当前文件夹./up.sh 3 --启动3个virtual machine 正常的话，就启动了c6801 c6802 c6803这三台虚拟机 登录VMs12vagrant ssh c6801su - ssh没问题的话，说明安装是没问题的，下面来安装ambari-server，以root用户完成下面的操作。 安装ambari-server下载ambari的源，安装并启动ambari-server 1234wget -O /etc/yum.repos.d/ambari.repo http://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.5.1.0/ambari.repoyum install ambari-server -yambari-server setup -sambari-server start 启动成功后，我们可以访问ambari的web界面： http://c6801.ambari.apache.org:8080，初始的登录用户名和密码均为admin，以同样的方式可以访问c6802 c6803，然后我们就可以对着三台虚拟机进行安装hadoop生态的各个组件了。 问题安装过程中会出现各种问题，具体问题及解决方案如下： 启动虚拟盒的时候报错运行命令： 1./up.sh 3 错误日志如下： 1234There was an error while excuting &apos;VBoxMange&apos;, a CLI used by vagrant for controlling VirtualBox, The command and stderr is shown below.Command: [&quot;startvm&quot;, &quot;afb1736b-3bab-4d1a-a968-16aba764195a&quot;, &quot;--type&quot;, &quot;gui&quot;]Stderr: VBoxManage: error: The virtual machine &quot;centos68-c6801-1520048454672_80399&quot; has terminated upexpectedly during startup because of singal 6.VBoxManage: error: Details: code NS_ERROR_FAILURE (0x80004005), componnent MachineWrap, interface IMachine. 错误原因：linux系统中的kernel module与ambari需要使用的kernel模块版本不匹配，导致vboxdrv服务启动异常，可以使用命令查看vboxdrv服务的启动情况： 1systemctl status vboxdrv 解决办法：1.安装更新kernel 1yum install kernel -y 2.安装kernel-devel 1yum install kernel -y 3.重启服务器 1reboot 4.启动vboxdrv服务 12systemctl start vboxdrvsystemctl status vboxdrv --查看状态 到这里配置就基本完成了，谢谢大家，如果有什么问题，请留言。]]></content>
      <categories>
        <category>环境安装</category>
      </categories>
      <tags>
        <tag>ambari</tag>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[windows下gvim打开文件中文乱码]]></title>
    <url>%2F2018%2F02%2F08%2Fwindows%E4%B8%8Bvim%E6%89%93%E5%BC%80%E6%96%87%E4%BB%B6%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81%2F</url>
    <content type="text"><![CDATA[习惯了linux操作命令，突然有需要使用到windows cmd命令，例如安装某些软件时，需要用命令方式去安装： 1npm install hexo-cli 安装完成后，需要编辑一些配置文件，这个时候，去’计算机’中重新定位到该配置文件的位置时，是很不方便的，如果有个类似vim的工具多好，windows自带的文本编辑工具notepad打开后还不能像vim一样操作，很是不适，不过总有神一般的人物开发出好用的工具。windows下有类似linux下的vim工具gvim，但是gvim打开某些文件时，中文乱码，很是让人烦恼，下面就来介绍如何解决乱码的问题。 windows下默认vim打开是gbk格式的，所以中文乱码，需要进行设置vim打开时加载文件时的编码，参照如下设置： 打开gvim客户端 编辑_vimrc配置文件方式一： 方式二：直接编辑文件%VIM_HOME%_vimrc添加如下配置：12set enc=utf8 设置打开文件缓冲区编码set fencs=utf8,gbk,gb2312,gb18030,cp936 设置文件编码 设置后，再次打开ok。如果gvim菜单栏中文乱码编辑配置文件_vimrc，添加如下配置： 12source $VIMRUNTIME/delmenu.vim 设置gvim菜单文件编码source $VIMRUNTIME/menu.vim 设置gvim菜单文件编码]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>windows</tag>
        <tag>gvim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu防火墙操作]]></title>
    <url>%2F2018%2F02%2F07%2Fubuntu%E9%98%B2%E7%81%AB%E5%A2%99%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[使用过了centos的同胞们，听说ubuntu的交互性很不错，可视化界面也很炫酷，果断更换ubuntu系统，但是安装完成之后，感觉都不会使用linux系统了，于是各种google查询操作方法，下面来简单介绍ubuntu防火墙的操作。 使用ubuntu系统，配置防火墙稍微跟centos不太一样，有一样工具，叫做ufw，即uncomplicated firewall简单防火墙，刚开始用的时候不太习惯，记住这两个单词就行。ubuntu系统自带就有这个工具，可能版本的原因，你的ubuntu可能没有，不用担心，没有先来安装。 安装ufw工具1sudo apt install ufw -y 如果报错找不到包： 1234Reading package lists... DoneBuilding dependency tree Reading state information... DoneE: Unable to locate package ufw 更新一下依赖库就行： 1sudo apt-get update 然后继续安装ufw，安装完成后，我们来启动它1sudo ufw enable 此时防火墙就开启了，默认可以访问部分端口，不如22、443，想关闭所有外部ip对本机的端口访问的话，执行命令：1sudo ufw default deny 查看防火墙状态1sudo ufw status 启用或者禁用端口、服务允许外部访问端口12sudo ufw allow 22sudo ufw allow sshd 禁止外部访问端口12sudo ufw delete allow 80sudo ufw delete allow apache2 允许某个ip访问本机所有端口1sudo ufw allow from 192.168.1.1 OK，希望对大家有帮助，我们一起进步，有问题欢迎在下方留言，或者给我发邮件，邮件地址：chenzuoli709@gmail.com。]]></content>
      <categories>
        <category>ubuntu</category>
      </categories>
      <tags>
        <tag>ubuntu</tag>
        <tag>防火墙</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[c3p0数据库连接池的使用方法]]></title>
    <url>%2F2018%2F02%2F03%2Fc3p0%E6%95%B0%E6%8D%AE%E5%BA%93%E8%BF%9E%E6%8E%A5%E6%B1%A0%E7%9A%84%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[c3p0数据库连接池，一个jdbc连接池，封装了增删改查的各种方法，并为我们自动优化了数据库连接，提高程序的运行效率。 需要添加的依赖： 123456789101112&lt;!-- https://mvnrepository.com/artifact/c3p0/c3p0 --&gt;&lt;dependency&gt; &lt;groupId&gt;c3p0&lt;/groupId&gt; &lt;artifactId&gt;c3p0&lt;/artifactId&gt; &lt;version&gt;0.9.1.2&lt;/version&gt;&lt;/dependency&gt;&lt;!-- https://mvnrepository.com/artifact/com.mchange/c3p0 --&gt;&lt;dependency&gt; &lt;groupId&gt;com.mchange&lt;/groupId&gt; &lt;artifactId&gt;c3p0&lt;/artifactId&gt; &lt;version&gt;0.9.5.2&lt;/version&gt;&lt;/dependency&gt; 项目src/resources下需要配置文件c3p0.properties 1234567891011c3p0.JDBC.url=jdbc:mysql://localhost:3306/ms_cms?characterEncoding=utf8 --jdbc连接urlc3p0.DriverClass=com.mysql.jdbc.Driver --数据库驱动c3p0.user=root --用户名c3p0.pwd=xxx --密码c3p0.acquireIncrement=3 --当连接池中的连接耗尽时，一次性获取的连接数c3p0.idleConnectionTestPeriod=60 --检查连接池中的空闲连接c3p0.initialPoolSize=10 --初始化连接数c3p0.maxIdleTime=60 --最大空闲时间c3p0.maxPoolSize=20 --连接池最大连接数c3p0.maxStatements=100 --最大会话数c3p0.minPoolSize=5 --连接池最小连接数 java代码使用方法： 1234567891011121314151617181920212223242526272829public Connection dd() throws FileNotFoundException, IOException, PropertyVetoException, SQLException&#123; Properties pr = new Properties(); String c3p0Properties = this.getClass().getClassLoader().getResource(&quot;c3p0.properties&quot;).getPath(); //获得src下的c3p0.properties的路径 c3p0Properties = URLDecoder.decode(c3p0Properties, &quot;utf-8&quot;); //路径的编码是UTF-8 java.io.File c3p0File = new java.io.File(c3p0Properties); //得到文件c3p0.properties文件 pr.load(new FileInputStream(c3p0File)); //读取c3p0文件的内容 // pr.save(new FileOutputStream(c3p0File), null); ComboPooledDataSource cpds = new ComboPooledDataSource(); //使用c3p0操作数据库 cpds.setDriverClass(pr.getProperty(&quot;c3p0DriverClass&quot;)); //加载数据驱动 cpds.setJdbcUrl(pr.getProperty(&quot;c3p0.JDBC.url&quot;)); //连接特定的数据库 cpds.setUser(pr.getProperty(&quot;c3p0.user&quot;)); //数据库用户名 cpds.setPassword(pr.getProperty(&quot;c3p0.pwd&quot;)); //数据库用户密码 Connection conn = cpds.getConnection(); //获得连接 return conn; &#125;]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>c3p0</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Windows远程连接linux图形界面配置详解]]></title>
    <url>%2F2017%2F12%2F28%2FWindows%E8%BF%9C%E7%A8%8B%E8%BF%9E%E6%8E%A5linux%E5%9B%BE%E5%BD%A2%E7%95%8C%E9%9D%A2%E9%85%8D%E7%BD%AE%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[Windows远程连接linux图形界面，利用VNC服务实现windows远程连接linux图形化界面，linux作为VNC Server，windows作为VNC Viewer。原理很简单，在vnc server端生成一个桌面号，在vnc client端去连接该桌面号即可。其中很神奇的地方在于，如果两个人同时连接上一个桌面号的话，一个人可以看到另一个人的操作。 安装步骤 mini版centos安装图形化界面如果已经安装了图形化界面，则此步骤可以省略。 安装X window1yum groupinstall &quot;X Window System&quot; 安装GNOME Desktop1yum groupinstall &quot;GNOME Desktop&quot; 如果是centos7以前的版本，则安装命令为 1yum groupinstall &quot;Desktop&quot; 如果找不到Desktop，那么试试： 1yum grouplist 查看可以安装的group，可能不同的版本group组的名字不同。 启动gnome1startx 切换到图形化界面 linux安装VNC Server安装1yum install vnc-server –y 配置12cp /lib/systemd/system/vncserver@.service /etc/systemd/system/cp /lib/systemd/system/vncserver@.service /etc/systemd/system/vncserver@:1.service 编辑vim /etc/systemd/system/vncserver@:1.service将更换为root 设置开机自启动1systemctl enable vncserver@:1.service 添加防火墙信任规则12firewall-cmd --permanent --add-service vnc-serverfirewall-cmd –reload 重启服务器reboot启动vnc服务启动方式： 1vncserver :桌面号 注意：中间需留有空格，桌面号用数字表示，表示每个用户占用一个桌面连接。以上命令执行的过程中，因为是第一次执行，需要输入密码，密码被加密/root/.vnc/passwd中；同时在用户主目录下的.vnc子目录中为用户自动建立xstartup配置文件（/root/.vnc/xstartup），在每次启动VND服务时，都会读取该文件中的配置信息。 VNC服务使用的端口号与桌面号的关系 windows安装VNC Viewer安装测试输入ip:桌面号连接 其他修改vnc密码1vncpasswd 关闭vnc服务1vncserver -kill :1 防火墙添加信任12firewall-cmd --permanent --add-service vnc-serverfirewall-cmd --reload]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>windows</tag>
        <tag>vnc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建maven私服nexus]]></title>
    <url>%2F2017%2F12%2F26%2F%E6%90%AD%E5%BB%BAmaven%E7%A7%81%E6%9C%8Dnexus%2F</url>
    <content type="text"><![CDATA[maven私服搭建，目的就是我们在使用maven打包时，如果私服中有相对应的包时，可以直接拉取过来，而不需要去下载，仅仅第一次使用该包时才会下载，这样会减少很多的时间，提高效率。 安装配置nexus下载：nexus下载解压： 1$ tar zxvf nexus-3.6.1-02-unix.tar.gz nexus详解文档参考 修改启动用户1$ vim $NEXUS_HOME/bin/nexus.rc 修改默认端口1$ vim $NEXUS_HOME/ etc/nexus-default.properties 启动1$ ./bin/nexus run 访问浏览器访问8081端口，默认登陆：user: adminpassword: admin123 配置maven-central：maven中央库，默认从https://repo1.maven.org/maven2上拉取jar包；maven-releases：私库发行版jar，初次安装nexus请将Deployment policy设置成Allow redeploy；maven-snapshots：私库快照调试版本jar；maven-public：仓库分组，把上面三个仓库组合起来一起对外提供服务，在本地maven配置settings.xml中使用。 本地maven使用私服nexusmaven默认配置settings.xml1234567891011121314151617181920212223242526272829303132333435363738394041&lt;servers&gt; &lt;server&gt; &lt;id&gt;releases&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt; &lt;/server&gt; &lt;server&gt; &lt;id&gt;snapshots&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt; &lt;/server&gt;&lt;/servers&gt;&lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;mirrorOf&gt;*&lt;/mirrorOf&gt; &lt;url&gt;http://123.207.66.156:8081/repository/maven-public/&lt;/url&gt; &lt;/mirror&gt;&lt;/mirrors&gt;&lt;profiles&gt; &lt;profile&gt; &lt;id&gt;dev&lt;/id&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;Nexus&lt;/id&gt; &lt;url&gt;http://123.207.66.156:8081/repository/maven-public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;/profile&gt;&lt;/profiles&gt;&lt;activeProfiles&gt; &lt;activeProfile&gt;dev&lt;/activeProfile&gt;&lt;/activeProfiles&gt; 修改工程pom.xml123456789101112&lt;distributionManagement&gt; &lt;repository&gt; &lt;id&gt;releases&lt;/id&gt; &lt;name&gt;Releases&lt;/name&gt; &lt;url&gt;http://123.207.66.156:8081/repository/maven-releases/&lt;/url&gt; &lt;/repository&gt; &lt;snapshotRepository&gt; &lt;id&gt;snapshots&lt;/id&gt; &lt;name&gt;Snapshot&lt;/name&gt; &lt;url&gt;http://123.207.66.156:8081/repository/maven-snapshots/&lt;/url&gt; &lt;/snapshotRepository&gt;&lt;/distributionManagement&gt; 注意上面的repository的id值一定要跟settings.xml文件中配置的server一致。]]></content>
      <categories>
        <category>maven</category>
      </categories>
      <tags>
        <tag>maven</tag>
        <tag>nexus</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark基于zookeeper的高可用搭建]]></title>
    <url>%2F2017%2F12%2F26%2Fspark%E5%9F%BA%E4%BA%8Ezookeeper%E7%9A%84%E9%AB%98%E5%8F%AF%E7%94%A8%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[spark提供服务时，master的角色非常的重要，它负责任务分发、任务调度，可谓任重道远啊，所以我们要对master做高可用，基于zookeeper的高可用，可以自动实现master挂掉后备用的master启动，堆外提供服务。 节点分布：zookeeper: node1 node2 node3spark: node1 node2 node3 node4 编辑SPARK_HOME/conf/spark-env.sh注释掉HADOOP_CONF_DIR，添加SPARK_DAEMON_JAVA_OPTS，其他配置不变。 1export SPARK_DAEMON_JAVA_OPTS=&quot;-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=node1:2181,node2:2181,node3:2181 -Dspark.deploy.zookeeper.dir=/spark20170302&quot; 同步该配置文件spark-env.sh到其他节点scp spark-env.sh root@node2:$SPARK_HOME/confscp spark-env.sh root@node3:$SPARK_HOME/confscp spark-env.sh root@node4:$SPARK_HOME/conf 在node2节点上编辑spark-env.sh，将SPARK_MASTER_HOST修改为node21SPARK_MASTER_HOST=node2 启动spark服务在node1节点上启动spark集群 1$ ./sbin/start-all.sh 启动另一个master在node2节点上只启动master 1$ ./sbin/start-master.sh 访问webUI查看启动情况如果配置正确，启动正常，那么master会有两个（node1， node2），一个为ACTIVE状态，一个为STANDBY状态。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019年世界上最好的机场]]></title>
    <url>%2F2017%2F12%2F26%2F2019%E5%B9%B4%E4%B8%96%E7%95%8C%E4%B8%8A%E6%9C%80%E5%A5%BD%E7%9A%84%E6%9C%BA%E5%9C%BA%2F</url>
    <content type="text"><![CDATA[机场的好坏，决定因素：1.要看城市的经济规模，政治、文化聚集度；2.有强大的基地航空公司或者合作关系；3.地理位置；4.政治环境； 如下是综合评价得到的世界机场排名： 1.樟宜（新加坡）2.羽田（东京）3.仁川（首尔）4.哈马德（多哈）5.香港机场（中国）6.中部国际机场（日本）7.慕尼黑机场（德国）8.希思罗机场（伦敦）9.成田（东京）10.苏黎世机场（瑞士）]]></content>
      <categories>
        <category>世界排名</category>
      </categories>
      <tags>
        <tag>世界机场</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[单台服务器安装spark、hadoop服务文档]]></title>
    <url>%2F2017%2F12%2F26%2F%E5%8D%95%E5%8F%B0%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AE%89%E8%A3%85spark%E3%80%81hadoop%E6%9C%8D%E5%8A%A1%E6%96%87%E6%A1%A3%2F</url>
    <content type="text"><![CDATA[spark作为分布式计算引擎，如果内存足够，是需要很少的磁盘空间的，在shuffle可能用到，在reduce阶段一定会用到，它是基于hdfs作为存储介质的，所以在使用spark时，应该搭建一个hdfs。 安装JDK1.8安装并配置环境变量，步骤略。 安装scala2.11.8安装并配置环境变量，步骤略。 hadoop伪分布式搭建关闭防火墙配置本机对本机免秘钥登录ssh-keygen -t rsa -P ‘’ -f ~/.ssh/id_rsassh-copy-id ip其中ip为本机ipSsh ip首次本机ssh本机需要输入密码或者yes，输入即可，第二次或者以后就不需要输入参数了。 下载hadoop-2.7.4.tar.gz包解压修改配置文件HADOOP_HOME/etc/hadoop下Hadoop.env.sh修改JAVA_HOME为jdk路径； Core-site.xmlFs.defaltFS属性修改为namenode的ipHadoop.tmp.dir修改为自定义目录，并创建好该目录 123456789101112&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://192.168.109.235:9000&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/root/chen/hadoop/data/temp&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;fs.trash.interval&lt;/name&gt; &lt;value&gt;1440&lt;/value&gt;&lt;/property&gt; Hdfs-site.xml使用默认值即可 12345678&lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.permissions&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt; Mapred-env.sh修改JAVA_HOME为jdk路径，其他默认。 Mapred-site.xml1234&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt; yarn-env.sh修改JAVA_HOME为java安装路径 yarn-site.xml123456789101112131415161718yarn.resourcemanager.hostname属性指定为namenode的ip地址。&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;description&gt;The hostname of the RM.&lt;/description&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;192.168.109.235&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt; &lt;value&gt;2048&lt;/value&gt;&lt;/property&gt; 添加slaves文件在HADOOP_HOME/etc/hadoop文件夹下添加slaves文件，指定datanode节点添加localhost即可。 格式化namenode./bin/hdfs namenode –format 启动hdfs./sbin/start-all.sh jps查看节点服务的启动情况如果启动正常，那么应该有NamenodeSecondaryNamenodeResourcemanagerNodemanagerDataNode这5个角色 Web Ui访问：http://ip:50070Spark搭建下载并解压spark-2.1.0-bin-hadoop2.7.tgz修改配置文件cp slaves.template slavescp spark-env.sh.template spark-env.shcp spark-defaults.conf.template spark-defaults.confvi spark-env.sh增加参数 123456789SPARK_MASTER_HOST=修改为ipSPARK_MASTER_PORT=7077SPARK_WORKER_CORES=2SPARK_WORKER_MEMORY=4gSPARK_WORKER_INSTANCES=3HADOOP_CONF_DIR=/chen/hadoop2.7/hadoop-2.7.4/etc/hadoop修改为hadoop配置文件的位置SPARK_DRIVER_MEMORY=1024MJAVA_HOME=/chen/jdk8/jdk1.8.0_144修改为jdk的路径MAVEN_OPTS=&quot;-Xms1024m -Xmx4096m -XX:PermSize=1024m&quot; vi spark-deafults.conf其中需要修改hdfs的ip地址，并创建路径/user/spark/logs 启动spark./sbin/start-all.sh正常启动的话应该有：1个Master3个Worker两个角色Web Ui访问http://ip:8080]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>spark</tag>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux高效编程]]></title>
    <url>%2F2017%2F12%2F25%2Flinux%E9%AB%98%E6%95%88%E7%BC%96%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[本文将介绍linux命令行经常使用到的一些快捷键、编辑命令，还有强大的vim编辑器，能让你在linux上编程更高效。作为一个程序员，会使用各种快捷键不也是更炫酷的一件事吗？ linux命令行光标移动1234567891011121314151617181920212223ctrl+a --- 把光标移到行首（ahead）ctrl+e --- 把光标移到行尾（end）ctrl+l --- 清除终端（clear）ctrl+u --- 删除当前字符到行首（带有剪切功能）ctrl+k --- 删除当前字符到行尾（带有剪切功能）ctrl+y --- 粘贴ctrl+f --- 向前移动一个字符（forward）ctrl+b --- 向后移动一个字符（back）ctrl+左右箭头 --- 把光标在单词之间左右移动ctrl+w --- 删除光标前面的单词cd ~ --- 进入home目录cd - --- 返回上一目录mkdir -p path/to/filealias cd3=”cd ../../../”rename ‘.java’ ‘.java.bak’ *.java --- 批量备份文件ctrl+r --- 查询历史命令history --- 历史命令执行历史命令方法 --- ！+ 命令序号ctrl+p --- 上一条命令（或者上下箭头） 查找进程12345678910进程 进程号 所占用端口号ps -ef显示所有进程信息，包括命令行，与grep配合使用，查找特定进程显示环境变量ps -aux显示所有进程信息，包括资源占用情况，与grep配合使用netstat -anp显示协议、端口、进程号、进程名称等信息 Vimvim与vi的区别：增加了新特性：语法高亮、可视化操作、多平台支持（windows、mac、terminal） 正常模式：浏览和修改文本内容1234567891011121314151617181920R --- 替换（覆盖）当前光标位置及后面的若干文本J --- 合并当前行及下一行为一行j --- 下k --- 上h --- 左l --- 右H --- 当前屏幕第一行M --- 当前屏幕中间行L --- 当前屏幕最后一行w --- 当前光标移至下一个单词词首b --- 当前光标移至上一个单词词首e --- 下一个单词词尾$ --- 当前光标移至行尾^ --- 当前光标移至行首u --- 撤销ctrl+r --- 恢复上一步被撤销的动作 复制123456yy --- 复制当前行5yy --- 复制当前行和后4行yw --- 当前字符到下一单词的起始位置y$ --- 当前字符到当前行末尾y0/y^ --- 当前字符到当前行行首yngg/ynG --- 复制当前行到文件第n行 粘贴1p 删除1234567dw --- 删除当前光标至单词末尾ndw --- 删除当前光标后的n个字符dd --- 删除当前行d$ --- 删除光标位置至行尾d^ --- 删除光标位置至行首dgg --- 删除首行至当前行dG --- 删除当前行至末行 编辑模式 — 编辑文本从正常模式进入编辑模式 123456a --- 在当前光标位置的右边添加文本A --- 在当前行的末尾位置添加文本i --- 在当前光标位置的左边添加文本I --- 在当前行的开始处添加文本(非空字符的行首)O --- 在当前行的上面新建一行o --- 在当前行的下面新建一行 可视模式：高亮选取文本后的正常模式1234v+hjkl --- 选中文本后y复制d剪切，p粘贴ctrl+v --- 以块为选取单位V --- 以行为选取单位行、块为选取单位的模式可以随意切换 命令行模式：操作文本文件123456w --- 保存wq --- 保存并退出q --- 退出q! --- 不保存退出/ --- 查询，n下一个匹配字符串，N上一个匹配字符串:set number --- 设置行号]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>programming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[以Hexo网页制作模板构建Github Pages个人网站]]></title>
    <url>%2F2017%2F12%2F19%2FcreateWebsiteHelp%2F</url>
    <content type="text"><![CDATA[个人网站制作过程，以Hexo为例，为大家讲解如何制作，如果有什么错误的地方，欢迎指正，如果有什么不懂的地方，可以email我：chenzuoli709@gmail.com。具体请看详细内容 —&gt; 1.准备环境Node.jsGit 2.安装Hexo1$ npm install -g hexo-cli 3.创建github pagesGithub官网项目名称为.github.io 4.配置Hexo123$ hexo init &lt;folder&gt;$ cd &lt;folder&gt;$ npm install 初始化完成后，该目录下的文件结构如下： 1234567_config.yml --- 全局配置文件package.jsonscaffolds --- 模板source --- 文件仓库 _drafts --- 草稿 _posts --- 发布过的文件themes --- 主题 编辑_config.yml：指定项目部署的方式为git，上传到远程仓库repo的master分支上 1234deploy: type: git repo: https://github.com/chenzuoli/chenzuoli.github.io.git branch: master 5.部署12hexo generate --- 生成hexo deploy --- 部署到github 6.域名映射第一步：登录你购买域名服务商提供给你的域名管理中心，我购买的是腾讯云，域名为chenzuoli.com，首先绑定主域名映射到github.com所对应的ip地址，绑定完成后，隔几分钟测试，因为DNS解析先从你的域名提供商开始，然后到其他的域名提供商，再到国外： 1ping chenzuoli.com 看是否能够ping通，如果ping通，说明域名映射已经ok了第二步：登录github到.github.io项目，进入settings选项，设置自定义域名chenzuoli.com，save后，可以看到在该项目下会自动生成一个CNAME的文件，文件内容就是你设置的域名chenzuoli.com。稍等几分钟，就可以访问chenzuoli.com了。这里说明一下域名映射的流程：chenzuoli.com -&gt; github.com -&gt; chenzuoli.github.iochenzuoli.com映射到github.com，然后github.com会解析该请求寻找CNAME为chenzuoli.com的项目，然后就找到了chenzuoli.github.io，于是就可以访问了。大家赶紧试试吧。]]></content>
      <tags>
        <tag>hexo</tag>
        <tag>github pages</tag>
        <tag>blog</tag>
      </tags>
  </entry>
</search>
