---
title: 上海面试总结
date: 2020-06-10 06:19:31
tags: [数据仓库]
categories: 数据仓库
notebook: 数据仓库
---

来上海面试，来看看面试的这几家公司问的问题。

<!-- more -->

# 一、叽里呱啦
## 1.sql解决数据倾斜问题
|     关键词     |   情形   |   后果   |
| :----------: | :------: | :------: |
| join  | 其中一个表较小，但是key集中 | 分发到某一个或几个reduce上的数据量远超于平均值  |
| join  | 大表与大表，但是分桶的判断是0值或空值过多 | 这些0值或空值都由一个reduce处理非常慢 |
| group by  | group by 维度过小，某值得数量过多 | 处理某值得reduce非常耗时 |
| count distinct | 某值得数量过多 | 处理此特殊值的reduce非常耗时 |

原因：
1）key分布不均匀
2）业务数据本身特性
3）建表时考虑不周
4）某些sql语句本身带有数据倾斜

解决方案：
1）参数调节
hive.map.aggr=true;
map端分聚合，相当于combiner
hive.groupby.skewindata=true;
有数据倾斜的时候进行负载均衡，当设定为true后，生成的执行计划有2个MR Job，第一个mr job中，map的输出结果会随机分配到reduce中，每个reduce做部分聚合操作，并输出结果，这样处理的结果是相同的group by key，会分配到不同的reduce中；第二个mr job，再根据第一个job得到的结果group by key进行reduce操作。

2）sql语句调节
a.大表join小表
hive.convert.join=true;
map端join操作让小的维表先进内存，在map端完成reduce操作，再reduce；

b.大表join大表
先对大表中的key做一个值分布，看看
把空值的key变成一个随机数，把倾斜的数据分发到不同的reduce中，由于null值关联不上，不影响最后结果

c.count distinct大量相同特殊值
将异常值单独处理，过滤调

d.group by 维度过小
sum() group by 替换count(distinct)

e.特殊情况特殊处理
在业务逻辑优化效果不大的情况下，有些时候是可以将特殊数据单独拿出来特殊处理，然后再union回去。


3）空值产生的数据倾斜
空值不参与关联
```
select * from log a
join users b
  on a.user_id is not null
  and a.user_id = b.user_id
union all
select * from log a
  where a.user_id is null;
```
赋予空值新的key
```
select *
  from log a
  left outer join users b
  on case when a.user_id is null then concat(‘hive’,rand() ) else a.user_id end = b.user_id;
```
方法二比方法一好：IO少，job数也变少了。方法一种log读2次，2个job，方法二log读一次，1个job

4）不同类型值关联产生数据倾斜
场景：user表user_id为int类型，log表中user_id为string类型，当2个表按照user_id进行join时，默认的hash操作会按照int类型的id进行分配，这样会导致所有string类型的user_id记录都在同一个reducer中。
解决办法：将数字类型转化成字符串类型
```
select * from users a
  left outer join logs b
  on a.usr_id = cast(b.user_id as string)
```

5）小表不大不小，怎么用map join解决数据倾斜
使用mapjoin 解决小表join大表出现的数据倾斜情况，但是如果小表不大不小，导致join的时候出现异常或者bug，这时就需要特别梳理。
```
select * from log a
  left outer join users b
  on a.user_id = b.user_id;
```
如果小表user表有600w记录，将user表分发到所有map端内存中也是不小的开支。这时，可以先进行筛选一遍user表，假如log表中的user_id有上百万个，当天的pv，uv，click pv不会太多，所以可以筛选掉大部分的user_id；
```
select * from log a
  left outer join (
    select  d.*
      from ( select distinct user_id from log ) c
      join users d
      on c.user_id = d.user_id
    ) b
  on a.user_id = b.user_id;
```


## 2.hive sql解析成MapReduce过程
AST tree -> Query Block -> Operator tree -> Map Reduce
a.Antlr定义SQL的语法规则，完成SQL词法，语法解析，将SQL转化为抽象语法树AST Tree
b.遍历AST Tree，抽象出查询的基本组成单元QueryBlock
c.遍历QueryBlock，翻译为执行操作树OperatorTree
d.逻辑层优化器进行OperatorTree变换，合并不必要的ReduceSinkOperator，减少shuffle数据量
e.遍历OperatorTree，翻译为MapReduce任务
f.物理层优化器进行MapReduce任务的变换，生成最终的执行计划

## 3.怎么理解数据开发与数据仓库
数据开发基于数据仓库，在仓库的基础上做分析、挖掘
数据仓库存储、管理数据

## 4.怎么理解在线教育

## 5.对数据工程师的理解
数据开发+数据仓库

## 6.Hive行列互转
1）列转行
concat_ws和collect_set、collect_list
2）行转列
lateral view explode和split

## 7.事实表分几类
从粒度划分
1）事务事实表
    在某个时间点上的一个事件，比如：下单、付款、退款等事实，
2）周期快照事实表
    某段时间内的数据进行分析，比如每天、月、周、季度、年。
3）累积快照事实表
    具有确定的开始和结束日期，时间段不固定，并且记录关键事件或者过程的里程碑。
    累积快照事实表和周期快照事实表有些相似之处，它们存储的都是事务数据的快照信息。但是它们之间也有着很大的不同，周期快照事实表记录的确定的周期的数据，而累积快照事实表记录的不确定的周期的数据。


从用途来区分
1）原子事实表
2）聚集事实表
    某些维度上的原子事实表，也可以做一定程度的汇总
3）合并事实表
    从多个事实表来，合并到一起组成新的事实表，可以做一定程度的汇总


## 8.维表分几类
桥接表
缓慢变化维表

## 9.领域建模的事实表和维度建模的事实表有什么区别？

## 10.kafka多分区情况下如何保证有序
同一key发送到同一partition

## 11.kafka ack机制
1）ack=1(默认)，producer生产消息，kafka只要一个副本成功写入就反馈成功消息给producer，这个副本一定要是leader副本
2）ack=0，producer只推送一次消息
3）ack=-1，kafka全部副本复制完成才会反馈成功消息给producer。

## 12.lucene倒排索引的原理
一般我们查询关系型表的时候，指定where条件，某个字段等于某个值，这是正排索引，通过指定值去找。
倒排索引是文章内容进行分词后，得到一个字典，这个字典是一个一个词组成，还会有频率文件，位置文件，当我们想查询某个词时，就找到了这个词的频率，在那些文件里面，直接定位文档。

# 二、鲲驰
## 1.设计的维度有哪些？
会员等级，地域，年龄，性别，订单，服务，时间

## 2.日活月活多少？
总用户100万，日活17万，月活

## 3.仓库怎么建设的？

# 三、松鼠AI
## 1.kafka消费者的rebalance
新增或移除customer时，自动触发rebalance机制
## 2.hbase的读写流程
写：
1）client访问ZK，获取表的meta信息（region位置信息）查询得到的namespace、表名、rowkey信息，获取到的可写region信息存储到client cache中；
2）client向regionserver发送请求，先将操作和数据写入hlog（预写日志wal），再将数据写入memstore，保持有序；
3）memstore超过阈值，将溢写磁盘，生成一个storefile，store中storefile数量达到阈值，会将若干小的storefile合并（compact）成大的storefile，当region中最大的storefile达到阈值，region分裂（split）成2个region

读：
1）client访问ZK，获取表的meta信息（region位置信息）查询得到的namespace、表名、rowkey信息，获取到的可读region信息存储到client cache中；
2）client向regionserver发送读请求；
3）regionserver先从memstore中读取数据，没读取到就从storefile中读取；

major compact
minor compact

## 3.spring用过哪些注解
## 4.join有哪几种类型
map join
common join
skew join
bucket map join

## 5.df rdd ds区别
df ds包含表头，df每行是一个row对象，ds每个字段是一个对象，底层都基于rdd
rdd=df.rdd
rdd=ds.rdd

## 6.hive sql优化
1）列裁剪和分区裁剪
2）where条件尽可能提前执行
3）sort by 代替 order by
4）group by 代替 distinct
5）数据倾斜均衡配置项
  hive.groupby.skewindata=true;
  生成2个job，第一个在map端做预聚合，相同的key分布到不同的reducer中，达到负载均衡的效果
  hive.optimize.skewjoin=true;
6）map join
7）bucket map join
8）移除空值、异常值
9）单独处理倾斜的key
10）不同数据类型进行join
  一个int，一个string，将int转string
11）大表转小表
  有时，build table会大到无法直接使用map join的地步，比如全量用户维度表，而使用普通join又有数据分布不均的问题。这时就要充分利用probe table的限制条件，削减build table的数据量，再使用map join解决。代价就是需要进行两次join
12）调整map、reducer个数
  mapred.map.tasks
  mapred.min.split.size
  mapred.max.split.size
  split_size = MAX(mapred.min.split.size, MIN(mapred.max.split.size, dfs.block.size))；
  split_num = total_input_size / split_size
  mapper_num = MIN(split_num, MAX(default_num, mapred.map.tasks))

reducer默认个数，1个reducer默认处理数据量大小，如何设置reducer个数
  默认1个，处理1G数据，最大999个
  mapred.reduce.tasks
  hive.exec.reducers.bytes.per.reducer
  hive.exec.reducers.max
  reducer_num = MIN(total_input_size / reducers.bytes.per.reducer, reducers.max)
13）合并小文件

  mapred.min.split.size.per.node
  mapred.min.split.size.per.rack
  含义是单节点和单机架上的最小split大小。如果发现有split大小小于这两个值（默认都是100MB），则会进行合并。具体逻辑可以参看Hive源码中的对应类

  hive.merge.mapfiles
  hive.merge.mapredfiles
  都设为true即可，前者表示将map-only任务的输出合并，后者表示将map-reduce任务的输出合并。
另外，hive.merge.size.per.task可以指定每个task输出后合并文件大小的期望值，hive.merge.size.smallfiles.avgsize可以指定所有输出文件大小的均值阈值，默认值都是1GB。如果平均大小不足的话，就会另外启动一个任务来进行合并。

14）启动压缩
压缩job的中间结果数据和输出数据，可以用少量CPU时间节省很多空间。压缩方式一般选择Snappy，效率最高。
要启用中间压缩，需要设定hive.exec.compress.intermediate为true，同时指定压缩方式hive.intermediate.compression.codec为org.apache.hadoop.io.compress.SnappyCodec。另外，参数hive.intermediate.compression.type可以选择对块（BLOCK）还是记录（RECORD）压缩，BLOCK的压缩率比较高。
输出压缩的配置基本相同，打开hive.exec.compress.output即可。

15）jvm重用
在MR job中，默认是每执行一个task就启动一个JVM。如果task非常小而碎，那么JVM启动和关闭的耗时就会很长。可以通过调节参数mapred.job.reuse.jvm.num.tasks来重用。例如将这个参数设成5，那么就代表同一个MR job中顺序执行的5个task可以重复使用一个JVM，减少启动和关闭的开销。但它对不同MR job中的task无效。

16）并行执行与本地模式
并行执行
Hive中互相没有依赖关系的job间是可以并行执行的，最典型的就是多个子查询union all。在集群资源相对充足的情况下，可以开启并行执行，即将参数hive.exec.parallel设为true。另外hive.exec.parallel.thread.number可以设定并行执行的线程数，默认为8，一般都够用。
本地模式
Hive也可以不将任务提交到集群进行运算，而是直接在一台节点上处理。因为消除了提交到集群的overhead，所以比较适合数据量很小，且逻辑不复杂的任务。
设置hive.exec.mode.local.auto为true可以开启本地模式。但任务的输入数据总量必须小于hive.exec.mode.local.auto.inputbytes.max（默认值128MB），且mapper数必须小于hive.exec.mode.local.auto.tasks.max（默认值4），reducer数必须为0或1，才会真正用本地模式执行。

17）严格模式
所谓严格模式，就是强制不允许用户执行3种有风险的HiveSQL语句，一旦执行会直接失败。这3种语句是：

查询分区表时不限定分区列的语句；
两表join产生了笛卡尔积的语句；
用order by来排序但没有指定limit的语句。
hive.mapred.mode设为strict

18）合适的文件存储格式
TextFile、SequenceFile、RCFile、Avro、ORC、Parquet等
仓库不同阶段，表的使用方式不一样，那么底层存储文件格式也应该根据不同格式的性能特点进行选择。


## 7.hive sql排序有哪几种
row_number
rank
dense_rank


# 四、paypal初试
## 1.awk grep sed
## 2.shell 
  for循环
  for ((i=0;i<10;i++>));
  do
    ...
  done

  条件判断
  if [  ];then

  elif [  ];then

  fi

  接收参数
  $0 运行命令
  $1 第一个参数
  $# 参数个数
  $? 最近一条命令的执行情况

# 四（二）、paypal复试
## 1.shell遍历list判断遍历是否存在
```
b=5
a=(1 2 3 4 5)
flag=false
for ele in ${a[*]}
do
  echo $ele
  if [ $b == $ele ];then
    flag=true
  fi
done
echo $flag
```

## 2.mysql优化
一个表
id  name  email ... ... ...
有几亿行，几百个字段
id唯一索引
通过name或email来查询其他字段信息，如何优化？

建立三个索引
create index index_name on table(name)
create index index_email on table(email)
create index index_name_email on table(name,email)

## 3.reducer端oom怎么处理


# 五、爱迪信
## 1.数据仓库怎么做的
  a）系统分析，确定主题
  b）选择满足仓库系统要求的软件平台
  c）建立数据仓库逻辑模型
  d）逻辑模型转化为仓库数据模型
  e）数据仓库模型优化
  f）数据清洗、转化、传输
  g）开发数据仓库分析应用
  h）数据仓库管理
  
## 2.用shell都做哪些事情？
  hive sql
  python脚本
  流程控制
  命令操作组件：sqoop airflow hive es
  数据重跑、修复

